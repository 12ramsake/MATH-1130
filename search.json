[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 1130 Companion Manual",
    "section": "",
    "text": "Introduction\nThis is a short companion manual for the course MATH 1130. It contains a brief overview of the major topics and some of the Python methods covered in each case. It is not a replacement for the cases, which are the main content of the course. It is meant to be used as a reference manual and as supplementary material to aid you in your understanding of the material. Please report any typos to kramsay2@yorku.ca .",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Case_1.html",
    "href": "Case_1.html",
    "title": "1  Python basics",
    "section": "",
    "text": "1.1 Python and Jupyter\nPython is a general purpose programming language that allows for both simple and complex data analysis. Python is incredibly versatile, allowing analysts, consultants, engineers, and managers to obtain and analyze information for insightful decision-making.\nThe Jupyter Notebook is an open-source web application that allows for Python code development. Jupyter further allows for inline plotting and provides useful mechanisms for viewing data that make it an excellent resource for a variety of projects and disciplines.\nThe following section will outline how to install and begin working with Python and Juypter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#setting-up-the-python-environment-locally-optional",
    "href": "Case_1.html#setting-up-the-python-environment-locally-optional",
    "title": "1  Python basics",
    "section": "1.2 Setting up the Python Environment locally (optional)",
    "text": "1.2 Setting up the Python Environment locally (optional)\nInstruction guides for Windows and MacOS are included below. Follow the one that corresponds with your operating system.\n\n1.2.1 Windows Install\n\nOpen your browser and go here\nClick on your OS and then “Download”\nRun the downloaded file found in the downloads section from Step 2\nClick through the install prompts\nGo to menu (or Windows Explorer), find the Anaconda3 folder, and double-click to run OR Use a Spotlight Search and type “Navigator”, select and run the Anaconda Navigator program. Note that MacOS also comes with Python pre-installed, but you should not use this version, which is used by the system. Anaconda will run a second installation of Python and you should ensure that you only use this second installation for all tasks.\n\n\n\n1.2.2 Compare and contrast Jupyter, Python and Anaconda\n\nJupyter Notebook is a web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.\nPython is a programming language that is often used in scientific computing, data science, and general-purpose programming.\nAnaconda is a distribution of Python and R for scientific computing and data science. It includes the conda package manager, which makes it easy to install packages for scientific computing and data science, as well as the Jupyter Notebook and other tools.\nIn simple terms Anaconda is a distribution and python is a language, Jupyter notebook is an application to create and share document that contains live code, equations, visualizations and narrative text.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#an-alternative-to-jupyter-is-vscode",
    "href": "Case_1.html#an-alternative-to-jupyter-is-vscode",
    "title": "1  Python basics",
    "section": "1.3 An alternative to Jupyter is VSCode",
    "text": "1.3 An alternative to Jupyter is VSCode\nVSCode is a very popular tool for programming in a variety of languages. In can be used in place of Jupyter. It can be downloaded from here. After installation, follow this guide to set up Jupyter in VSCode. You will also need the Python extension and the Jupyter extension.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#file-management-with-python-and-jupyter",
    "href": "Case_1.html#file-management-with-python-and-jupyter",
    "title": "1  Python basics",
    "section": "1.4 File Management with Python and Jupyter",
    "text": "1.4 File Management with Python and Jupyter\nIt is common practice to have a main folder where all projects will be located (e.g. “jupyter_research”). The following are guidelines you can use for Python projects to help keep your code organized and accessible:\n\nCreate subfolders for each Jupyter-related project\nGroup related .ipynb (the file format for Jupyter Notebook) files together in the same folder\nCreate a “Data” folder within individual project folders if using a large number of related data files\n\nYou should now be set up and ready to begin coding in Python!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#fundamentals-of-python",
    "href": "Case_1.html#fundamentals-of-python",
    "title": "1  Python basics",
    "section": "1.5 Fundamentals of Python",
    "text": "1.5 Fundamentals of Python\nIn this case, we will introduce you to more basic Python concepts. If you prefer, you can first go through the more extensive official Python tutorial and/or the W3School Python tutorial and practice more fundamental concepts before proceeding with this case. It is highly recommended that you go through either or both of these tutorials either before or after going through this notebook to solidify and augment your understanding of Python. For a textbook introduction to Python, see this text, from which some of the following material is adapted/taken.\n\n1.5.1 What is a program?\n\nA program is a sequence of instructions that specifies how to perform a computation.\nThe computation might be something mathematical, such as solving a system of equations or finding the roots of a polynomial, but it can also be a symbolic computation, such as searching and replacing text in a document or something graphical, like processing an image or playing a video.\nThe details look different in different languages, but a few basic instructions appear in just about every language:\n\ninput: Get data from the keyboard, a file, the network, or some other device.\noutput: Display data on the screen, save it in a file, send it over the network, etc.\nmath: Perform basic mathematical operations like addition and multiplication.\nconditional execution: Check for certain conditions and run the appropriate code.\nrepetition: Perform some action repeatedly, usually with some variation.\n\n\nEvery program you’ve ever used, no matter how complicated, is made up of instructions that look pretty much like these. So you can think of programming as the process of breaking a large, complex task into smaller and smaller subtasks until the subtasks are simple enough to be performed with one of these basic instructions.\nNote that writing a program involves typing out the correct code, and then running, or executing that code. For example, the print() function allows you to print an object. Placing a python object in the round brackets and running the code makes the computer print the object.\n\nprint(\"Hello World\")\n\nHello World\n\n\nAfter printing, the simplest use of Python is as a calculator. We can use the operators +, -, /, and * perform addition, subtraction, division and multiplication. We can use ** for exponentiation. See the following examples:\n\nprint(40 + 2)\nprint(43 - 1)\nprint(6 * 7)\nprint(2**2)\n\n42\n42\n42\n4\n\n\nNote that a Python program contains one or more lines of code, which are executed in a top-down fashion.\n\n\n1.5.2 Values and types\n\nA value is one of the basic things a program works with, like a letter or a number.\nSome values we have seen so far are 2, 5.0, and ‘Hello, World!’.\nThese values belong to different types: 2 is an integer, 5.0 is a floating-point number, and ‘Hello, World!’ is a string, so-called because the letters it contains are strung together.\n\nUse type() to determine the type of a value. Example:\n\nprint(type(2))\nprint(type(42.0))\nprint(type('Hello, World!'))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\nIf you are used to using Microsoft Excel, this is similar to how Excel distinguishes between data types such as Text, Number, Currency, Date, or Scientific. As noted above, some common data types that you will come across in Python are:\n\nInteger type int: 1\nFloat type float: 25.5\nString type str: 'Hello'\n\n\nHere we see (1) integers and (2) floats store numeric data. The difference between the two is that floats store decimal variables (fractions), whereas the integer type can only store integer variables (whole numbers).\nFinally, (3) is the string type. Strings are used to store textual data in Python (a string of one or more characters). Later in this case we will use string variables to store country names. They are often used to store identifiers such as names of people, city names, and more.\n\nThere are other data types available in Python; however, these are the three fundamental types that you will see across almost every Python program. Always keep in mind that every value, or object, in Python has a type and some of these “types” can be custom-defined by the user, which is one of the benefits of Python.\n\n\n1.5.3 Variables\nWe can assign names to objects in python so that they are easy to manipulate, a named object is called a variable. Use the = sign to assign a name to a variable.\n\nFor example, if a user aims to store the integer 5 in an object named my_int, this can be accomplished by writing the Python statement, my_int = 5.\nIn this case, my_int is a variable, much like you might find in algebra, but the = sign is for assignment not equality .\nSo my_int = 5 should be taken to mean something more like Let my_int be equal to 5 rather than my_int is equal to 5.\n\n\nmy_int=5\nprint(my_int)\n\n5\n\n\nHere, my_int is an example of a variable because it can change value (we could later assign a different value to it) and it is of type Integer, known in Python simply as int. Unlike some other programming languages, Python guesses the type of the variable from the value that we assign to it, so we do not need to specify the type of the variable explicitly. For example,\n\nIntegers, type int: my_int = 1\nFloat type float: my_float = 25.5\nStrings, type str: my_string = 'Hello'\n\nNote that the names my_int, my_float and my_string are arbitrary here. While it is useful to name your variables so that the names contain some information about what they are used for, this code would be functionally identical to if we had used x, xrtqp2 and my_very_long_variable_name, respectively.\n\nmy_int=5\nprint(my_int)\ncountry=\"Canada\"\nprint(country)\n\n5\nCanada\n\n\nWe mentioned before that variables can change value. Let’s take a look at how this works, and also introduce a few more Python operations:\n\nx = 4\nprint(x)\ny = 2\nx = y + x\nprint(x)\n\n4\n6\n\n\nAgain, if you’re used to syntax from mathematics “x = y + x” might look very wrong at first glance. However, it simply means “throw out the existing value of x, and assign a new value which is calculated as the sum of y and x”. Here, we can see that the value of x changes, demonstrating why we call them “variables”.\nWe can also use more operators than just + and -. We use * for multiplication, / for division, and ** for power. The standard order of operations rules from mathematics also applies and parentheses () can be used to override these.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#data-structures",
    "href": "Case_1.html#data-structures",
    "title": "1  Python basics",
    "section": "1.6 Data structures",
    "text": "1.6 Data structures\nA data structure is a data/value type which organizes and stores multiple values. These are more complicated data types that comprise many single pieces of data, organized in a specific way. Examples include dictionaries, arrays, lists, stacks, queues, trees, and graphs. Each type of data structure is designed to handle specific scenarios.\n\nAs before, we use =, the assignment operator, to assign a value to the variable.\n\n\n1.6.1 Dictionaries\n\nPython’s dictionary type stores a mapping of key-value pairs that allow users to quickly access information for a particular key.\nBy specifying a key, the user can return the value corresponding to the given key. Python’s syntax for dictionaries uses curly braces {}:\n\nSyntax for creation:\nuser_dictionary = {'Key1': Value1, 'Key2': Value2, 'Key3': Value3}\nNotes:\nIn Python, the dictionary type has built-in methods to access the dictionary keys and values.\n\nThese methods are called by typing .keys() or .values() after the dictionary object.\nWe will change the return type of calling .keys() and .values() to a list by using the list() method. Below when we print the unconverted keys, the first thing you see is dict_keys, indicating the type of the data. Convert it to a list which is a simpler and more common data type. We can do this by passing the data into the list(...) function.\n\nExample:\n\n# Creating a dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n# Accessing values\nprint(person[\"name\"])  # Output: Alice\nprint(person[\"age\"])   # Output: 30\nprint(person[\"city\"])  # Output: New York\n\n# Adding a new key-value pair\nperson[\"job\"] = \"Engineer\"\n\n# Updating an existing value\nperson[\"age\"] = 31\n\n# Deleting a key-value pair\ndel person[\"city\"]\n\n# Printing the updated dictionary\nprint(person)\n\n# Getting all keys\nkeys = person.keys()\n\n# Converting to list\nkeys_list=list(keys)\nprint(keys)  # Output: dict_keys(['name', 'age', 'job'])\nprint(keys_list) # Output: type list\n\n# Getting all values\nvalues = person.values()\nprint(values)  # Output: dict_values(['Alice', 31, 'Engineer'])\n\n# Converting to list\nvalues_list=list(values)\nprint(values_list) # Output: type list\n\n\nprint(type(values_list))\nprint(type(list(values)))\n\n\n# Creating a nested dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"address\": {\n        \"city\": \"New York\",\n        \"zipcode\": \"10001\"\n    }\n}\n\n# Accessing elements in a nested dictionary\nprint(person[\"address\"][\"city\"])    # Output: New York\nprint(person[\"address\"][\"zipcode\"]) # Output: 10001\n\nAlice\n30\nNew York\n{'name': 'Alice', 'age': 31, 'job': 'Engineer'}\ndict_keys(['name', 'age', 'job'])\n['name', 'age', 'job']\ndict_values(['Alice', 31, 'Engineer'])\n['Alice', 31, 'Engineer']\n&lt;class 'list'&gt;\n&lt;class 'list'&gt;\nNew York\n10001\n\n\n\n\n1.6.2 Lists\nA list is an incredibly useful data structure in Python that can store any number of Python objects. Lists are denoted by the use of square brackets []:\nSyntax:\nuser_list = [Value1, Value2, Value3]\nExample:\n\n# Creating a list\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n\n# Adding a new element\nfruits.append(\"orange\")\n\n# Updating an existing element\nfruits[1] = \"blueberry\"\n\n# Deleting an element\ndel fruits[2]\n\n# Printing the updated list\nprint(fruits)\n\n# Getting the length\nprint(len(fruits))\n\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n\n\n\n# Accessing elements\nprint(fruits[0])  # Output: apple\nprint(fruits[1])  # Output: banana\nprint(fruits[2])  # Output: cherry\n\n# Accessing elements by negative index\nprint(fruits[-1])  # Output: date\nprint(fruits[-2])  # Output: cherry\nprint(fruits[-3])  # Output: banana\nprint(fruits[-4])  # Output: apple\n\n\n\n# Accessing a range of elements\nprint(fruits[1:3])  # Output: ['banana', 'cherry']\nprint(fruits[:2])   # Output: ['apple', 'banana']\nprint(fruits[2:])   # Output: ['cherry', 'date']\nprint(fruits[:])    # Output: ['apple', 'banana', 'cherry', 'date']\n\n# Creating a nested list\nnested_list = [[\"apple\", \"banana\"], [\"cherry\", \"date\"]]\n\n# Accessing elements in a nested list\nprint(nested_list[0][0])  # Output: apple\nprint(nested_list[0][1])  # Output: banana\nprint(nested_list[1][0])  # Output: cherry\nprint(nested_list[1][1])  # Output: date\n\n['apple', 'blueberry', 'date', 'orange']\n4\napple\nbanana\ncherry\ndate\ncherry\nbanana\napple\n['banana', 'cherry']\n['apple', 'banana']\n['cherry', 'date']\n['apple', 'banana', 'cherry', 'date']\napple\nbanana\ncherry\ndate\n\n\nList computations example:\n\n# Creating a list of numbers\nnumbers = [3.5, 1.2, 6.8, 2.4, 5.1]\n\n# Finding the minimum value\nmin_value = min(numbers)\nprint(\"Minimum value:\", min_value)  # Output: Minimum value: 1.2\n\n# Finding the maximum value\nmax_value = max(numbers)\nprint(\"Maximum value:\", max_value)  # Output: Maximum value: 6.8\n\n# Summing all elements in the list\ntotal_sum = sum(numbers)\nprint(\"Sum of all elements:\", total_sum)  # Output: Sum of all elements: 19.0\n\n# A more complicated example - this is a list comprehension - more on this later. \n# Rounding each element to the nearest integer\nrounded_numbers = [round(num) for num in numbers]\nprint(\"Rounded numbers:\", rounded_numbers)  # Output: Rounded numbers: [4, 1, 7, 2, 5]\n\nMinimum value: 1.2\nMaximum value: 6.8\nSum of all elements: 19.0\nRounded numbers: [4, 1, 7, 2, 5]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#the-in-operator",
    "href": "Case_1.html#the-in-operator",
    "title": "1  Python basics",
    "section": "1.7 The in operator",
    "text": "1.7 The in operator\nThe in operator in Python is used to check for the presence of an element within a collection, such as a list, tuple, set, or dictionary. (Tuples and sets are other data structures we have not learnt about yet.) When used with lists or other sequences, it checks if a specific value is contained in the sequence and returns True if it is, and False otherwise. When used with dictionaries, the in operator checks for the presence of a specified key. If the key exists in the dictionary, it returns True; otherwise, it returns False. This operator provides a simple and readable way to perform membership tests in various data structures.\nExample:\n\n# Creating a dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n# Using the 'in' operator to check for a key\nprint(\"name\" in person)  # Output: True\nprint(\"job\" in person)   # Output: False\n\n# Using the 'in' operator to check for a value\nprint(\"Alice\" in person.values())  # Output: True\nprint(\"Engineer\" in person.values())  # Output: False\n\n\n# Creating a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Using the 'in' operator to check for an element\nprint(\"banana\" in fruits)  # Output: True\nprint(\"orange\" in fruits)  # Output: False\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#comments-and-debugging",
    "href": "Case_1.html#comments-and-debugging",
    "title": "1  Python basics",
    "section": "1.8 Comments and debugging",
    "text": "1.8 Comments and debugging\n\nComments are lines of code which begin with the # symbol. Nothing happens when you run these lines. Their purpose is to describe the code you have written, especially if it would be unclear to someone else reading it. You should be commenting your code as you go along. For example “these lines compute the average” or “These lines remove missing data” etc.\nWe have seen that programs may have errors or bugs. The process of resolving bugs is known as debugging. Debugging can be a very frustrating activity. Please be prepared for this. Following, this Python textbook, there are three kinds of errors you may encouter:\n\nSyntax error: “Syntax” refers to the rules of the language. If there is a syntax error anywhere in your program, Python displays an error message and quits.\nRuntime error: The second type of error is a runtime error, so called because the error does not appear until after the program has started running. These errors are also called exceptions because they usually indicate that something exceptional (and bad) has happened.\nSemantic error: If there is a semantic error in your program, it will run without generating error messages, but it will not do the right thing. It will do something else. Specifically, it will do what you told it to do. Identifying semantic errors can be tricky because it requires you to work backward by looking at the output of the program and trying to figure out what it is doing. Try runnning your code one line at a time and ensuring each line is doing what you expect, using the print feature.\n\n\nDebugging means to fix errors in your program or code. Debugging is a constant and necessary part of writing code, do not be surprised if you are spending most of your time debugging. In order to debug syntax and runtime errors, the first step is to read the error message.\nFor example if you run the following:\nfruits=['apple','banana','orange']\nfruits[3]\nYou will get the following error message:\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile \n      1 fruits=['apple','banana','orange']\n----&gt; 2 fruits[3]\n\nIndexError: list index out of range\"\n\nYou can see that there is an error pointing to the second line fruits[3]. That is the line where the program encountered an error. The error it encountered is displayed at the bottom. “IndexError: list index out of range.”\nWe can use the error for clues as to what could be going wrong. In this case, out of range means the index is too large. Inspecting the code further, you can see that Python indexing starts at 0, and so to access the third element of the list, we use the index 2.\nIf you do not understand the error message, you should Google search the error message to find out what it means. You can also use tools such as StackOverflow threads and ChatGPT to help you debug your code. When you do not know what is causing the error, it can be helpful to go through the code line by line, printing the output of each variable as you go, to see where the program is going wrong.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#control-flow-elements",
    "href": "Case_1.html#control-flow-elements",
    "title": "1  Python basics",
    "section": "1.9 Control flow elements",
    "text": "1.9 Control flow elements\n\n1.9.1 For loops\nOne control flow element in Python is the for loop.\n\nThe for loop allows one to execute the same statements over and over again (i.e. looping).\nThis saves a significant amount of time coding repetitive tasks and aids in code readability.\n\nSyntax:\nfor iterator_variable in some_sequence:\n    statements(s)\nThe for loop iterates over some_sequence and performs statements(s) at each iteration.\n\nThat is, at each iteration the iterator_variable is updated to the next value in some_sequence.\nAs a concrete example, consider the loop:\n\nExample:\n\nfor i in [1,2,3,4]:\n    print(i*i)\n\n1\n4\n9\n16\n\n\n\nHere, the for loop will print to the screen four times; that is it will print 1 on the first iteration of the loop, 4 on the second iteration, 9 on the third, and 16 on the fourth.\nHence, the for loop statement will iterate over all the elements of the list [1,2,3,4], and at each iteration it updates the iterator variable i to the next value in the list [1,2,3,4].\nIn for loops, it is an extremely good habit to choose an iterator variable that provides context rather than a random letter.\nIn this case, we will use both to get you accustomed to both.\nThis is because you will see both throughout the course of your data science career, but we encourage you to not use a generic name like i whenever possible for ease of communication.\n\n\n\n1.9.2 List comprehensions\nA list comprehension is a concise way to create a new list by applying an expression to each element of an existing list while optionally filtering elements based on a condition. It combines loops and conditional statements into a single line of code, making it efficient and readable for creating lists with specific transformations or filters. It can be used to replace a for loop with shorter code.\nExample:\n\n# Using a for loop\nnumbers = [1, 2, 3, 4, 5]\nsquared_numbers = []\nfor num in numbers:\n    squared_numbers.append(num ** 2)\nprint(\"Squared numbers (using for loop):\", squared_numbers)\n\n# Using list comprehension\nsquared_numbers = [num ** 2 for num in numbers]\nprint(\"Squared numbers (using list comprehension):\", squared_numbers)\n\n\n# Using a for loop\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\neven_numbers = []\nfor num in numbers:\n    if num % 2 == 0:\n        even_numbers.append(num)\nprint(\"Even numbers (using for loop):\", even_numbers)\n\n# Using list comprehension\neven_numbers = [num for num in numbers if num % 2 == 0]\nprint(\"Even numbers (using list comprehension):\", even_numbers)\n\nSquared numbers (using for loop): [1, 4, 9, 16, 25]\nSquared numbers (using list comprehension): [1, 4, 9, 16, 25]\nEven numbers (using for loop): [2, 4, 6, 8, 10]\nEven numbers (using list comprehension): [2, 4, 6, 8, 10]\n\n\n\n\n1.9.3 If statements and booleans\nA boolean is a data type that represents one of two possible states: True or False. Booleans are crucial for controlling the flow of programs, making decisions, and executing code based on specific conditions being met or not. Booleans are used extensively for making decisions and comparisons. They are often the result of logical operations, such as comparisons (e.g., greater than, less than) or boolean operations (e.g., and, or, not).\nAs stated above, a boolean object can take on two values:\n\nTrue: Represents a condition that is considered true or valid.\nFalse: Represents a condition that is considered false or invalid.\n\nIf statements are conditional statements that allow you to execute certain blocks of code based on specified conditions. They form the foundation of decision-making in code, enabling programs to make choices and take different actions depending on whether certain conditions are True or False.\nSyntax:\nif test_expression_1:\n    block1_statement(s)\nelif test_expression_2:\n    block2_statement2(s)\nelse:\n    block3_statement(s)\nExample:\n\n# Example 0: A boolean\nx=2\ny=7\nprint(True)\nprint(x==y)\nprint(x &gt; 5)\n\n\n# Example 1: Simple if statement\nx = 10\n\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\n# Example 2: if-else statement\ny = 3\n\nif y % 2 == 0:\n    print(\"y is even\")\nelse:\n    print(\"y is odd\")\n\n# Example 3: if-elif-else statement\ngrade = 75\n\nif grade &gt;= 90:\n    print(\"Grade is A\")\nelif grade &gt;= 80:\n    print(\"Grade is B\")\nelif grade &gt;= 70:\n    print(\"Grade is C\")\nelif grade &gt;= 60:\n    print(\"Grade is D\")\nelse:\n    print(\"Grade is F\")\n\nTrue\nFalse\nFalse\nx is greater than 5\ny is odd\nGrade is C\n\n\nExplanation:\nExample 0: Creates different types of boolean variables.\nExample 1: Checks if x is greater than 5. If true, it prints “x is greater than 5”.\nExample 2: Checks if y is even (remainder of division by 2 is zero). If true, it prints “y is even”; otherwise, it prints “y is odd”.\nExample 3: Evaluates the value of grade and prints a corresponding grade based on the ranges specified using if, elif (else if), and else statements. This demonstrates chaining conditions to determine a grade based on numerical thresholds.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#string-formatting",
    "href": "Case_1.html#string-formatting",
    "title": "1  Python basics",
    "section": "1.10 String formatting",
    "text": "1.10 String formatting\nString formatting refers to the various techniques used to insert dynamic values into strings in a controlled and formatted manner. We cover .format and f-strings.\n\n1.10.1 f-strings (Formatted String Literals):\nSyntax:\nf\"some text {expression1} more text {expression2} ...\"\n\nf prefix before the string indicates it’s an f-string.\n{expression} inside curly braces {} evaluates expression and inserts its value into the string.\nYou can directly embed Python expressions, variables, or function calls inside {}.\n\n\n\n1.10.2 .format() Method:\nSyntax:\n\"some text {} more text {}\".format(value1, value2)\n\n{} acts as placeholders in the string.\n.format() method is called on a string object, and values passed to it replace corresponding {} in the string.\nYou can specify the order of substitution using {0}, {1}, etc., or use named placeholders {name}, {age}.\n\nDifferences: - f-strings are more concise and readable. - .format() method offers more flexibility, such as specifying formatting options or reusing values.\nExamples:\n\n# Example using .format() method\nname = \"Bob\"\nage = 25\nformatted_string = \"Hello, {}! You are {} years old.\".format(name, age)\nprint(formatted_string)\n\n\n# Example using f-strings\nname = \"Charlie\"\nage = 35\nformatted_string = f\"Hello, {name}! You are {age} years old.\"\nprint(formatted_string)\n\nHello, Bob! You are 25 years old.\nHello, Charlie! You are 35 years old.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_2.html",
    "href": "Case_2.html",
    "title": "2  Data extraction and transformation",
    "section": "",
    "text": "2.1 Installing and importing packages\nExternal libraries (a.k.a. packages) are code bases that contain a variety of pre-written functions and tools. This allows you to perform a variety of complex tasks in Python without having to “reinvent the wheel”, i.e., build everything from the ground up. We will use two core packages: pandas and numpy.\npandas is an external library that provides functionality for data analysis. Pandas specifically offers a variety of data structures and data manipulation methods that allow you to perform complex tasks with simple, one-line commands.\nnumpy is a external library that offers numerous mathematical operations. We will use numpy later in the case. Together, pandas and numpy allow you to create a data science workflow within Python. numpy is in many ways foundational to pandas, providing vectorized operations, while pandas provides higher level abstractions built on top of numpy.\nBefore you use a module/package/library, it must be installed. Note that you only need to install each module/package/library once per machine. The syntax for installing a module/package/library on your machine will be either:\nor\nFor example, you can run one of the following commands in a code cell to install the package pandas.\nBefore using a package in each notebook or session, it must be imported. Unlike installation, importing must be done every time you use python. Let’s import both packages using the import keyword. We will rename pandas to pd and numpy to np using the as keyword. This allows us to use the short name abbreviation when we want to reference any function that is inside either package. The abbreviations we chose are standard across the data science industry and should be followed unless there is a very good reason not to.\n# Import the Pandas package\nimport pandas as pd\n\n# Import the NumPy package\nimport numpy as np",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#installing-and-importing-packages",
    "href": "Case_2.html#installing-and-importing-packages",
    "title": "2  Data extraction and transformation",
    "section": "",
    "text": "!pip install package name\n\n!conda install package name\n\n# If your machine uses pip\n!pip install pandas\n# If your machine uses Anaconda\n!conda install pandas",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#fundamentals-of-pandas",
    "href": "Case_2.html#fundamentals-of-pandas",
    "title": "2  Data extraction and transformation",
    "section": "2.2 Fundamentals of pandas",
    "text": "2.2 Fundamentals of pandas\n\n2.2.1 Series and DataFrame value types\npandas is a Python library that facilitates a wide range of data analysis and manipulation. Before, you saw basic data structures in Python such as lists and dictionaries. While you can build a basic data table (similar to an Excel spreadsheet) using nested lists in Python, they get quite difficult to work with. By contrast, in pandas the table data structure, known as the DataFrame, is a first-class citizen. It allows us to easily manipulate data by thinking of data in terms of rows and columns.\nIf you’ve ever used or heard of R or SQL before, pandas brings some functionality from each of these to Python, allowing you to structure and filter data more efficiently than pure Python. This efficiency is seen in two distinct ways:\n\nScripts written using pandas will often run faster than scripts written in pure Python\nScripts written using pandas will often contain far fewer lines of code than the equivalent script written in pure Python.\n\nAt the core of the pandas library are two fundamental data structures/objects:\n\nSeries\nDataFrame\n\nA Series object stores single-column data along with an index. An index is just a way of “numbering” the Series object. For example, in Case 2, the indices were dates, while the single-column data was stock prices or daily trading volume.\nA DataFrame object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index - the index of the DataFrame object.\nBelow is the syntax for creating a Series object, followed by the syntax for creating a DataFrame object. Note that DataFrame objects can also have a single-column – think of this as a DataFrame consisting of a single Series object:\n\nSeries: A one-dimensional labeled array capable of holding data of any type (integer, string, float, etc.). Created using pd.Series(data, index=index), where data can be a list, dictionary, or scalar value.\nDataFrame: A two-dimensional labeled data structure with columns of potentially different types. Created using pd.DataFrame(data, index=index, columns=columns), where data can be a dictionary of lists, list of dictionaries, or 2D array-like object.\n\nExample:\n\n# Create a simple Series object\nsimple_series = pd.Series(\n    index=[0, 1, 2, 3], name=\"Volume\", data=[1000, 2600, 1524, 98000]\n)\nsimple_series\n\n\n# Create a simple DataFrame object\nsimple_df = pd.DataFrame(\n    index=[0, 1, 2, 3], columns=[\"Volume\"], data=[1000, 2600, 1524, 98000]\n)\nsimple_df\n\n\n\n\n\n\n\n\nVolume\n\n\n\n\n0\n1000\n\n\n1\n2600\n\n\n2\n1524\n\n\n3\n98000\n\n\n\n\n\n\n\nDataFrame objects are more general than Series objects, and one DataFrame can hold many Series objects, each as a different column. Let’s create a two-column DataFrame object:\n\n# Create another DataFrame object\nanother_df = pd.DataFrame(\n    index=[0, 1, 2, 3],\n    columns=[\"Date\", \"Volume\"],\n    data=[[20190101, 1000], [20190102, 2600], [20190103, 1524], [20190104, 98000]]\n)\nanother_df\n\n\n\n\n\n\n\n\nDate\nVolume\n\n\n\n\n0\n20190101\n1000\n\n\n1\n20190102\n2600\n\n\n2\n20190103\n1524\n\n\n3\n20190104\n98000\n\n\n\n\n\n\n\nNotice how a list of lists was used to specify the data in the another_df DataFrame. Each element of the outer list corresponds to a row in the DataFrame, so the outer list has 4 elements because there are 4 indices. Each element of the each inner list has 2 elements because the DataFrame has two columns.\n\n\n2.2.2 Reading in data\npandas allows easy loading of CSV files through the use of the method pd.read_csv().\nSyntax:\ndf = pd.read_csv(File name with path as a string)\nBefore loading the CSV file, you need to specify its location on your computer. The file path is the address that tells Python where to find the file. You can use one of the following ways to specify the location\n\nAbsolute Path: This is the complete path to the file starting from the root directory (e.g., C:/Users/username/Documents/data.csv or /Users/YourUsername/Documents/data.csv).\nRelative Path: This is the path relative to the current working directory where your Python script or Jupyter notebook is located (e.g., data/data.csv). If your CSV file is in the same directory as your Python script or Jupyter notebook, you can just provide the file name.\n\nExamples:\n# Load a CSV file as a DataFrame and assign to df\n# Same folder: Here D.csv is in the same folder as my notebook\ndf = pd.read_csv(\"D.csv\")\n\n# Relative path: Here D.csv is in a folder called data, and the folder data is in the same folder as my notebook\ndf = pd.read_csv(\"data/D.csv\")\n\n# Absolute path: Here the full path starting from my hardrive, C:, to the file D.csv is stated. \n# On mac, it will look like: /Users/YourUsername/Documents/data.csv\ndf = pd.read_csv(\"C:\\Users\\Teaching\\Courses\\Math1130\\D.csv\")\nTo find out which folder your relative path starts from, use the command getcwd() from the os module.\n\nimport os\n\n# Get the current working directory\ncurrent_directory = os.getcwd()\n\n# Print the current working directory\nprint(\"Current Directory:\", current_directory)\n\n\n2.2.3 Basic commands for DataFrames\nThere are several common methods and attributes that allow one to take a peek at the data and get a sense of it:\n\nDataFrame.head() -&gt; returns the column names and first 5 rows by default\nDataFrame.tail() -&gt; returns the column names and last 5 rows by default\nDataFrame.shape -&gt; returns (num_rows, num_columns)\nDataFrame.columns -&gt; returns index of columns\nDataFrame.index -&gt; returns index of rows\n\nIn your spare time please check the pandas documentation and explore the parameters of these methods as well as other methods. Familiarity with this library will dramatically improve your productivity as a data scientist.\nUsing df.head() and df.tail() we can take a look at the data contents. Unless specified otherwise, Series and DataFrame objects have indices starting at 0 and increase monotonically upward along the integers.\nExample:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\n\n# 1. DataFrame.head()\nprint(\"DataFrame.head():\")\nprint(df.head())\nprint()  # Blank line for separation\n\n# 2. DataFrame.tail()\nprint(\"DataFrame.tail():\")\nprint(df.tail())\nprint()  # Blank line for separation\n\n# 3. DataFrame.shape\nprint(\"DataFrame.shape:\")\nprint(df.shape)  # Output: (5, 3) - 5 rows, 3 columns\nprint()  # Blank line for separation\n\n# 4. DataFrame.columns\nprint(\"DataFrame.columns:\")\nprint(df.columns)  # Output: Index(['Name', 'Age', 'City'], dtype='object')\nprint()  # Blank line for separation\n\n# 5. DataFrame.index\nprint(\"DataFrame.index:\")\nprint(df.index)  # Output: RangeIndex(start=0, stop=5, step=1)\nprint()  # Blank line for separation\n\n# Attributes\n# 1. shape attribute\nprint(\"df.shape attribute:\", df.shape)  # Output: (5, 3) - 5 rows, 3 columns\n\n# 2. columns attribute\nprint(\"df.columns attribute:\", df.columns)  # Output: Index(['Name', 'Age', 'City'], dtype='object')\n\n# 3. index attribute\nprint(\"df.index attribute:\", df.index)  # Output: RangeIndex(start=0, stop=5, step=1)\n\nDataFrame.head():\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nDataFrame.tail():\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nDataFrame.shape:\n(5, 3)\n\nDataFrame.columns:\nIndex(['Name', 'Age', 'City'], dtype='object')\n\nDataFrame.index:\nRangeIndex(start=0, stop=5, step=1)\n\ndf.shape attribute: (5, 3)\ndf.columns attribute: Index(['Name', 'Age', 'City'], dtype='object')\ndf.index attribute: RangeIndex(start=0, stop=5, step=1)\n\n\n\n\n2.2.4 Creating new columns and variables\nWe can create new columns by adding new columns to the DataFrame or creating a new column based on existing columns:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\n\n# 1. Adding new columns to the DataFrame\ndf['Gender'] = ['Female', 'Male', 'Male', 'Male', 'Female']\ndf['Salary'] = [50000, 60000, 75000, 80000, 70000]\n\nprint(\"DataFrame with new columns:\")\nprint(df)\nprint()  # Blank line for separation\n\n# 2. Creating a new column based on existing ones\ndf['Age_Squared'] = df['Age']**df['Age']\n\nprint(\"DataFrame with new 'Age_Squared' column:\")\nprint(df)\n\n# 3. We can also create columns based on multiple, other columns\ndf['Salary_over_Age'] = df['Salary']/df['Age']\n\nprint(\"DataFrame with new 'Salary_over_Age' column:\")\nprint(df)\n\nDataFrame with new columns:\n      Name  Age         City  Gender  Salary\n0    Alice   25     New York  Female   50000\n1      Bob   30  Los Angeles    Male   60000\n2  Charlie   35      Chicago    Male   75000\n3    David   40      Houston    Male   80000\n4      Eve   45        Miami  Female   70000\n\nDataFrame with new 'Age_Squared' column:\n      Name  Age         City  Gender  Salary          Age_Squared\n0    Alice   25     New York  Female   50000 -6776596920136667815\n1      Bob   30  Los Angeles    Male   60000  2565992168703393792\n2  Charlie   35      Chicago    Male   75000  8407224849895527163\n3    David   40      Houston    Male   80000                    0\n4      Eve   45        Miami  Female   70000  2604998672350111773\nDataFrame with new 'Salary_over_Age' column:\n      Name  Age         City  Gender  Salary          Age_Squared  \\\n0    Alice   25     New York  Female   50000 -6776596920136667815   \n1      Bob   30  Los Angeles    Male   60000  2565992168703393792   \n2  Charlie   35      Chicago    Male   75000  8407224849895527163   \n3    David   40      Houston    Male   80000                    0   \n4      Eve   45        Miami  Female   70000  2604998672350111773   \n\n   Salary_over_Age  \n0      2000.000000  \n1      2000.000000  \n2      2142.857143  \n3      2000.000000  \n4      1555.555556  \n\n\nHere we see the power of pandas. We can simply perform mathematical operations on columns of DataFrames just as if the DataFrames were single variables themselves.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#distributions-and-summary-statistics",
    "href": "Case_2.html#distributions-and-summary-statistics",
    "title": "2  Data extraction and transformation",
    "section": "2.3 Distributions and summary statistics",
    "text": "2.3 Distributions and summary statistics\nA common first step in data analysis is to learn about the characteristics or distribution of each of the relevant columns.\n\n2.3.1 Summary statistics\nSummary statistics are numerical measures that describe important aspects of a column in a dataset. They provide a concise overview of the data’s characteristics without needing to examine each individual value.\n\nExamples of Summary Statistics:\n\nMean: The average value of all data points.\nMedian: The middle value in a sorted list of numbers.\nMode: The most frequently occurring value.\nMax and Minimum: The maximum and minimum values in a column.\nRange: The difference between the maximum and minimum values.\nStandard Deviation: A measure of the amount of variation or dispersion in a set of values. The standard deviation is the square root of the average of the squared distances between the data points and the the mean of the column.\nPercentiles: Values below which a given percentage of observations fall.\n\n\nFor now, we can think of the distribution of a data column as a description of various aspects of that column. The distribution can be described through summary statistics, or as we will see later, through plots.\n\n\n2.3.2 Standard deviation:\nStandard Deviation is a measure of how spread out or dispersed the values in a dataset are from the mean (average) of the dataset. It tells you how much the individual data points typically differ from the mean value.\n\nSmall Standard Deviation:\n\nWhat it means: Most of the data points are close to the mean.\nExample: If the standard deviation of test scores in a class is small, it means most students scored close to the average score. There is less variability in scores.\n\nLarge Standard Deviation:\n\nWhat it means: The data points are spread out over a wider range of values.\nExample: If the standard deviation of test scores in a class is large, it means students’ scores vary widely from the average. Some students scored much higher or lower than the average.\n\nZero Standard Deviation:\n\nWhat it means: All data points are exactly the same.\nExample: If every student in a class scored the same on a test, the standard deviation would be zero, indicating no variability.\n\n\nImagine you have two sets of data representing the ages of two different groups of people.\n\nGroup 1: Ages are [25, 26, 25, 24, 25].\n\nThe mean age is 25.\nThe standard deviation is small because all ages are very close to the mean.\n\nGroup 2: Ages are [20, 30, 25, 40, 10].\n\nThe mean age is also 25.\nThe standard deviation is large because the ages are spread out over a wide range (from 10 to 40).\n\n\nIn summary, the standard deviation helps you understand the variability of your data. A smaller standard deviation indicates data points are close to the mean, while a larger standard deviation indicates data points are more spread out. This information is useful for comparing datasets, understanding data consistency, and identifying outliers.\n\n\n2.3.3 Summary statistics example:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 30, 40, 35],\n    'Salary': [50000, 60000, 75000, 80000, 70000]\n}\n\ndf = pd.DataFrame(data)\n\n\n# Mean: The average value of all data points\nmean_value = df['Age'].mean()\nprint(f\"Mean: {mean_value}\")\n\n# Median: The middle value in a sorted list of numbers\nmedian_value = df['Age'].median()\nprint(f\"Median: {median_value}\")\n\n# Mode: The most frequently occurring value\nmode_value = df['Age'].mode()\nprint(f\"Mode: {mode_value.values}\")\n\n# Max and Minimum: The maximum and minimum values in a column\nmax_value = df['Age'].max()\nmin_value = df['Age'].min()\nprint(f\"Max: {max_value}\")\nprint(f\"Min: {min_value}\")\n\n# Range: The difference between the maximum and minimum values\nrange_value = max_value - min_value\nprint(f\"Range: {range_value}\")\n\n# Standard Deviation: A measure of the amount of variation or dispersion in a set of values\nstd_dev = df['Age'].std()\nprint(f\"Standard Deviation: {std_dev}\")\n\n# Percentiles: Values below which a given percentage of observations fall\npercentile_25 = df['Age'].quantile(0.25)\npercentile_50 = df['Age'].quantile(0.50)\npercentile_75 = df['Age'].quantile(0.75)\nprint(f\"25th Percentile: {percentile_25}\")\nprint(f\"50th Percentile: {percentile_50}\")\nprint(f\"75th Percentile: {percentile_75}\")\n\n\n# Describe:\ndf['Name'].describe()\ndf['Age'].describe()\ndf.describe()\n\nMean: 32.0\nMedian: 30.0\nMode: [30]\nMax: 40\nMin: 25\nRange: 15\nStandard Deviation: 5.70087712549569\n25th Percentile: 30.0\n50th Percentile: 30.0\n75th Percentile: 35.0\n\n\n\n\n\n\n\n\n\nAge\nSalary\n\n\n\n\ncount\n5.000000\n5.000000\n\n\nmean\n32.000000\n67000.000000\n\n\nstd\n5.700877\n12041.594579\n\n\nmin\n25.000000\n50000.000000\n\n\n25%\n30.000000\n60000.000000\n\n\n50%\n30.000000\n70000.000000\n\n\n75%\n35.000000\n75000.000000\n\n\nmax\n40.000000\n80000.000000\n\n\n\n\n\n\n\nIn addition to describe, there is a value_counts() method for checking the frequency of elements in categorical data. When applied to a DataFrame class, value_counts() will return the frequency of each row in the DataFrame. In other words, for each unique row it returns how many instances of that row are in the DataFrame. When applied to a Series class value_counts() will return the frequency of each unique value in the given Series class:\n\ndict_data = {\n    \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8,1],\n    \"color\": [\"red\", \"red\", \"red\", \"blue\", \"blue\", \"green\", \"blue\", \"green\",\"red\"],\n}\ncategory_df = pd.DataFrame(data=dict_data)\n\ncategory_df\n\n#Gives the frquency of each unique row in the DataFrame\ncategory_df.value_counts()\n\n#Gives the frquency of each unique value in the Series\ncategory_df['color'].value_counts()\n\ncolor\nred      4\nblue     3\ngreen    2\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#more-on-pandas",
    "href": "Case_2.html#more-on-pandas",
    "title": "2  Data extraction and transformation",
    "section": "2.4 More on pandas",
    "text": "2.4 More on pandas\n\n2.4.1 Aggregating DataFrames\nOne way to combined multiple DataFrames is through the use of the pd.concat() method from pandas. We can input a list of DataFrames into pd.concat() that we’d like to concatenate. The pd.concat() function is used to concatenate (combine) two or more DataFrames or Series along a particular axis (rows or columns).\nExample:\n\n# 1\n\n# Create two example DataFrames\ndata1 = {\n    'Name': ['Alice', 'Bob'],\n    'Age': [25, 30],\n    'City': ['New York', 'Los Angeles']\n}\ndf1 = pd.DataFrame(data1)\n\ndata2 = {\n    'Name': ['Charlie', 'David'],\n    'Age': [35, 40],\n    'City': ['Chicago', 'Houston']\n}\ndf2 = pd.DataFrame(data2)\n\n# Concatenate the two DataFrames\nresult = pd.concat([df1, df2], ignore_index=True)\n\nprint(\"Concatenated DataFrame:\")\nprint(result)\n\nConcatenated DataFrame:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\nExplanation: Two DataFrames (df1 and df2) are created with identical columns. The pd.concat([df1, df2]) call concatenates df1 and df2 along the rows (default behavior). The ignore_index=True argument reindexes the resulting DataFrame to have a continuous index.\nExample:\n\n# 2\n\n# Create two example DataFrames\ndata1 = {\n    'Name': ['Alice', 'Bob'],\n    'Age': [25, 30]\n}\ndf1 = pd.DataFrame(data1)\n\ndata2 = {\n    'City': ['New York', 'Los Angeles'],\n    'Salary': [50000, 60000]\n}\ndf2 = pd.DataFrame(data2)\n\n# Concatenate the two DataFrames along columns\nresult = pd.concat([df1, df2], axis=1)\n\nprint(\"Concatenated DataFrame along columns:\")\nprint(result)\n\nConcatenated DataFrame along columns:\n    Name  Age         City  Salary\n0  Alice   25     New York   50000\n1    Bob   30  Los Angeles   60000\n\n\nExplanation: pd.concat([df1, df2], axis=1) concatenates df1 and df2 along the columns, resulting in a DataFrame that combines the columns of both input DataFrames. Using pd.concat(), you can easily combine multiple DataFrames or Series into a single DataFrame, which is useful for data manipulation and analysis tasks.\n\n\n2.4.2 Filtering DataFrames\nFiltering a DataFrame means selecting rows that meet certain criteria. This is often done using conditions on one or more columns. Here’s a simple example to illustrate how filtering works:\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Filter rows where Age is greater than 30\nfiltered_df = df[df['Age'] &gt; 30]\n\nprint(\"\\nFiltered DataFrame (Age &gt; 30):\")\nprint(filtered_df)\n\n# Filter rows where City is 'Chicago'\nfiltered_df_city = df[df['City'] == 'Chicago']\n\nprint(\"\\nFiltered DataFrame (City is Chicago):\")\nprint(filtered_df_city)\n\n\n# Filter rows where Age is between 30 and 40 (inclusive)\nfiltered_df_age_range = df[(df['Age'] &gt;= 30) & (df['Age'] &lt;= 40)]\n\nprint(\"\\nFiltered DataFrame (30 &lt;= Age &lt;= 40):\")\nprint(filtered_df_age_range)\n\nOriginal DataFrame:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nFiltered DataFrame (Age &gt; 30):\n      Name  Age     City\n2  Charlie   35  Chicago\n3    David   40  Houston\n4      Eve   45    Miami\n\nFiltered DataFrame (City is Chicago):\n      Name  Age     City\n2  Charlie   35  Chicago\n\nFiltered DataFrame (30 &lt;= Age &lt;= 40):\n      Name  Age         City\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\nExplanation:\n\ndf[‘Age’] &gt; 30: This creates a boolean Series that is True for rows where the ‘Age’ value is greater than 30 and False otherwise.\ndf[df[‘Age’] &gt; 30]: This filters the DataFrame, returning only the rows where the condition is True.\ndf[‘City’] == ‘Chicago’: This creates a boolean Series that is True for rows where the ‘City’ value is ‘Chicago’.\ndf[(df[‘Age’] &gt;= 30) & (df[‘Age’] &lt;= 40)]: This filters the DataFrame using multiple conditions. The & operator is used to combine conditions, ensuring both conditions must be True for a row to be included.\n\nThis example demonstrates how to filter a pandas DataFrame based on different conditions, helping you to extract specific subsets of data that meet your criteria.\n\n\n2.4.3 Sorting\nThe sort_values() method in pandas is used to sort a DataFrame or Series by one or more columns or indices.\nSyntax:\nDataFrame.sort_values(by, axis=0, ascending=True,  na_position='last', ignore_index=False)\nParameters:\n\nby: (str or list of str) The name(s) of the column(s) or index level(s) to sort by.\naxis: (int or str, default 0) The axis to sort along. 0 or ‘index’ to sort rows, 1 or ‘columns’ to sort columns.\nascending: (bool or list of bool, default True) Sort ascending vs. descending. Specify list for multiple sort orders.\nna_position: (str, default ‘last’) ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end.\nignore_index: (bool, default False) If True, the resulting index will be labeled 0, 1, …, n - 1.\n\nExample:\nLet’s create an example DataFrame and sort it using sort_values().\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'Salary': [50000, 60000, 75000, 80000, 70000]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Sort the DataFrame by 'Age' in ascending order\nsorted_df = df.sort_values(by='Age')\nprint(\"\\nDataFrame sorted by Age:\")\nprint(sorted_df)\n\n# Sort the DataFrame by 'Salary' in descending order\nsorted_df_desc = df.sort_values(by='Salary', ascending=False)\nprint(\"\\nDataFrame sorted by Salary in descending order:\")\nprint(sorted_df_desc)\n\n# Sort the DataFrame by 'Age' and then by 'Salary'\nsorted_df_multi = df.sort_values(by=['Age', 'Salary'])\nprint(\"\\nDataFrame sorted by Age and then by Salary:\")\nprint(sorted_df_multi)\n\n\n2.4.4 Groupby\npandas offers the ability to group related rows of DataFrames according to the values of other rows. This useful feature is accomplished using the groupby() method. The groupby method in pandas is used to group data based on one or more columns. It is often used with aggregation functions like sum(), mean(), count(), etc., to summarize data.\nSyntax:\nDataFrame.groupby(by, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=&lt;no_default&gt;, observed=False, dropna=True)\nParameters descriptions:\n\nby: Specifies the column(s) or keys to group by. This can be a single column name, a list of column names, or a dictionary mapping column names to group keys.\naxis: Determines whether to group by rows (axis=0, default) or columns (axis=1).\nas_index: If True (default), the group labels are used as the index. If False, the group labels are retained as columns.\nsort: If True (default), the groups are sorted. If False, the groups are not sorted.\ngroup_keys: If True (default), adds group keys to the index. If False, the group keys are not added.\n\nSyntax of common usages:\n\n# Grouping by a Single Column\ngrouped = df.groupby('column_name')\n\n\n# **Grouping by Multiple Columns**:\n\n\ngrouped = df.groupby(['column_name1', 'column_name2'])\n\n\n# **Applying Aggregation Functions**:\n\ngrouped_mean = df.groupby('column_name')['target_column'].mean()\n\n\n# **Using Multiple Aggregation Functions**:\n\n\ngrouped_agg = df.groupby('column_name').agg({\n    'target_column1': 'mean',\n    'target_column2': 'sum'\n})\nExample:\n\n# Use the groupby() method, notice a DataFrameGroupBy object is returned\ndf[['City',\"Age\"]].groupby('City').mean()\n\n\n\n\n\n\n\n\nAge\n\n\nCity\n\n\n\n\n\nChicago\n35.0\n\n\nHouston\n40.0\n\n\nLos Angeles\n30.0\n\n\nMiami\n45.0\n\n\nNew York\n25.0\n\n\n\n\n\n\n\n\nHere, the DataFrameGroupBy object can be most readily thought of as containing a DataFrame object for every group (in this case, a DataFrame object for each city).\nSpecifically, each item of the object is a tuple, containing the group identifier (in this case the city), and the corresponding rows of the DataFrame that have that city.\n\nLonger example:\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n    'Age': [25, 30, 35, 40, 45, 30],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami', 'Chicago'],\n    'Salary': [50000, 60000, 75000, 80000, 70000, 65000]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Group by 'City' and calculate the mean salary for each city\ngrouped = df.groupby('City')['Salary'].mean()\n\nprint(\"\\nMean Salary by City:\")\nprint(grouped)\n\n# Group by 'City' and calculate the sum of salaries for each city\ngrouped_sum = df.groupby('City')['Salary'].sum()\n\nprint(\"\\nSum of Salaries by City:\")\nprint(grouped_sum)\n\n# Group by 'City' and count the number of people in each city\ngrouped_count = df.groupby('City')['Name'].count()\n\nprint(\"\\nCount of People by City:\")\nprint(grouped_count)\n\nOriginal DataFrame:\n      Name  Age         City  Salary\n0    Alice   25     New York   50000\n1      Bob   30  Los Angeles   60000\n2  Charlie   35      Chicago   75000\n3    David   40      Houston   80000\n4      Eve   45        Miami   70000\n5    Frank   30      Chicago   65000\n\nMean Salary by City:\nCity\nChicago        70000.0\nHouston        80000.0\nLos Angeles    60000.0\nMiami          70000.0\nNew York       50000.0\nName: Salary, dtype: float64\n\nSum of Salaries by City:\nCity\nChicago        140000\nHouston         80000\nLos Angeles     60000\nMiami           70000\nNew York        50000\nName: Salary, dtype: int64\n\nCount of People by City:\nCity\nChicago        2\nHouston        1\nLos Angeles    1\nMiami          1\nNew York       1\nName: Name, dtype: int64\n\n\nExplanation:\n\ndf.groupby(‘City’): This groups the DataFrame by the ‘City’ column. Each unique value in ‘City’ will form a group.\n[‘Salary’].mean(): This calculates the mean salary for each group (city).\n[‘Salary’].sum(): This calculates the sum of salaries for each group (city).\n[‘Name’].count(): This counts the number of entries for each group (city).\nGrouping: df.groupby('City') creates groups based on unique values in the ‘City’ column.\nAggregation: Using aggregation functions like mean(), sum(), and count() allows you to summarize the data within each group.\nResult: The output shows the mean salary, sum of salaries, and count of people for each city, respectively.\n\nThe groupby method is very powerful for data analysis and manipulation, allowing you to easily aggregate and summarize data based on specific criteria.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#numpys-where",
    "href": "Case_2.html#numpys-where",
    "title": "2  Data extraction and transformation",
    "section": "2.5 Numpy’s where()",
    "text": "2.5 Numpy’s where()\nThe np.where function in is used to return elements chosen from two values based on whether a condition holds.\nSyntax:\nnp.where(condition, x, y)\nParameters:\n\ncondition: An array-like object (e.g., a series or NumPy array, list, or etc) that evaluates to True or False.\nx: The value to choose when the condition is True.\ny: The value to choose when the condition is False.\n\nnp.where(condition, x, y) returns an array with elements from x where the condition is True and elements from y where the condition is False.\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45]\n}\ndf = pd.DataFrame(data)\n\n# Use np.where to create a new column 'Age Group'\ndf['Age Group'] = np.where(df['Age'] &gt;= 35, 'Senior', 'Junior')\n\nprint(df)\n\n      Name  Age Age Group\n0    Alice   25    Junior\n1      Bob   30    Junior\n2  Charlie   35    Senior\n3    David   40    Senior\n4      Eve   45    Senior\n\n\nExplanation:\n\nCondition: df['Age'] &gt;= 35 checks if the ‘Age’ column values are greater than or equal to 35.\nTrue: For rows where the condition is True, it assigns ‘Senior’.\nFalse: For rows where the condition is False, it assigns ‘Junior’.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#plotting-with-matplotlib",
    "href": "Case_2.html#plotting-with-matplotlib",
    "title": "2  Data extraction and transformation",
    "section": "2.6 Plotting with Matplotlib",
    "text": "2.6 Plotting with Matplotlib\nThe standard Python plotting library is matplotlib. Let’s import the library and instruct Jupyter to display the plots inline (i.e. display the plots to the notebook screen so we can see them as we run the code).\n\n# import fundamental plotting library in Python\nimport matplotlib.pyplot as plt\n\n# Instruct jupyter/VS Code to plot in the notebook\n%matplotlib inline\n\nTo plot a series, use .plot(). We will come back to matplotlib later.\n\ndf['Age'].plot()\n\n\n\n\n\n\n\n\n\n2.6.1 Datetime objects\nPython’s internal data representation of dates is given by DateTime objects. Datetime objects are crucial for handling time-related data in a structured way, enabling various operations like comparison, arithmetic, and formatting. pandas offers the to_datetime() method to convert a string that represents a given date format into a datetime-like object. This is useful for ensuring that date and time data are properly recognized and can be used for time series analysis, indexing, and plotting.\nSyntax:\npd.to_datetime(arg, format=None)\nParameters:\n\narg: The date/time string(s) or list-like object to convert.\nformat: The strftime to parse time. For example, “%Y-%m-%d”.\n\nThe format parameter in pd.to_datetime() is used to specify the exact format of the date/time strings being parsed. This is particularly useful when the input date strings do not conform to standard formats or when you want to improve parsing performance by explicitly defining the format.\nDate formatting directives:\n\n%Y: Four-digit year (e.g., 2023).\n%y: Two-digit year (e.g., 23 for 2023).\n%m: Month as a zero-padded decimal number (e.g., 07 for July).\n%B: Full month name (e.g., July).\n%b or %h: Abbreviated month name (e.g., Jul for July).\n%d: Day of the month as a zero-padded decimal number (e.g., 03 for the 3rd).\n%A: Full weekday name (e.g., Monday).\n%a: Abbreviated weekday name (e.g., Mon).\n%H: Hour (24-hour clock) as a zero-padded decimal number (e.g., 14 for 2 PM).\n%I: Hour (12-hour clock) as a zero-padded decimal number (e.g., 02 for 2 PM).\n%p: AM or PM designation.\n%M: Minute as a zero-padded decimal number (e.g., 30 for 30 minutes past the hour).\n%S: Second as a zero-padded decimal number (e.g., 00 for 0 seconds).\n%f: Microsecond as a decimal number, zero-padded on the left (e.g., 000000).\n%j: Day of the year as a zero-padded decimal number (e.g., 189 for July 8th).\n%U: Week number of the year (Sunday as the first day of the week) as a zero-padded decimal number (e.g., 27).\n%W: Week number of the year (Monday as the first day of the week) as a zero-padded decimal number (e.g., 27).\n%w: Weekday as a decimal number (0 for Sunday, 6 for Saturday).\n%Z: Time zone name (e.g., UTC, EST).\n%z: UTC offset in the form +HHMM or -HHMM (e.g., +0530, -0800).\n%%: A literal ‘%’ character.\n\nExample formulae:\n\n%Y-%m-%d matches dates like 2023-07-03.\n%d/%m/%Y matches dates like 03/07/2023.\n%B %d, %Y matches dates like July 3, 2023.\n%I:%M %p matches times like 02:30 PM.\n%H:%M:%S.%f matches times like 14:30:00.000000.\n\nExamples:\nParsing Date in Non-Standard Format:\n\n# Example date string in non-standard format\ndate_str = '03-07-23'  # This represents July 3, 2023 in YY-MM-DD format\n\n# Convert to datetime using format parameter\ndate = pd.to_datetime(date_str, format='%y-%m-%d')\nprint(date)\n\n2003-07-23 00:00:00\n\n\nIn this example: - %y-%m-%d specifies the format where %y represents the two-digit year, %m represents the month, and %d represents the day.\nParsing Date and Time Together:\n\ndate_time_str = '07/03/2023 14:30:00'\n\n# Convert to datetime using format parameter\ndate_time = pd.to_datetime(date_time_str, format='%m/%d/%Y %H:%M:%S')\nprint(date_time)\n\n2023-07-03 14:30:00\n\n\nIn this example: - %m/%d/%Y %H:%M:%S specifies the format where %m/%d/%Y represents the date in MM/DD/YYYY format, and %H:%M:%S represents the time in HH:MM:SS format.\nHandling Dates with Textual Month:\n\ndate_str_textual = 'July 3, 2023'\n\n# Convert to datetime using format parameter\ndate_textual = pd.to_datetime(date_str_textual, format='%B %d, %Y')\nprint(date_textual)\n\n2023-07-03 00:00:00\n\n\nIn this example: - %B %d, %Y specifies the format where %B represents the full month name (e.g., July), %d represents the day, and %Y represents the four-digit year.\nYou can also use ChatGPT or Google the format of your date at hand! - But it is faster to just learn the formatting directives.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_3.html",
    "href": "Case_3.html",
    "title": "3  Data Transformation I",
    "section": "",
    "text": "3.1 Preliminary modules\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#some-more-pandas-functions",
    "href": "Case_3.html#some-more-pandas-functions",
    "title": "3  Data Transformation I",
    "section": "3.2 Some more pandas functions",
    "text": "3.2 Some more pandas functions\n\n3.2.1 Unique\nThe unique function in pandas is used to find the unique values in a Series or a column of a DataFrame. It returns the unique values as a NumPy array. This function is useful when you need to identify the distinct values in a dataset.\nSyntax:\npandas.Series.unique()\nThis method returns the unique values in the Series.\nExamples:\n\n# Creating a Series\ndata = pd.Series([1, 2, 2, 3, 4, 4, 4, 5])\n\n# Finding unique values\nunique_values = data.unique()\n\nprint(unique_values)\n\n\n# In this example, `data.unique()` returns a NumPy array containing the unique values `[1, 2, 3, 4, 5]` in the Series `data`.\n\n\n# Example 2: Finding Unique Values in a DataFrame Column\n\n# In this example, `df['A'].unique()` returns a NumPy array of unique values in column 'A', and `df['B'].unique()` returns a NumPy array of unique values in column 'B'.\n\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 2, 3, 4, 4, 4, 5],\n    'B': ['a', 'b', 'b', 'c', 'd', 'd', 'd', 'e']\n})\n\n# Finding unique values in column 'A'\nunique_values_A = df['A'].unique()\n\n# Finding unique values in column 'B'\nunique_values_B = df['B'].unique()\n\nprint(\"Unique values in column A:\", unique_values_A)\nprint(\"Unique values in column B:\", unique_values_B)\n\n[1 2 3 4 5]\nUnique values in column A: [1 2 3 4 5]\nUnique values in column B: ['a' 'b' 'c' 'd' 'e']\n\n\n\n\n3.2.2 The .apply function\nThe .apply() function in pandas is used to apply a function along the axis of a DataFrame or to elements of a Series. This function is highly versatile and can be used to perform complex operations on your data.\nBasic Syntax\nFor a Series:\nSeries.apply(func)\nFor a DataFrame:\nDataFrame.apply(func, axis=0)\nParameters:\n\nfunc: The function to apply to each element (Series) or to each column/row (DataFrame).\naxis: {0 or ‘index’, 1 or ‘columns’}, default 0. The axis along which the function is applied:\n\n0 or ‘index’: apply function to each column.\n1 or ‘columns’: apply function to each row.\n\n\nExamples:\nExample 1: Applying a Function to Each Element in a Series\n\n# Creating a Series\ndata = pd.Series([1, 2, 3, 4, 5])\n\n# Function to square each element\ndef square(x):\n    return x * x\n\n# Applying the function\nsquared_data = data.apply(square)\n\nprint(squared_data)\n\n0     1\n1     4\n2     9\n3    16\n4    25\ndtype: int64\n\n\nIn this example, the square function is applied to each element of the Series data, resulting in a new Series squared_data where each value is the square of the corresponding original value.\nExample 2: Applying a Function to Each Column in a DataFrame\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Function to sum the elements of a column\ndef column_sum(col):\n    return col.sum()\n\n# Applying the function to each column\ncolumn_sums = df.apply(column_sum, axis=0)\n\nprint(column_sums)\n\nA     6\nB    15\nC    24\ndtype: int64\n\n\nIn this example, the column_sum function is applied to each column of the DataFrame df, resulting in a Series column_sums containing the sum of the elements in each column.\nExample 3: Applying a Function to Each Row in a DataFrame\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Function to find the maximum value in a row\ndef row_max(row):\n    return row.max()\n\n# Applying the function to each row\nrow_maxs = df.apply(row_max, axis=1)\n\nprint(row_maxs)\n\n0    7\n1    8\n2    9\ndtype: int64\n\n\nIn this example, the row_max function is applied to each row of the DataFrame df, resulting in a Series row_maxs containing the maximum value in each row.\n\n\n3.2.3 The str.count() function\nThe str.count() function counts the occurrences of a pattern or substring in a Series.\nSyntax:\nSeries.str.count(pat)\nHere, pat is the pattern or substring to count.\nExample: Counting substrings in a Series:\n\n# Creating a Series\ndata = pd.Series(['apple', 'banana', 'apple pie', 'cherry'])\n\n# Counting occurrences of 'apple'\napple_count = data.str.count('apple')\n\nprint(apple_count)\n\n# Counting occurrences of 'a'\na_count = data.str.count('a')\n\nprint(a_count)\n\n0    1\n1    0\n2    1\n3    0\ndtype: int64\n0    1\n1    3\n2    1\n3    0\ndtype: int64\n\n\n\n\n3.2.4 Pivot Tables\nThe pd.pivot_table() function in pandas is a powerful tool for reshaping data. It allows you to aggregate data and create a new table that is a more compact and organized representation of your original DataFrame. Loosely, pivot tables can be used when you would like to make one column in your original DataFrame into the column names of a new DataFrame, one column in your original DataFrame into the new rows of that new DataFrame, and lastly, an aggregation of another column as the entries in the new DataFrame.\nSyntax:\npd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None,  dropna=True)\n\ndata: The DataFrame to pivot.\nvalues: Column(s) to aggregate.\nindex: Column(s) to set as index.\ncolumns: Column(s) to pivot.\naggfunc: Function to aggregate the data (default is ‘mean’).\nfill_value: Value to replace missing values.\ndropna: Do not include columns whose entries are all NaN (default is True).\n\nExample:\nLet’s consider a dataset containing sales data:\n\n# Sample data\ndata = {\n    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'],\n    'Product': ['A', 'A', 'B', 'B', 'A', 'B', 'A', 'B'],\n    'Sales': [100, 150, 200, 130, 120, 170, 160, 180],\n    'Quantity': [10, 15, 20, 13, 12, 17, 16, 18]\n}\n\ndf = pd.DataFrame(data)\n\nprint(df)\n\n  Region Product  Sales  Quantity\n0  North       A    100        10\n1  South       A    150        15\n2   East       B    200        20\n3   West       B    130        13\n4  North       A    120        12\n5  South       B    170        17\n6   East       A    160        16\n7   West       B    180        18\n\n\nLet’s create a pivot table to summarize the total sales and quantity for each region and product.\n\npivot_table = pd.pivot_table(df, values=['Sales', 'Quantity'], index=['Region'], columns=['Product'], aggfunc='sum', fill_value=0)\n\nprint(pivot_table)\n\n        Quantity     Sales     \nProduct        A   B     A    B\nRegion                         \nEast          16  20   160  200\nNorth         22   0   220    0\nSouth         15  17   150  170\nWest           0  31     0  310\n\n\nExplanation:\n\ndata: The DataFrame to pivot (df in this case).\nvalues: The columns to aggregate ('Sales' and 'Quantity').\nindex: The column(s) to set as the index of the pivot table ('Region').\ncolumns: The column(s) to pivot ('Product').\naggfunc: The aggregation function ('sum'), to get the total sales and quantity.\nfill_value: The value to replace missing values (0).\n\nThe resulting pivot table shows the total sales and quantities for each combination of region and product. The rows represent the regions, and the columns represent the products. The values in the table are the sums of sales and quantities.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#controlling-the-case-in-string-columns",
    "href": "Case_3.html#controlling-the-case-in-string-columns",
    "title": "3  Data Transformation I",
    "section": "3.3 Controlling the case in string columns",
    "text": "3.3 Controlling the case in string columns\n\nstr.lower: Converts all characters in a string to lowercase.\nstr.upper: Converts all characters in a string to uppercase.\n\n\nstr.lower\n\nThe str.lower function in pandas is used to convert all characters in a string to lowercase. It is often used when you want to standardize text data, making it easier to compare strings that might have different cases.\nSyntax:\nSeries.str.lower()\nExample:\n\n# Creating a Series\ndata = pd.Series(['Hello', 'World', 'PANDAS'])\n\n# Converting to lowercase\nlowercase_data = data.str.lower()\n\nprint(lowercase_data)\n\n0     hello\n1     world\n2    pandas\ndtype: object\n\n\n\nstr.upper\n\nThe str.upper function in pandas is used to convert all characters in a string to uppercase.\nSyntax:\nSeries.str.upper()\nExample:\n\n# Creating a Series\ndata = pd.Series(['hello', 'world', 'pandas'])\n\n# Converting to uppercase\nuppercase_data = data.str.upper()\n\nprint(uppercase_data)\n\n0     HELLO\n1     WORLD\n2    PANDAS\ndtype: object",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#personalized-functions",
    "href": "Case_3.html#personalized-functions",
    "title": "3  Data Transformation I",
    "section": "3.4 Personalized functions",
    "text": "3.4 Personalized functions\n\n3.4.1 Custom functions\nIn Python, you can create your own functions to encapsulate reusable code, improve readability, and make your programs more modular. A function is defined using the def keyword, followed by the function name, parentheses (), and a colon :. The code block within the function is indented.\nSyntax:\ndef function_name(parameters):\n    \"\"\"Docstring (optional): A brief description of the function.\"\"\"\n    # Function body\n    # Code to be executed\n    return value  # Optional: return statement to return a value\n\nfunction_name: The name of the function.\nparameters: A list of parameters (or arguments) that the function accepts.\nDocstring: A string describing what the function does.\nreturn: The value that the function returns.\n\nExample 1: A Simple Function\nHere’s a simple example of a function that takes no parameters and prints a message:\n\ndef greet():\n    \"\"\"Print a greeting message.\"\"\"\n    print(\"Hello, world!\")\n\n# Calling the function\ngreet()\n\nHello, world!\n\n\nExample 2: A Function with Parameters\nHere’s a function that takes two parameters and returns their sum:\n\ndef add(a, b):\n    \"\"\"Return the sum of two numbers.\"\"\"\n    return a + b\n\n# Calling the function\nresult = add(3, 5)\nprint(result)\n\n8\n\n\nExample 3: A Function with Default Parameters\nYou can also define functions with default parameter values:\n\ndef greet(name=\"world\"):\n    \"\"\"Print a greeting message to the given name.\"\"\"\n    print(f\"Hello, {name}!\")\n\n# Calling the function with and without an argument\ngreet(\"Alice\")\ngreet()\n\nHello, Alice!\nHello, world!\n\n\nExample 4: A Function with Variable Number of Arguments\nSometimes you might want to define a function that can accept a variable number of arguments using *args and **kwargs:\n\ndef print_numbers(*args):\n    \"\"\"Print all the numbers passed as arguments.\"\"\"\n    for number in args:\n        print(number)\n\n# Calling the function with multiple arguments\nprint_numbers(1, 2, 3, 4, 5)\n\n1\n2\n3\n4\n5\n\n\nExample 5: A Function with Keyword Arguments\nYou can use **kwargs to accept a variable number of keyword arguments:\n\ndef print_info(**kwargs):\n    \"\"\"Print key-value pairs of the passed keyword arguments.\"\"\"\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\n# Calling the function with multiple keyword arguments\nprint_info(name=\"Alice\", age=30, city=\"New York\")\n\nname: Alice\nage: 30\ncity: New York\n\n\n\nDefine a function using the def keyword.\nProvide parameters within the parentheses (optional).\nAdd a docstring to describe the function (optional but recommended).\nWrite the function body with the code to be executed.\nReturn a value using the return statement (optional).\n\n\n\n3.4.2 The functions map, filter and sorted\nRecall that an iterable object is any object that can be looped over, meaning you can go through its elements one by one. Common examples of iterable objects include lists, strings, and dictionaries. In Python, an object is considered iterable if it implements the __iter__() method, which returns an iterator. This allows you to use it in a for loop. Essentially, if you can apply a for loop to it, it’s an iterable object.\nThe map(), filter(), and sorted() functions are powerful tools for working with iterable objects. Here’s a brief overview of each function:\nmap()\nThe map() function applies a given function to all items in an iterable.\nSyntax:\nmap(function, iterable, ...)\n\nfunction: A function that is applied to each item of the iterable.\niterable: One or more iterable objects (e.g., lists, tuples).\n\nExample:\n\n# Example function\ndef square(x):\n    return x ** 2\n\n# Input list\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply `square` function to each item in the list\nsquared_numbers = map(square, numbers)\n\n# Convert the map object to a list and print\nprint(list(squared_numbers))\n\n[1, 4, 9, 16, 25]\n\n\nThe filter() function constructs an iterator from elements of an iterable for which a function returns true.\nSyntax:\nfilter(function, iterable)\n\nfunction: A function that tests if each element of an iterable returns true or false.\niterable: An iterable to be filtered.\n\nExample:\n\n# Example function\ndef is_even(x):\n    return x % 2 == 0\n\n# Input list\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply `is_even` function to filter even numbers\neven_numbers = filter(is_even, numbers)\n\n# Convert the filter object to a list and print\nprint(list(even_numbers))\n\n[2, 4]\n\n\nsorted()\nThe sorted() function returns a new sorted list from the items in an iterable.\nSyntax:\nsorted(iterable, key=None, reverse=False)\n\niterable: An iterable to be sorted.\nkey: A function that serves as a key for the sort comparison (optional).\nreverse: A boolean value. If True, the list elements are sorted as if each comparison were reversed (optional, default is False).\n\nExample:\n\n# Input list\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n\n# Sort the list\nsorted_numbers = sorted(numbers)\n\nprint(sorted_numbers)\n\n[1, 1, 2, 3, 4, 5, 5, 6, 9]\n\n\n\nmap(): Applies a function to every item in an iterable and returns a map object of the results.\nfilter(): Constructs an iterator from elements of an iterable for which a function returns true.\nsorted(): Returns a new sorted list from the items in an iterable, with optional custom sorting behavior.\n\n\n\n3.4.3 Anonymous functions\nAnonymous functions are defined using the lambda keyword. These functions are often called “lambda functions” and are used for creating small, one-time, and inline function objects. Unlike regular functions defined with def, lambda functions are limited to a single expression.\nSyntax of Lambda Functions\nlambda arguments: expression\n\nlambda: The keyword used to define an anonymous function.\narguments: A comma-separated list of parameters.\nexpression: A single expression that is evaluated and returned.\n\nExample 1: Simple Lambda Function\nHere’s a simple example of a lambda function that adds two numbers:\n\n# Lambda function to add two numbers\nadd = lambda x, y: x + y\n\n# Using the lambda function\nresult = add(3, 5)\nprint(result)\n\n8\n\n\nExample 2: Lambda Function in map()\nLambda functions are often used with functions like map(), filter(), and sorted().\n\n# List of numbers\nnumbers = [1, 2, 3, 4, 5]\n\n# Using lambda with map() to square each number\nsquared_numbers = list(map(lambda x: x ** 2, numbers))\n\nprint(squared_numbers)\n\n[1, 4, 9, 16, 25]\n\n\nExample 3: Lambda Function in filter()\n\n# List of numbers\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Using lambda with filter() to get even numbers\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n\nprint(even_numbers)\n\n[2, 4, 6, 8, 10]\n\n\nExample 4: Lambda Function in sorted()\n\n# List of tuples\npoints = [(1, 2), (3, 1), (5, -1), (2, 3)]\n\n# Using lambda with sorted() to sort by the second element of each tuple\nsorted_points = sorted(points, key=lambda x: x[1])\n\nprint(sorted_points)\n\n[(5, -1), (3, 1), (1, 2), (2, 3)]\n\n\nCharacteristics and Limitations:\n\nSingle Expression: Lambda functions can only contain a single expression. They cannot include statements or annotations.\nNo return Statement: The expression result is implicitly returned.\nLimited Use: Lambda functions are best used for short, simple operations. For more complex functions, it is better to define a regular function using def.\n\nWhen to Use Lambda Functions:\n\nShort, Simple Functions: When you need a quick function for a short, simple operation.\nInline Functions: When you need a function for a one-time use, especially within functions like map(), filter(), or sorted().\nReadability: When using a lambda function improves the readability and conciseness of your code.\n\nLambda functions in Python provide a concise way to create anonymous functions for simple, one-time operations. They are defined using the lambda keyword and can be used wherever function objects are required. However, due to their limitations, they are best suited for short, simple tasks and should be used judiciously to maintain code readability.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#line-plots-with-seaborn",
    "href": "Case_3.html#line-plots-with-seaborn",
    "title": "3  Data Transformation I",
    "section": "3.5 Line plots with seaborn",
    "text": "3.5 Line plots with seaborn\nLine plots are useful for visualizing data over continuous intervals or time series. The module seaborn is a powerful data visualization library built on top of Matplotlib, you can create line plots easily using the lineplot function.\nSyntax:\nseaborn.lineplot(data=None, *, x=None, y=None, hue=None, size=None, style=None, palette=None, data_frame=None, **kwargs)\n\ndata: DataFrame, array, or list of arrays, optional. Dataset for plotting.\nx: Name of the x-axis variable.\ny: Name of the y-axis variable.\nhue: Grouping variable that will produce lines with different colors.\nsize: Grouping variable that will produce lines with different widths.\nstyle: Grouping variable that will produce lines with different dash patterns.\npalette: Colors to use for different levels of the hue variable.\n\nExample 1: Simple Line Plot\nHere’s a basic example of creating a line plot with Seaborn.\n\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700]\n}\n\ndf = pd.DataFrame(data)\n\n# Create a line plot\nsns.lineplot(data=df, x='Year', y='Sales')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nExample 2: Line Plot with Multiple Lines\nYou can plot multiple lines by specifying a hue parameter.\n\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020, 2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700, 100, 150, 200, 250, 300, 350],\n    'Product': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B']\n}\n\ndf = pd.DataFrame(data)\n\n# Create a line plot with multiple lines\nsns.lineplot(data=df, x='Year', y='Sales', hue='Product')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nExample 3: Customizing Line Plot\nYou can customize the appearance of your line plot by using various parameters and options.\n\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700]\n}\n\ndf = pd.DataFrame(data)\n\n# Create a customized line plot\nsns.lineplot(data=df, x='Year', y='Sales', marker='o', linestyle='--', color='red')\n\n# Add titles and labels\nplt.title('Yearly Sales')\nplt.xlabel('Year')\nplt.ylabel('Sales')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nTo summarize:\n\nCreating Line Plots: Use sns.lineplot() to create line plots easily.\nMultiple Lines: Differentiate groups with the hue parameter.\nCustomization: Customize appearance using parameters like marker, linestyle, and color.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#writing-to-a-csv-file",
    "href": "Case_3.html#writing-to-a-csv-file",
    "title": "3  Data Transformation I",
    "section": "3.6 Writing to a csv file",
    "text": "3.6 Writing to a csv file\nThe to_csv() function in pandas is used to export a DataFrame to a CSV (Comma-Separated Values) file.\nSyntax:\nDataFrame.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None,, encoding=None, date_format=None)\nParameters:\n\npath_or_buf: The file path or object where the CSV will be saved. If not specified, the result is returned as a string.\nsep: The string to use as the separating character (default is a comma ,).\nna_rep: String representation of missing data.\nfloat_format: Format string for floating-point numbers.\ncolumns: Subset of columns to write to the CSV file.\nheader: Write out the column names (default is True).\nindex: Write row names (index) (default is True).\nindex_label: Column label for the index column(s) if desired.\nencoding: Encoding to use for writing (default is None).\ndate_format: Format string for datetime objects.\n\nExample 1: Simple Export\nHere’s a basic example of exporting a DataFrame to a CSV file.\n\n\n# Sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\n\ndf = pd.DataFrame(data)\n\n# Export DataFrame to CSV\ndf.to_csv('people.csv')\nThis will create a file named people.csv with the following content:\n,Name,Age,City\n0,Alice,25,New York\n1,Bob,30,Los Angeles\n2,Charlie,35,Chicago\nExample 2: Customizing Output\nYou can customize the output by specifying different parameters.\n# Export DataFrame to CSV without the index and with a custom separator\ndf.to_csv('people_no_index.csv', index=False, sep=';')\nThis will create a file named people_no_index.csv with the following content:\nName;Age;City\nAlice;25;New York\nBob;30;Los Angeles\nCharlie;35;Chicago\nExample 3: Handling Missing Values and Specifying Columns\n# Sample DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, None, 35, 40],\n    'City': ['New York', 'Los Angeles', 'Chicago', None]\n}\n\ndf = pd.DataFrame(data)\n\n# Export DataFrame to CSV with custom NA representation and specific columns\ndf.to_csv('people_missing_values.csv', na_rep='N/A', columns=['Name', 'Age'])\nThis will create a file named people_missing_values.csv with the following content:\n,Name,Age\n0,Alice,25.0\n1,Bob,N/A\n2,Charlie,35.0\n3,David,40.0\nSummary of usage:\n\nBasic Usage: Use to_csv('filename.csv') to export a DataFrame to a CSV file.\nCustom Separator: Change the delimiter using the sep parameter.\nNo Index: Exclude the DataFrame index using index=False.\nHandle Missing Values: Specify a representation for missing values with na_rep.\nSelect Columns: Export only specific columns using the columns parameter.\nAdvanced Options: Control formatting, quoting, encoding, and more with additional parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_4.html",
    "href": "Case_4.html",
    "title": "4  Data Transformation II",
    "section": "",
    "text": "4.1 Preliminary modules\n# Load packages\nimport os\nimport pandas as pd\nimport numpy as np\n\n# This line is needed to display plots inline in Jupyter Notebook\n%matplotlib inline\n\n# Required for basic python plotting functionality\nimport matplotlib.pyplot as plt\n\n# Required for formatting dates later in the case\nimport datetime\nimport matplotlib.dates as mdates\n\n# Advanced plotting functionality with seaborn\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")  # can set style depending on how you'd like it to look",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-matplotlib-plotting",
    "href": "Case_4.html#overview-of-matplotlib-plotting",
    "title": "4  Data Transformation II",
    "section": "4.2 Overview of matplotlib plotting",
    "text": "4.2 Overview of matplotlib plotting\nFor an introduction to matplotlib please use their quickstart guide.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-seaborn-plotting",
    "href": "Case_4.html#overview-of-seaborn-plotting",
    "title": "4  Data Transformation II",
    "section": "4.3 Overview of seaborn plotting",
    "text": "4.3 Overview of seaborn plotting\nSeaborn can be used to make more advanced plots, it also has a fairly simple syntax. We give a very brief introduction here. For more information, please follow the tutorials here.\nSyntax:\nsns.plot_function(data=pandas DF, x=column for the x-axis, y=column for the y-axis, hue=column to seperate the lines or points by color, kind= lines or points)\nEssentially, you specify the DataFrame, which variables go on the \\(y\\)-axis and the \\(x\\)-axis. Optionally, you can specidy a variable for the hue parameter. When it is specified, Seaborn assigns different colors to different levels of the hue variable, making it easy to see how different categories compare. kind tells seaborn whether to use lines or points etc.\nExample:\nOur example will use relplot. The relplot function in Seaborn is a high-level interface for creating relational plots that combine several plots like scatterplot and lineplot. It allows you to easily visualize relationships between multiple variables in a dataset. We’ll use the built-in tips dataset from Seaborn, which contains information about restaurant tips.\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a relational plot using relplot\nsns.relplot(data=tips, x='total_bill', y='tip', hue='day', kind='scatter')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nExplanation:\n\nData: The tips dataset contains columns such as total_bill, tip, sex, smoker, day, time, and size.\nx: The total_bill column is used for the x-axis.\ny: The tip column is used for the y-axis.\nhue: The day column is used to color the points differently for each day of the week.\n\nThe resulting plot shows a scatter plot of total_bill vs. tip, with different colors representing different days of the week. This makes it easy to see if there are any trends or differences in tipping behavior on different days.\nYou can further customize the relplot with additional parameters, such as changing the kind of plot to a line plot and adding more dimensions with style and size.\n\n# Create a line plot with relplot\nsns.relplot(data=tips, x='total_bill', y='tip', hue='day', style='time', size='size', kind='line')\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-new-plots",
    "href": "Case_4.html#overview-of-new-plots",
    "title": "4  Data Transformation II",
    "section": "4.4 Overview of new plots",
    "text": "4.4 Overview of new plots\nBelow is a brief overview of each of the new plot-types introduced in Case 4, along with the Seaborn syntax for creating them:\n\n4.4.1 Scatterplot\nA scatter plot is used to display the relationship between two continuous variables. Each point represents an observation in the dataset.\nExample:\n\n# Load example data\ntips = sns.load_dataset('tips')\n\n# Create a scatter plot\nsns.scatterplot(data=tips, x='total_bill', y='tip', hue='day')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.2 Histogram\nA histogram displays the distribution of a single continuous variable by dividing the data into bins and counting the number of observations in each bin. When analyzing a histogram, people typically look for several key characteristics:\n\nShape of the Distribution\n\nSymmetry: Whether the distribution is symmetric or asymmetric. A symmetric histogram has a bell-shaped curve.\nSkewness: The direction of the tail. A histogram with a long tail on the right side is right-skewed (positive skewness), and a histogram with a long tail on the left side is left-skewed (negative skewness).\nModality: The number of peaks in the histogram. A unimodal histogram has one peak, a bimodal histogram has two peaks, and a multimodal histogram has multiple peaks.\n\nCentral Tendency\n\nMean: The average value of the data, which can be roughly identified by the center of the distribution.\nMedian: The middle value of the data, which is helpful to compare with the mean.\n\nSpread or Variability\n\nRange: The difference between the maximum and minimum values, indicating the spread of the data.\nStandard Deviation: Although not directly visible on the histogram, the spread of the bars gives an idea of how dispersed the data is.\nInterquartile Range (IQR): The range within which the central 50% of the data lies, often inferred by looking at the central bulk of the histogram.\n\nOutliers\n\nExtreme Values: Data points that fall far outside the general distribution, which can be seen as isolated bars away from the main body of the histogram.\n\n\nExample:\nConsider a histogram of the variable total_bill from a restaurant tips dataset:\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a histogram\nsns.histplot(data=tips, x='total_bill', bins=20, kde=True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nWhen looking at this histogram, you might analyze it as follows:\n\nShape: Determine if the distribution is symmetric, skewed left, or skewed right. For example, if the histogram is right-skewed, it suggests that higher total bills are less common.\nCentral Tendency: Look for the central peak to get a sense of where most of the total_bill values lie. This provides insight into the average bill amount.\nSpread: Assess how spread out the total_bill values are. A wide spread indicates more variability in the bill amounts.\nOutliers: Identify any bars that are far away from the main distribution, indicating unusually high or low bill amounts.\nFrequency: Observe the height of the bars to understand how many observations fall into each bin.\n\n\n\n4.4.3 Boxplot\nLike a histogram, a boxplot (or box-and-whisker plot) displays the distribution of a continuous variable. It displays the five-number summary:\n\nLower whisker: The maximum of 1.5 times the interquartile range and the minimum.\nFirst quartile (Q1): The 25th percentile of the data. It is the left (or bottom) edge of the box.\nMedian (Q2): The middle value of the data set (50th percentile). It is represented by the line inside the box.\nThird quartile (Q3): The 75th percentile of the data. It is the right (or top) edge of the box.\nUpper whisker: The minimum of 1.5 times the interquartile range and the maximum.\n\nWe can view the following characteristics: 1. Spread of the data - The Interquartile range (IQR) is the difference between the first quartile (Q1) and the third quartile (Q3). It represents the middle 50% of the data. This is the size of the box. THe larger the box, the larger the spread of the data.\n\nLocation of the data\n\nThe location of the data can be read by looking at the line in the middle of the box, the median.\n\nSymmetry and Skewness of the data\n\nSymmetry: If the median is in the center of the box and the whiskers are of equal length, the data is symmetric.\nSkewness: If the median is not centered or the whiskers are of unequal length, the data is skewed. A longer whisker on the right indicates right skewness (positive skew), and a longer whisker on the left indicates left skewness (negative skew).\n\nTails or outliers\n\nMany points beyond the whiskers represents a heavy tail (a high tendency for observations to be far from the median.) A few points beyond the whiskers may indicate outliers.\n\n\nMultiple boxplots are often plotted together, to compare distributions for different populations. In this case, the boxplot can be used to determine the relationship between a categorical variable and a continuous variable.\nExample:\nLet’s create a boxplot using the tips dataset from Seaborn:\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a box plot\nsns.boxplot(data=tips, x='day', y='total_bill')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere, the spread of Saturday is larger than that of the Thursday. In addition, we have that average the bills are larger on the weekend. The distributions appear symmetric for all days.\n\n\n4.4.4 Heatmap\nA heatmap compares two categorical variables to a continuous variable. The two categorical variables are represented on the horizontal and vertical axes, and the intensity of the cells represent the continuous variable.\nExample:\n\n# Load example data\nflights = sns.load_dataset('flights')\n\n# Pivot the data to create a matrix\nflights_pivot = flights.pivot(index='month', columns='year', values='passengers')\n\n# Create a heatmap\nsns.heatmap(flights_pivot, annot=True, fmt='d', cmap='YlGnBu')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.5 Stripplot\nA strip plot is used to display the distribution of a single continuous variable or the relationship between a continuous variable and a categorical variable. Each observation is represented as a point.\nExample:\n\n# Create a strip plot\nsns.stripplot(data=tips, x='day', y='total_bill', jitter=True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.6 Lineplot\nA line plot is used to display the relationship between two continuous variables, often to visualize trends over time.\n\n# Create a line plot\nsns.lineplot(data=tips, x='size', y='tip', hue='time', style='time', markers=True, dashes=False)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.7 Summary\n\nScatterplot: Shows the relationship between two continuous variables.\nHistogram: Displays the distribution of a single continuous variable.\nBoxplot: Summarizes the distribution of a continuous variable or the relationship between a continuous and a categorical variable.\nHeatmap: Represents data in a matrix format with colors indicating values.\nStripplot: Displays the distribution of a continuous variable or the relationship between a continuous and a categorical variable.\nLineplot: Visualizes the relationship between two continuous variables, often to show trends.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#misc.-python-functions-from-case-4",
    "href": "Case_4.html#misc.-python-functions-from-case-4",
    "title": "4  Data Transformation II",
    "section": "4.5 Misc. Python functions from Case 4",
    "text": "4.5 Misc. Python functions from Case 4\nHere are brief explanations for some of the new functions introduced in Case 4:\n\n.dropna():\n\nThis method is used to remove missing values (NaNs) from a DataFrame or Series. By default, it drops any row containing at least one missing value. However, it can be configured to drop columns instead, or only rows/columns with all missing values, depending on the parameters passed. This is useful for cleaning data before analysis or visualization.\n\npd.merge():\n\nThis function from the pandas library is used to merge two DataFrames based on one or more common columns or indices. This is useful for combining datasets that have related information spread across different tables.\n\nplt.subplots():\n\nThis function from the matplotlib library creates a figure and a set of subplots. It returns a tuple containing a figure object and an array of axes objects. This function is handy for creating complex plots with multiple subplots in a single figure, allowing for more detailed and organized visualizations. The function can be customized to specify the number of rows and columns of subplots.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_5.html",
    "href": "Case_5.html",
    "title": "5  Interpretation of charts and graphs",
    "section": "",
    "text": "5.1 Steps you can take to avoid bad charts\nHere are some steps to help avoid people misinterpretating your charts and ensure your charts communicate data effectively and accurately:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#steps-you-can-take-to-avoid-bad-charts",
    "href": "Case_5.html#steps-you-can-take-to-avoid-bad-charts",
    "title": "5  Interpretation of charts and graphs",
    "section": "",
    "text": "Label Axes Clearly:\n\nAction: Always provide clear and descriptive labels for the x-axis and y-axis.\nBenefit: This helps viewers understand what the chart is measuring and comparing.\n\nUse Appropriate Scales:\n\nAction: Ensure the scales on your axes are appropriate for the data being presented, starting from zero where necessary.\nBenefit: This avoids exaggerating differences or trends in the data.\n\nProvide Context:\n\nAction: Include necessary context or background information, such as data source, time period, and any relevant notes.\nBenefit: Context helps viewers interpret the data correctly and understand its relevance.\n\nChoose the Right Chart Type:\n\nAction: Select the chart type that best represents your data (e.g., bar chart for categorical data, line chart for trends over time).\nBenefit: This ensures that the data is presented in the most understandable format.\n\nAvoid Overloading with Data:\n\nAction: Limit the amount of data displayed in a single chart to avoid clutter.\nBenefit: A cleaner chart helps viewers focus on the key insights without being overwhelmed.\n\nUse Consistent Color Schemes:\n\nAction: Use consistent and color-blind friendly color schemes that differentiate data points without causing confusion.\nBenefit: This helps in distinguishing different categories or series and avoids misinterpretation due to color issues.\n\nHighlight Key Insights:\n\nAction: Use annotations, different colors, or other visual cues to highlight key data points or trends.\nBenefit: This draws attention to the most important parts of the data and aids in interpretation.\n\nAvoid Distorting Data:\n\nAction: Avoid using 3D effects or other distortions that can misrepresent data.\nBenefit: This ensures that the data is presented accurately and prevents misleading viewers.\n\nShow Data Distribution:\n\nAction: Use boxplots, histograms, or other charts to show data distribution, not just summary statistics.\nBenefit: This provides a fuller picture of the data, revealing any underlying patterns or outliers.\n\nAdd Descriptive Titles and Legends:\n\nAction: Include descriptive titles and legends that clearly explain what the chart is showing.\nBenefit: This helps viewers quickly grasp the purpose and content of the chart.\n\nUse Gridlines Sparingly:\n\nAction: Use gridlines to help viewers accurately read values, but don’t overuse them to the point of clutter.\nBenefit: This aids in precision without overwhelming the chart.\n\nAvoid Cherry-Picking Data:\n\nAction: Present all relevant data, not just data that supports a particular narrative or viewpoint.\nBenefit: This ensures a fair and unbiased representation of the data.\n\nReview and Test Charts:\n\nAction: Review charts with colleagues or test with a sample audience to identify potential misinterpretations.\nBenefit: Feedback can help identify and correct any issues before presenting the chart to a wider audience.\n\nUse Interactive Elements:\n\nAction: When possible, use interactive charts that allow viewers to explore the data more deeply.\nBenefit: This enables viewers to drill down into the data and gain a better understanding.\n\nRegularly Update Data:\n\nAction: Ensure that the data in your charts is up-to-date and accurate.\nBenefit: This maintains the relevance and reliability of the insights provided by the charts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#parallel-coordinates-plots",
    "href": "Case_5.html#parallel-coordinates-plots",
    "title": "5  Interpretation of charts and graphs",
    "section": "5.2 Parallel coordinates plots",
    "text": "5.2 Parallel coordinates plots\nA parallel coordinates plot displays the values of multiple variables for a set of data points on parallel axes, allowing you to see patterns and relationships between the variables.\n\nAxes: Each variable in the dataset is represented by a vertical axis.\nLines: Each data point is represented by a line that connects the values of each variable across the parallel axes.\nPatterns: By observing how the lines intersect and align, you can identify patterns, correlations, and clusters in the data.\nComparisons: It is useful for comparing multiple observations and understanding the overall structure of the dataset.\n\nExample:\nLet’s create an example using the pandas and matplotlib libraries in Python. We’ll use the Iris dataset, which contains measurements of different species of iris flowers. Install scikit-learn if neccessary:\n!pip install scikit-learn\nLet’s create the plot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\n\n# Load the Iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris()\n\n# Convert to a DataFrame\niris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris['species'] = pd.Categorical.from_codes(iris_data.target, iris_data.target_names)\n\n# Create a parallel coordinates plot\nplt.figure(figsize=(10, 6))\nparallel_coordinates(iris, 'species', color=('#556270', '#4ECDC4', '#C7F464'))\n\n# Show the plot\nplt.title('Parallel Coordinates Plot of Iris Data')\nplt.xlabel('Attributes')\nplt.ylabel('Values')\nplt.show()\n\n\n\n\n\n\n\n\n\nLines: Each line represents an individual iris flower sample.\nAxes: The vertical axes represent the four measurements (sepal length, sepal width, petal length, and petal width).\nColors: Different colors represent different species of iris flowers (Setosa, Versicolor, Virginica).\nPatterns: By examining how the lines group and intersect, we can observe that:\n\nSetosa flowers have distinctly different measurements compared to Versicolor and Virginica flowers.\nVersicolor and Virginica have overlapping measurements, but with careful observation, patterns and differences can still be discerned.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#colored-backgrounds-on-scatterplots",
    "href": "Case_5.html#colored-backgrounds-on-scatterplots",
    "title": "5  Interpretation of charts and graphs",
    "section": "5.3 Colored backgrounds on scatterplots",
    "text": "5.3 Colored backgrounds on scatterplots\nIn this case, we colored the background based on the color of the closest point in the dataset. This is useful for classification tasks - where we want to determine if continuous variates are able to classify data points. Here is a simple example of how to do this in Python. This process includes creating a mesh grid of points, determining the closest point in the dataset for each point in the grid, and coloring the grid accordingly.\nExample:\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import cKDTree\nfrom matplotlib.colors import ListedColormap\n\n# Load the Iris dataset\niris = sns.load_dataset('iris')\n\n# Extract the relevant data\nx = iris['sepal_length']\ny = iris['sepal_width']\nspecies = iris['species']\n\n# Create a color map for species - dictionary of colors to species\nspecies_colors = {'setosa': (0.1215, 0.466666, 0.705882), \n                  'versicolor': (0.17254, 0.6274, 0.17259), \n                  'virginica': (0.83921, 0.15294, 0.1568)}\n\n# This is a series object where each row is the color associated with teh species\ncolors = iris['species'].map(species_colors)\nprint(colors)\n\n# Create a mesh grid\nx_min, x_max = x.min() - 0.5, x.max() + 0.5\ny_min, y_max = y.min() - 0.5, y.max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n\n# Combine the grid points into a single array\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# This code finds the closes points\n\n# Create a KDTree for fast nearest-neighbor lookup\ntree = cKDTree(np.c_[x, y])\n\n# Find the index of the closest point in the dataset for each grid point\n_, idx = tree.query(grid_points)\n\n# Map the indices to colors\ngrid_colors = np.array(colors.tolist())[idx].reshape(xx.shape + (3,))\n\n# Plot the background grid\nplt.figure(figsize=(10, 6))\nplt.imshow(grid_colors, extent=(x_min, x_max, y_min, y_max), origin='lower', aspect='auto', alpha=0.3)\n\n# Overlay the scatter plot\nsns.scatterplot(x=x, y=y, hue=species, palette=species_colors, style=species, edgecolor='k')\n\n# Customize and show the plot\nplt.title('2D Scatterplot of Iris Data with Colored Background Grid')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.legend(title='Species')\nplt.show()\n\n0      (0.1215, 0.466666, 0.705882)\n1      (0.1215, 0.466666, 0.705882)\n2      (0.1215, 0.466666, 0.705882)\n3      (0.1215, 0.466666, 0.705882)\n4      (0.1215, 0.466666, 0.705882)\n                   ...             \n145      (0.83921, 0.15294, 0.1568)\n146      (0.83921, 0.15294, 0.1568)\n147      (0.83921, 0.15294, 0.1568)\n148      (0.83921, 0.15294, 0.1568)\n149      (0.83921, 0.15294, 0.1568)\nName: species, Length: 150, dtype: object",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_6.html",
    "href": "Case_6.html",
    "title": "6  Exploratory Data Analysis",
    "section": "",
    "text": "6.1 Preliminary modules\nimport numpy                 as np\nimport pandas                as pd\nimport matplotlib.pyplot     as plt\nimport seaborn               as sns\nimport sklearn.metrics       as Metrics\n\n\nimport folium  #needed for interactive map\nfrom folium.plugins import HeatMap\n\nfrom   collections           import Counter\nfrom   sklearn               import preprocessing\nfrom   datetime              import datetime\nfrom   collections           import Counter\nfrom   math                  import exp\nfrom   sklearn.linear_model  import LinearRegression as LinReg\nfrom   sklearn.metrics       import mean_absolute_error\nfrom   sklearn.metrics       import median_absolute_error\nfrom   sklearn.metrics       import r2_score\n\n%matplotlib inline\nsns.set()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#attributes-of-the-dataset",
    "href": "Case_6.html#attributes-of-the-dataset",
    "title": "6  Exploratory Data Analysis",
    "section": "6.2 Attributes of the dataset",
    "text": "6.2 Attributes of the dataset\nWhen loading in a new dataset it is important to answer the following questions:\n\nWhat is the size of the dataset?\n\nPotentially useful Python attributes/methods: shape\n\nWhat are the important columns to my research question?\n\nPotentially useful Python attributes/methods: .columns, .head(), .index\n\nWhat does the data look like in these columns? Is it clean? What are the unique values?\n\nPotentially useful Python attributes/methods: .head(), .unique(), .isnull()\n\nHow many missing values are there? How are they coded?\n\nPotentially useful Python attributes/methods: .isnull(), .dropna(), .isnull().sum()\n\nDo I need to create new columns?\n\nPotentially useful Python attributes/methods: .apply()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#assessing-variables-individually",
    "href": "Case_6.html#assessing-variables-individually",
    "title": "6  Exploratory Data Analysis",
    "section": "6.3 Assessing variables individually",
    "text": "6.3 Assessing variables individually\nOnce our dataset is clean and we have identified the important variables, then we can investigate the distributions of the important variables in isolation. This helps identify anything “odd” going on in our data, helps us understand what the dataset looks like, and helps us understand the properties of each of the variables (location, spread, skewness, tails).\nOne way to do this is through printing summary statistics. Potentially useful Python attributes/methods: .describe(), .value_counts(), .mean(), .quantile(), .std(), .median() - See Data extraction and transformation for more.\nWe can also make one-dimensional charts - bar charts, histograms and boxplots - to assess each variate. Potentially useful Python attributes/methods: .hist(), .barplot(), .boxplot(), .bar() - See the plots covered in the previous cases for more.\n\n6.3.1 Barplots\nBarplots provide a way to summarize a categorical column or variable. Each bar displays the number of occurences of each category. Sometimes, when one category has most of the responses, it is useful to instead set the height of the bar to be the proportion of the responses attributed to each category.\nExample:\n\nSeaborn: Use sns.barplot() for creating bar plots with a high-level, statistical focus.\nMatplotlib: Use plt.bar() for more basic bar plots and lower-level control over plot elements.\n\nHere is an example of how to use Seaborn’s .barplot() function:\n\n# Sample DataFrame\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 12, 29]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.barplot(x='Category', y='Values', data=df)\nplt.title('Barplot of Categories vs. Values')\nplt.show()\n\n\n\n\n\n\n\n\nWhile .barplot() is specific to Seaborn, Matplotlib provides similar functionality through the bar() function:\n\n# Sample data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 12, 29]\n\n# Create a barplot\nplt.bar(categories, values)\nplt.title('Barplot of Categories vs. Values')\nplt.xlabel('Category')\nplt.ylabel('Values')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#relationships-between-two-variables",
    "href": "Case_6.html#relationships-between-two-variables",
    "title": "6  Exploratory Data Analysis",
    "section": "6.4 Relationships between two variables",
    "text": "6.4 Relationships between two variables\n\n6.4.1 Investigating relationships between two variables\nOnce we have sufficient knowledge of the single variable distributions, known as the univariate distributions, we can inspect relationships between variables. Sometimes, it is too much to look at each pair of variables, so our overall research question(s) should guide which relationships we investigate. Here, to compare continuous to categorical variables, we can use boxplots, and to compare two continuous variables, we can use scatterplots or line plots. Potentially useful Python attributes/methods: .relplot(), .boxplot() - See the plots covered in the previous cases for more.\nWe can also use the correlation (see below) between two continuous variates. Note that when a correlation does not match with our intution, there may be an interaction effect, and we should investigate three-way relationships with these variables.\nOn the plots, we should be looking for how the mean/median, scale, skewness and outliers of one variate changes with respect to the other. Take note of any interesting patterns. If we see something unusual or unexpected, then we should investigate further to explain it. This will also help in the future, when we use staitsitcal/machine learning models on the data. Having a good knowledge of what the dataset looks like will help you build better models, and identify problems with your models.\n\n\n6.4.2 Correlation\nCorrelation is a statistical measure that describes the strength and direction of a relationship between two variables. Correlation tells us how closely two variables move together. When two variables are correlated, knowing the value of one variable helps you predict the value of the other. Correlation falls on a scale of -1 to 1. There are two main types of correlation:\n\nPositive Correlation: This occurs when two variables tend to increase or decrease together. If one variable increases, the other variable also tends to increase. Conversely, if one decreases, the other tends to decrease.\nNegative Correlation: This occurs when one variable tends to increase as the other decreases, and vice versa. When one variable increases, the other tends to decrease.\n\nExamples:\n\nPositive Correlation:\n\nExample: As the number of hours studied increases, test scores tend to increase.\nExample: Higher levels of exercise are correlated with lower levels of obesity.\n\nNegative Correlation:\n\nExample: As the temperature decreases, heating costs tend to increase.\nExample: Increased smoking is correlated with a higher incidence of lung cancer.\n\n\nPitfalls of Correlation:\n\nCorrelation Does Not Imply Causation: Just because two variables are correlated does not mean that one causes the other to change. There may be other factors (confounding variables) influencing both variables.\nNon-linear Relationships: Correlation measures only linear relationships. If the relationship between variables is non-linear (e.g., quadratic), correlation may not accurately reflect the strength of the relationship.\nOutliers: Extreme values (outliers) can disproportionately influence correlation calculations, leading to misleading results.\nSpurious Correlations: Sometimes variables may appear to be correlated by chance, without any meaningful relationship. Care should be taken to analyze whether we have enough sample points to ensure that the computed correlations are useful.\n\nCorrelation is a useful measure for understanding relationships between variables. However, it’s important to interpret correlation carefully, considering other factors and potential limitations, to avoid drawing incorrect conclusions.\nThe relevent python function is DataFrame.corr(). This returns the correlation matrix of the variates in the relevant DataFrame. The correlation matrix is a matrix, whose \\(ij\\) th entry is the correlation between variable in column \\(i\\) and column \\(j\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#relationships-between-three-variables",
    "href": "Case_6.html#relationships-between-three-variables",
    "title": "6  Exploratory Data Analysis",
    "section": "6.5 Relationships between three variables",
    "text": "6.5 Relationships between three variables\n\n6.5.1 Location data and folium\nWhenever one variable we are interested in is a set of locations, it is useful to use the folium package to create visuals. Please visit this Quickstart guide to learn more about using folium.\n\n\n6.5.2 Other relationships between three variables\nAgain, we can use color and/or size to add a third variate to the scatter and boxplots. You can use the hue parameter in the seaborn package to achieve this. This is particularly useful for identifying or investigating interaction effects.\n\n\n6.5.3 Interaction effects\nAn interaction effect refers to a situation where the effect of one variable on an outcome depends on the level or value of another variable.\nDependence: An interaction effect means that the relationship between one variable and an outcome (AKA another variable) differs across different levels/values of another, third variable.\nA very simple example can be used to explain the definition:\nImagine you have two variables:\n\nVariable A: Age (Young or Old)\nVariable B: Treatment (New Drug or Placebo)\nVariable C: Overall health (Continuous scale)\n\nAn interaction effect between “age” and “treatment” on “overall health” occurs when the effect of “treatment” on the overall health is different depending on whether the patient’s is young or old.\nSignificance: Finding an interaction effect can change how we interpret the impact of individual variables on outcomes. It suggests that the combined effect of variables is different from what would be predicted by their individual effects alone.\nExample:\nLet’s say researchers are studying the effect of a new drug on health improvement in both young and old patients. They find that the drug is more effective in younger patients than older patients. This shows a main effect of age on health improvement.\nInteraction Effect: However, further analysis reveals that the difference in effectiveness between young and old patients depends on whether they received the drug or a placebo. If the drug has a significantly larger effect on young patients compared to old patients (more than what would be expected from just the main effects of age and treatment), then we say there is an interaction effect between age and treatment.\nExample:\nImagine we have a study on the effectiveness of a new teaching method (Variable A) on student performance (Outcome), where we also consider the student’s prior knowledge (Variable B). If the teaching method is more effective for students with high prior knowledge but less effective for students with low prior knowledge, then there is an interaction effect between teaching method and prior knowledge.\n\n\n6.5.4 Kernel density plots\nWe learned a new plot, called the kernel density plot. This is a smoothed version of the histogram. The bandwidth parameter controls the smoothness of the resulting KDE plot. Kernel density plots are useful to compare the distributions of two or more different samples, as they can be nicely overlaid. They also look nicer than a histogram.\nExample:\nTo create a kernel density plot, use the sns.kdeplot() function.\n\n# Sample data\ndata = pd.Series([23, 45, 12, 29, 30, 35, 42, 18, 25, 33])\n\n# Create a KDE plot with Seaborn\nsns.kdeplot(data, shade=True)\nplt.title('Kernel Density Estimation (KDE) Plot')\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.show()\n\nC:\\Users\\12RAM\\AppData\\Local\\Temp\\ipykernel_14408\\850350042.py:5: FutureWarning:\n\n\n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Code:\n\nsns.kdeplot(): This function plots the KDE of the given data.\n\ndata: The input data for which the KDE plot will be generated. It can be a Pandas Series, NumPy array, or a list.\nshade=True: Adds shading beneath the KDE curve for better visualization.\n\nplt.title(), plt.xlabel(), plt.ylabel(): These functions from Matplotlib are used to add a title, x-axis label, and y-axis label to the plot, respectively.\n\nAdditional Options:\n\nAdjusting Bandwidth: You can adjust the bandwidth of the KDE plot using the bw_adjust parameter in sns.kdeplot(). Lower values result in a smoother plot, while higher values result in a more jagged plot.\nMultiple Plots: sns.kdeplot() can also be used to plot KDEs for multiple variables or groups by passing multiple datasets or using the hue parameter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_7.html",
    "href": "Case_7.html",
    "title": "7  Data cleaning/wrangling",
    "section": "",
    "text": "7.1 Preliminary modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport base64\nimport datetime\n# from datetime import datetime, timezone, timedelta\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom geopy.geocoders import Nominatim",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#handling-missing-data",
    "href": "Case_7.html#handling-missing-data",
    "title": "7  Data cleaning/wrangling",
    "section": "7.2 Handling missing data",
    "text": "7.2 Handling missing data\nOne of the first steps in data cleaning is to deal with null or missing values. First, one should determine how missing values are labelled in the dataset, as this can vary widely. Some common encodings for missing data are as follows:\n\nNaN or nan: This is the most common representation of missing values, especially in Python.\nNone: In some programming languages, None is used to indicate the absence of a value.\nNull or NULL: Common in SQL databases.\nEmpty strings (“’’): Often used in text data.\nSpecial values: Sometimes a special value, like -9999 or 9999, is used to represent missing data.\nNA or N/A: These are often used in spreadsheets and text files to denote missing values.\n0 or -1: Occasionally used in cases where 0 or -1 are not valid values in the context of the data.\nBlanks or Whitespaces: Spaces or tabs might be used to signify missing values in some text data.\nPlaceholders like “missing” or “unknown”: Textual placeholders indicating the data is not available.\nINF or -INF: Infinity values, sometimes used to denote missing values.\n\n\n\n\n\n\n\nNote\n\n\n\nBefore moving forward, it is important to convert the missing values to either ‘NaN’ or ‘nan’, as this is what python functions built for missing values will expect to see.\n\n\nPrevious cases only gave a passing treatment of missing data and resulted in dropping the rows containing null values entirely. Here, we will be more nuanced and look at ways that missing values can be replaced appropriately. As always, domain knowledge is crucial in selecting what method to go forward with.\n\n\n\n\n\n\nImportant\n\n\n\nThe goal of filling in missing data is to not neccessarily to recover the exact values of the missing values, but rather to ensure that the conclusions of the analysis are not overtly impacted by the missing values.\n\n\nWhen dealing with missing data, it is important not to just use generic pandas/Python functions without thinking to fill them in. This can drastically corrupt your analysis.\nInstead, a first step is to find the source of the missing data. Why is it missing? and/or how did it go missing? If we can answer these questions, then this will help us in determining how to fill in the missing values.\nOne way to handle missing values is to simply remove any row which contains a missing value from the dataset. This is known as complete case analysis. Complete case analysis is desirable when there are not that many rows with missing values, and we do not lose any critical sub-populations as a results of dropping those rows. For instance, a rule of thumb is that if &lt;1% of the rows are missing, then it might be desirable to do a complete case analysis. It is fast and easy, and does not require extensive statistical knowledge. This also depends on the size of the dataset. If you have 10 million observations, and 1 million within each subpopulation, then removing even 5% of the rows should not impact the analysis too much. On the other hand, if we have only 20 observations, we should try not to remove any rows.\nAnother approach is to fill in the missing values - this is known as imputation. Usually, we use the other observations or rows to fill in the missing values.\n\n\n\n\n\n\nCaution\n\n\n\nOne popular method of imputation which you should not use is replacing missing values with the mean/median of the available values in the column, or replacing categorical missing values with the most common value.\nThe reason for this is that it lowers the sample standard deviation of the resulting column/changes the proportion distribution and can bias the resulting analysis.\n\n\nSome other methods of imputation are given below:\n\nForward/Backward Fill: Use the next or previous value to fill in the missing value. Forward/backward filling may make sense when there is a relationship between the rows. Example: In a time series dataset where stock prices are recorded daily, you might use forward fill to propagate the last known price to the days when the market data is missing.\nInterpolation: Estimate missing values using interpolation techniques. Example: In a climate dataset with daily temperature readings, you could use linear interpolation to estimate the temperature on days when data is missing by averaging the temperatures of the surrounding days.\nK-Nearest Neighbors (KNN): The KNN algorithm uses points that are similar to the points with missing data (called nearest neighbors) to impute missing values. Example: In a survey dataset where some participants skipped a few questions, KNN can impute their missing responses by considering the responses of the most similar participants (neighbors).\nDomain-Specific Imputation: Depending on the domain, you might use specific rules or external datasets to impute missing values. Example: In an educational dataset where some students’ test scores are missing, you might impute the missing scores using the average scores from other tests taken by the same student or from other students in the same class or grade level. This method leverages the relationship between students’ performance across different tests or the performance of their peers.\nRegression/Machine Learning Models: Use models, such as regression models or Neural Networks to predict and fill in missing values. Example: In a housing dataset where some houses are missing the number of bedrooms, you could use a regression model that predicts the number of bedrooms based on the house’s size, location, and price. In a complex dataset with multiple features about customer behavior, a Random Forest model can be used to predict and fill in missing values by leveraging the patterns and interactions between the different features.\n\n\n7.2.1 Interpolate\nThe .interpolate() function allows us to interpolate numerical values, based on the surrounding values. Interpolation is a method of estimating unknown values based on values that are close to it (in terms of rows). Interpolation can be used to fill in missing points continuously over some domain, such as time. For instance, this could include interest rates, daily temperatures, run speed etc. In general, we should be able to assume that the the variable we are interpolating could be written as a continuous function over the interval on which we are interpolating.\nThere are a number of methods of intperolation that can be specified via the method parameter. Be sure to read and understand each method carefully before using it. Otherwise, you could just be filling in the values with nonsense.\nParameters of .interpolate()\n\nmethod: Specifies the interpolation method to use.\n\n'linear' (default): Linear interpolation.\n'time': Interpolation for time-series data.\n'index': Uses the index for interpolation.\nOther methods like 'polynomial', 'spline', 'barycentric', etc. see the documentation.\n\naxis: Specifies the axis along which to interpolate.\n\n0 or 'index': Interpolates along the index (default for DataFrame).\n1 or 'columns': Interpolates along the columns.\n\n\nExample:\n\n# Creating a sample DataFrame with NaN values\ndf = pd.DataFrame({\n    'A': [1, np.nan, 3, np.nan, 5],\n    'B': [np.nan, 2, np.nan, 4, np.nan]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Interpolating missing values\ndf_interpolated = df.interpolate()\n\nprint(\"\\nInterpolated DataFrame:\")\nprint(df_interpolated)\n\nOriginal DataFrame:\n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  NaN\n3  NaN  4.0\n4  5.0  NaN\n\nInterpolated DataFrame:\n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  4.0  4.0\n4  5.0  4.0",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#handling-string-formats",
    "href": "Case_7.html#handling-string-formats",
    "title": "7  Data cleaning/wrangling",
    "section": "7.3 Handling string formats",
    "text": "7.3 Handling string formats\nIt is important to print the unique values of a string variable, in order to check for spaces, mixed case and spelling mistakes. If the variable has many entries, we may not be able to check all unique values. In that case, it is a good idea to use .strip() and str.lower()/str.upper() to remove extra spaces and convert all characters to lower case. In the natural language processing case, we will learn more about manipulating strings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#unique-ids",
    "href": "Case_7.html#unique-ids",
    "title": "7  Data cleaning/wrangling",
    "section": "7.4 Unique IDs",
    "text": "7.4 Unique IDs\nWhen generating IDs, it is important to make sure the generation process is idempotent (i.e. the same ID should be generated for each trip no matter how many times you run the script). The idempotency is required because there may be chances that the same datum is input into your data storage system or analysis software multiple times. For example, your coworker may first upload the data set for the first week of the month (may be for testing purposes, or based on data availability, etc.) and then later may upload the data for the entire month. Now if the same datum is assigned different IDs on each upload, then it might result in the analytics platform interpreting this as two different points and this will corrupt the analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#more-on-datetime-objects",
    "href": "Case_7.html#more-on-datetime-objects",
    "title": "7  Data cleaning/wrangling",
    "section": "7.5 More on datetime objects",
    "text": "7.5 More on datetime objects\nWe have learnt about datetime objects in the past. In this case, we covered some new functionalities of these objects. We should almost always convert dates and times to datetime objects.\nHere’s an overview of some new attributes of datetime objects that were introduced in Case 7:\n\n.year: Returns the year.\n.month: Returns the month (1-12).\n.day: Returns the day of the month (1-31).\n.hour: Returns the hour (0-23).\n.minute: Returns the minute (0-59).\n.second: Returns the second (0-59).\n.microsecond: Returns the microsecond (0-999999).\n.tzinfo: Returns the timezone information.\n\nIn addition, you can use the following methods to get week-related information:\n\n.weekday(): Returns the day of the week as an integer, where Monday is 0 and Sunday is 6.\n.isoweekday(): Returns the day of the week as an integer, where Monday is 1 and Sunday is 7.\n.isocalendar(): Returns a tuple containing the ISO year, ISO week number, and ISO weekday.\n\nExample:\n\n# Create a datetime object\ndt = datetime.datetime(2023, 7, 5, 14, 30, 0)\n\n# Year, month, day, etc.\nprint(\"Year:\", dt.year)          # Output: 2023\nprint(\"Month:\", dt.month)        # Output: 7\nprint(\"Day:\", dt.day)            # Output: 5\nprint(\"Hour:\", dt.hour)          # Output: 14\nprint(\"Minute:\", dt.minute)      # Output: 30\nprint(\"Second:\", dt.second)      # Output: 0\n\n# Weekday (Monday = 0, Sunday = 6)\nprint(\"Weekday:\", dt.weekday())  # Output: 2 (Wednesday)\n\n# ISO Weekday (Monday = 1, Sunday = 7)\nprint(\"ISO Weekday:\", dt.isoweekday())  # Output: 3 (Wednesday)\n\n# ISO Calendar (Year, Week number, Weekday)\niso_calendar = dt.isocalendar()\nprint(\"ISO Calendar:\", iso_calendar)    # Output: (2023, 27, 3)\n\nYear: 2023\nMonth: 7\nDay: 5\nHour: 14\nMinute: 30\nSecond: 0\nWeekday: 2\nISO Weekday: 3\nISO Calendar: datetime.IsoCalendarDate(year=2023, week=27, weekday=3)\n\n\nAdditional operations:\n\nCurrent Date and Time:\n\nnow = datetime.datetime.now()\nprint(now)  # Output: current date and time\n\n2024-07-22 14:16:03.925533\n\n\nParsing Dates:\n\ndate_str = \"2023-07-05\"\nparsed_date = datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\nprint(parsed_date)  # Output: 2023-07-05 00:00:00\n\n2023-07-05 00:00:00\n\n\nFormatting Dates:\n\ndt = datetime.datetime(2023, 7, 5, 14, 30, 0)\nformatted_date = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\nprint(formatted_date)  # Output: 2023-07-05 14:30:00\n\n2023-07-05 14:30:00\n\n\nDate Arithmetic:\n\ndt1 = datetime.datetime(2023, 7, 5)\ndt2 = datetime.datetime(2023, 7, 10)\ndelta = dt2 - dt1\nprint(delta)  # Output: 5 days, 0:00:00\n\n5 days, 0:00:00\n\n\nHandling Time Zones:\n\nutc_dt = datetime.datetime(2023, 7, 5, 14, 30, 0, tzinfo=datetime.timezone.utc)\nlocal_tz = datetime.timezone(datetime.timedelta(hours=-5))\nlocal_dt = utc_dt.astimezone(local_tz)\nprint(local_dt)  # Output: 2023-07-05 09:30:00-05:00\n\n2023-07-05 09:30:00-05:00",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#adding-new-data",
    "href": "Case_7.html#adding-new-data",
    "title": "7  Data cleaning/wrangling",
    "section": "7.6 Adding new data",
    "text": "7.6 Adding new data\nOften, you will have to add data from other datasets, websites or sources to your dataset. For instance, in Case 7, we showed how to get address data using geopy.\nThe function geolocator.geocode() can be used to obtain longitude and latitude from a string which contains the address. To learn more about geopy, you can follow the functions and tutorials here. In general, you can use geopy to obtain coordinates and other location data, given data like address, postal code etc. You can use this in conjunction with the folium package to create some nice visualizations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_8.html",
    "href": "Case_8.html",
    "title": "8  Hypothesis testing I",
    "section": "",
    "text": "8.1 Preliminary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels\nfrom scipy import stats \nfrom pingouin import pairwise_tests #this is for performing the pairwise tests\nfrom pingouin import pairwise_ttests #this is for performing the pairwise tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#hypothesis-testing-overview",
    "href": "Case_8.html#hypothesis-testing-overview",
    "title": "8  Hypothesis testing I",
    "section": "8.2 Hypothesis testing overview",
    "text": "8.2 Hypothesis testing overview\nHypothesis help you rule out sampling variation as an explanation for an observed pattern in a data set. It helps us lift a pattern from a sample to a population. The idea is this - every data set we have seen in this course is comprised of a small subset of a population we are interested in learning about. That small subset is called a sample. For instance, in the previous case, we were interested in the population of trips taken on the rentable bikes. We had access to a small subset of the trips taken - this is the sample. Now, there is a small problem with the results of any data analysis. The subset of observations, or sample of observations, that we have access to is random. We could have easily drawn a different subset and a different subset will give different output in data analysis - that is - different plots and summary statistics. For instance, the average length of a bike trip could be 45 minutes in one subset and 30 minutes in another. The true average bike trip length for the population could be anything - say 39. The purpose of hypothesis testing is to rule out the situation where the sample may not resemble the population. In other words, it is useful to say that the conclusions made in our data analysis are likely to reflect that of the population, and are not due to drawing a bad sample.\nIn this course, we will mainly focus on saying something about the mean of potentially several populations. In general, the hypothesis testing procedure concerns two hypotheses - the null and alternative hypothesis. These contain mutually exclusive statements about a population.\nFor example: \\[H_0: \\text{The population trip length is } 40\\ vs.\\ H_a: \\text{The population trip length is not } 40.\\]\n\\(H_0\\) is called the null hypothesis. The null hypothesis often corresponds to the situation of “no effect” - but not always.\nIn opposition to the null hypothesis, we define an alternative hypothesis (often indicated with \\(H_1\\) or \\(H_a\\)) to challenge the null hypothesis. In general, this is the hypothesis of there existing some pattern or effect in the data.\nFor instance, we may suspect that the average trip length is 40, and we may be concerned about whether our data shows that the trip length is longer than this. This may be of concern for computing bike maintenance costs. In that case, we would define \\(H_a: \\text{The population trip length is greater than } 40.\\)\nThe next step in a hypothesis test is to compute a \\(p\\)-value using our sample. The \\(p\\)-value measures the evidence against the null hypothesis. It is a number between 0 and 1. The closer to 0 the \\(p\\)-value is, the more evidence there is against the null hypothesis. The \\(p\\)-value can be interpreted as follows: The \\(p\\)-value is the probability that, assuming the null hypothesis is true, we drew a sample that differs from the null hypothesis at least as much as the one at hand.\nLet’s unpack this statement using our example. First, before computing our \\(p\\)-value we assume that the null hypothesis is true. In our example, we would assume that the mean population trip length is 40 minutes. Then, if the mean population trip length is 40 minutes we can measure how far the sample mean is from 40. Say that our sample mean is 45, and so then we would have observed a sample whose mean is 5 minutes higher than that of the assumed population mean. The \\(p\\)-value is then the probability that, assuming the true mean is 40, we saw a sample whose sample mean is at least 5 minutes away from 40. You will learn in a later course how to compute such a probability. Intuitively, the higher the distance of the sample mean from 40, the lower the \\(p\\)-value. Therefore, if we observe a sample mean far from 40, the \\(p\\)-value will be very close to 0. We can interpret the \\(p\\)-value as the probability, assuming the mean population trip length is 40, that we drew a sample whose sample mean differs from 40 at least as much as the sample mean of the data set at hand.\n\n\n\n\n\n\nCaution\n\n\n\nThe \\(p\\)-value DOESN’T mean that the probability of \\(H_a\\) is 1-(\\(p\\) - value).\n\n\nIn general, \\(p\\)-values close to 0, as in &lt;0.1 or &lt;0.05 present evidence against the null hypothesis. This threshold can depend on the application.\nWhen doing a formal hypothesis tests, there are two possible outcomes for a test:\n\nWe conclude \\(H_0\\) is false, and say we reject \\(H_0\\). In this case we will conclude that there is statistical evidence for the alternative \\(H_a\\).\nWe fail to reject \\(H_0\\). In this case, we conclude that there is not enough statistical evidence to say that \\(H_0\\) is false.\n\n\n\n\n\n\n\nCaution\n\n\n\nNotice that in the second case we cannot say that the null hypothesis is true - it might be that we just don’t have enough data to rule it out.\n\n\nIn a formal hypothesis test, we define a threshold \\(\\alpha\\), where if the \\(p\\)-value falls below this threshold, then we reject the null hypothesis. In general, less formal cases, one may just compute the \\(p\\)-value and use it to inform decision making, along with other factors.\nTo summarize:\n\nA hypothesis test is used to confirm that a pattern is a feature of the population, and is not due to sampling variation.\nTo conduct a hypothesis test, we define a null and alternative hypothesis.\nAfter defining the hypohtheses, we then compute the evidence against the null hypothesis, given by the \\(p\\)-value.\nIf the \\(p\\)-value is small, we reject the null hypothesis and conclude the alternative. Otherwise, we fail to reject the null hypothesis.\nYou should always interpret the \\(p\\)-value and state your conclusion as the final step in the hypothesis test.\n\nTo elaborate on point 5. - the point of any statistical analysis is not to run the correct code, but to properly choose the analysis procedure and interpret the results appropriately.\n\n\n\n\n\n\nWarning\n\n\n\nA hypothesis test cannot tell you which scenario is certainly true - we would have to have access to the whole population to know that. It can, however, tell us that certain patterns are features of the population with very high certainty, which is sufficient for most situations.\n\n\nThe hypothesis tests introduced in Case 8 concern only testing for the mean of one or more populations. We cover each of those in turn.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-the-mean-in-a-single-population",
    "href": "Case_8.html#testing-for-the-mean-in-a-single-population",
    "title": "8  Hypothesis testing I",
    "section": "8.3 Testing for the mean in a single population",
    "text": "8.3 Testing for the mean in a single population\nWe first cover how to perform a hypothesis test concerning the mean value of a single population. Define the population mean of a single population to be \\(\\mu\\). The null hypothesis in this case is in the form of \\(H_0\\colon \\mu=\\mu_0\\). For instance, above, \\(\\mu_0=40\\).\nWe have three different ways to define an alternative hypothesis:\n\n\\(H_a: \\mu \\neq \\mu_0\\) (two-sided test)\n\\(H_a: \\mu &gt; \\mu_0\\) (one-sided test)\n\\(H_a: \\mu &lt; \\mu_0\\) (one-sided test)\n\nA common way to perform this test is to use the one-sample \\(t\\)-test.\nThe syntax for perfomring the test is given as follows:\nstats.ttest_1samp(Series you want to test, popmean= mu_0, alternative= specify which of (1-3) here)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.\nExample:\n\n# Create a Pandas Series with sample data\ndata = pd.Series([2.3, 1.9, 2.7, 2.5, 2.1])\n\n# Perform a one-sample t-test\nt_stat, p_value = stats.ttest_1samp(data, popmean=2.0, alternative='greater')\n\n# Print the results\nprint(f'T-statistic: {t_stat}, P-value: {p_value}')\n\nT-statistic: 2.121320343559641, P-value: 0.0505957536091478",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-a-difference-in-mean-for-two-populations",
    "href": "Case_8.html#testing-for-a-difference-in-mean-for-two-populations",
    "title": "8  Hypothesis testing I",
    "section": "8.4 Testing for a difference in mean for two populations",
    "text": "8.4 Testing for a difference in mean for two populations\nWe now cover how to perform a hypothesis test concerning the difference in the mean values between two populations. We would like to test whether two populations have different population means. That is, whether the difference between the population mean of group 1 (\\(\\mu_1\\)) is different from the population mean of group 2 (\\(\\mu_2\\)). The hypotheses look like: \\[ H_0: \\mu_1=\\mu_2\\] \\[H_a: \\mu_1 \\neq \\mu_2\\]\nA common way to perform this test is to use the two-sample \\(t\\)-test with unequal variance. (If you think the two populations have approximately the same variance, you can set equal_var=True in the following code.)\nThe syntax for perfomring the test is given as follows:\nstats.ttest_ind(data for group 1 , data for group 2, equal_var=False)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.\nExample:\n\n# Create two Pandas Series with sample data for two groups\ngroup1 = pd.Series([2.3, 1.9, 2.7, 2.5, 2.1])\ngroup2 = pd.Series([3.1, 3.6, 3.2, 3.8, 3.0])\n\n# Perform an independent two-sample t-test\nt_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n\n# Print the results\nprint(f'T-statistic: {t_stat}, P-value: {p_value}')\n\nT-statistic: -4.980696683149987, P-value: 0.0011005506250116467",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-a-difference-in-mean-for-several-populations",
    "href": "Case_8.html#testing-for-a-difference-in-mean-for-several-populations",
    "title": "8  Hypothesis testing I",
    "section": "8.5 Testing for a difference in mean for several populations",
    "text": "8.5 Testing for a difference in mean for several populations\nLastly, if we would like to perform a hypothesis test for whether or not all the population means are the same when considering \\(k&gt;2\\) populations, we can do the following.\nFirst, the hypotheses are given by\n\\[H_0: \\mu_1=\\mu_2\\ldots\\mu_3=\\mu_k,\\] vs. \\[H_a : \\mathrm{At \\,least\\, one\\, of\\, the\\, means\\,} \\mu_j \\mathrm{\\,is \\,different\\, from\\, the \\,others}.\\]\nTo test this hypothesis we need an extension of the capabilities of the \\(t\\) - tests (which can test at most only two groups at the same time). This test is called Analysis of Variance (ANOVA).\nThe syntax is given as follows:\n# This is code you can fill in to perform this test\nmod = ols('quantity of interest ~ grouping variable', data= YOUR_DATAFRAME).fit()  \nsm.stats.anova_lm(mod, typ=2)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.\nExample:\n\n# Create a sample DataFrame\ndata = pd.DataFrame({\n    'quantity': [2.3, 1.9, 2.7, 2.5, 2.1, 3.1, 3.6, 3.2, 3.8, 3.0],\n    'group': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n})\n\n# Fit the linear model using OLS\nmod = ols('quantity ~ group', data=data).fit()\n\n# Perform ANOVA\nanova_results = sm.stats.anova_lm(mod, typ=2)\n\n# Print the results\nprint(anova_results)\n\n          sum_sq   df          F    PR(&gt;F)\ngroup      2.704  1.0  24.807339  0.001079\nResidual   0.872  8.0        NaN       NaN",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#errors-in-hypothesis-testing",
    "href": "Case_8.html#errors-in-hypothesis-testing",
    "title": "8  Hypothesis testing I",
    "section": "8.6 Errors in hypothesis testing",
    "text": "8.6 Errors in hypothesis testing\nThere are two ways that a test can lead us to an incorrect decision:\n\nWhen \\(H_0\\) is true and we reject it. This is called Type 1 Error. It corresponds to obtaining a false positive.\nWhen \\(H_0\\) is false and we do not reject it. This is called Type 2 Error. It corresponds to having a false negative.\n\nThis can be summarized as follows:\n\n\n\n\n\n\\(H_0\\) is true \n\n\n \\(H_0\\) is False\n\n\n\n\nReject \\(H_0\\)\n\n\nType I error\n\n\nCorrect Decision (True Positive)\n\n\n\n\nFail to Reject \\(H_0\\) \n\n\nCorrect Decision (True negative)\n\n\nType II error\n\n\n\nIn general, a Type I error is thought to be more serious and so it is standard practice to control the probability of making a Type I error. In a formal hypothesis test, when the null hypothesis is true, the probability of making a Type I error is the threshold \\(\\alpha\\), also known as the significance level, that we introduced above. Often, we choose our significance level \\(\\alpha\\) to be small, e.g., \\(1\\%,5\\%,10\\%\\). Lowering the \\(\\alpha\\) value (say to \\(1\\%\\)) will decrease the probability of making a false positive conclusion, when the null hypothesis is true. Of course, because we control \\(\\alpha\\), we cannot control the Type II error we make. Note that lowering \\(\\alpha\\) is not without consequence, as, if the alternative hypothesis is true, then a lower threshold increases the probability of a Type II error.\nIn summary, it is important to be aware of the two types of error and evaluate the gravity of what making each error would mean for your context. In particular, you should evaluate the consequences of making a Type I error and choose your the threshold \\(\\alpha\\) accordingly. Lastly, you should know that there is a trade-of between Type I error and Type II error in a given hypothesis test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#misc.-python-functions-introduced-in-case-8",
    "href": "Case_8.html#misc.-python-functions-introduced-in-case-8",
    "title": "8  Hypothesis testing I",
    "section": "8.7 Misc. Python functions introduced in Case 8",
    "text": "8.7 Misc. Python functions introduced in Case 8\n\nplt.subplot(rows, cols, curr_plot) - use this function to position multiple plots in a single figure. For example, plt.subplot(2, 3, 4) creates a grid with 2 rows and 3 columns and activates the 4th subplot.\nsns.countplot - creates a barplot.\nenumerate Use this function to get both the index and the value from an iterable in a loop. Example:\n\n\nfruits = ['apple', 'banana', 'cherry']\nfor index, fruit in enumerate(fruits, start=1):\n    print(f\"{index}: {fruit}\")\n\n1: apple\n2: banana\n3: cherry\n\n\n\nplt.xticks(rotation = 90) used to rotate the \\(x\\)-axis labels.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_9.html",
    "href": "Case_9.html",
    "title": "9  Hypothesis testing II",
    "section": "",
    "text": "9.1 Preliminary modules\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing II</span>"
    ]
  },
  {
    "objectID": "Case_9.html#contingency-tables",
    "href": "Case_9.html#contingency-tables",
    "title": "9  Hypothesis testing II",
    "section": "9.2 Contingency tables",
    "text": "9.2 Contingency tables\n\n9.2.1 What are contingency tables\nContingency tables can be used to investigate relationships between two categorical variables. A contingency table counts the number of times each pair of categories (one from each of the considered variables) occured together in the dataset. Given that the counts are not normalized - that is, the scale depends on the total number of observations - we may wish to normalize the counts by \\(1/n\\) and report proportions. Medium to large variations in column or row counts/proportions indicate that there is a relationship between the two categorical variables in the table.\n\n\n9.2.2 Creating contingency tables in Python\nThe function pd.crosstab() can be used to make a contingency table.\nSyntax:\ncontingency_table = pd.crosstab(series 1, series 2)\nExample:\n\n# Sample data\ndata = pd.DataFrame({\n    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male'],\n    'Preference': ['Tea', 'Coffee', 'Coffee', 'Tea', 'Coffee', 'Coffee', 'Tea', 'Tea', 'Coffee', 'Coffee']\n})\n\n# Create the contingency table of gender and preference\ncontingency_table = pd.crosstab(data['Gender'], data['Preference'])\n\n# Print the contingency table\nprint(contingency_table)\n\nprint(contingency_table/len(data.index))\n\nPreference  Coffee  Tea\nGender                 \nFemale           4    1\nMale             2    3\nPreference  Coffee  Tea\nGender                 \nFemale         0.4  0.1\nMale           0.2  0.3\n\n\nIn this example: - data is a DataFrame containing the sample data. - pd.crosstab(data['Gender'], data['Preference']) creates the contingency table, showing the frequency of each combination of Gender and Preference.\nWe see that 40% of the females enjoy coffee over tea, compared to that of 20% of the males. In this case, the distribution is non-uniform.\n\n\n9.2.3 Stacked barplots\nA stacked bar plot is a type of bar chart used to visualize the relationship between two categorical variables. The \\(x\\)-axis displays each category of one of the categorical variables. There is one bar for each category displayed on the \\(x\\)-axis. Each bar in a stacked bar plot represents one category on the \\(x\\)-axis, and segments within the bar represent the proportion of observations attributed to each of the categories in the categorical variable not displayed on the \\(x\\)-axis. If the bars look similar accross the categories, this indicates no relationship between the two categories. On the other hand, bars that differ indicate a relationship between the two displayed categorical variables.\nIn Python, we make a stacked barplot using a contingency table. The syntax is given by:\nCont_table.plot(kind='bar', stacked = True)\nwhere Cont_table is a contingency table for the two categorical variables.\nToy example:\n\n# Create the contingency table\ncontingency_table = pd.crosstab(data['Gender'], data['Preference'])\n\n# Plot the stacked bar plot\ncontingency_table.plot(kind='bar', stacked=True)\n\n# Add labels and title\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Preference by Gender')\nplt.legend(title='Preference')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe larger orange bars for the male category indicate that a higher proportion of males in the data set preferred tea over coffee than females in the data set.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing II</span>"
    ]
  },
  {
    "objectID": "Case_9.html#chi-squared-tests",
    "href": "Case_9.html#chi-squared-tests",
    "title": "9  Hypothesis testing II",
    "section": "9.3 Chi-squared tests",
    "text": "9.3 Chi-squared tests\nIf we observe in the stacked barplots and contingency table that there is a relationship between two categorical variables, then we may wish to rule out sampling variability. For instance, how nonuniform does the stacked bar chart have to be for us to conclude there is a relationship between the two variables? There are many different ways to perform the test but we will focus on the most widely used test: the Chi-square test (\\(\\chi^2\\)-test). The hypotheses for the Chi-square test is:\n\\[\nH_0:\\text{Variable A is independent of Variable B}\n\\] vs. \\[\nH_a:\\text{Variable A is not independent of Variable B}.\n\\]\nIntuitively, “independence between two variables” means that knowledge of one of the variables does not inform us about what the other variable might be. For instance, in the coffee and tea example, if beverage preference and gender are independent, then knowing someone’s gender will not help us with figuring out if they prefer coffee or tea. Another way to state it is that the distribution of values of one variable remains the same even as the value of the other variable changes (and vice versa).\n\n\n\n\n\n\nNote\n\n\n\nThe different categories in a categorical variable are often referred to as levels.\n\n\nTo execute the test, we must first compute the proportions of each level of each category within the data. To do that, we can use Python’s value_counts().\n\n# Number in each category \nnum_gen=data[\"Gender\"].value_counts()\n# Divide by total \nprop_g=(num_gen/data[\"Gender\"].count()).sort_index()\n\n# Number in each category \nnum_p=data['Preference'].value_counts()\n# Divide by total \nprop_p=(num_p/data['Preference'].count()).sort_index()\nn=data['Preference'].count()\nprint(n)\nprint(prop_g)\nprint(prop_p)\n\n10\nGender\nFemale    0.5\nMale      0.5\nName: count, dtype: float64\nPreference\nCoffee    0.6\nTea       0.4\nName: count, dtype: float64\n\n\nLet’s go back to the contingency table. Each cell in the contingency table corresponds to one level from variable A and one from variable B.\n\nprint(contingency_table)\n\nPreference  Coffee  Tea\nGender                 \nFemale           4    1\nMale             2    3\n\n\nIn this case, each cell corresponds to one of either tea or coffee and one of either male or female. For instance, the upper left cell correponds to coffee and female. You will learn in your probability course that, under the null hypothesis, on average, each cell should contain roughly the number of samples, times the proportion of observations in the level from the row variable that correponds to that cell, times the proportion of observations in the level from the column variable that correponds to that cell. For instance, we expect the upper left cell to contain \\[\\text{number of total samples} \\times \\text{proportion of females}\\times \\text{proportion of those who prefer coffee}= 10\\times0.5\\times0.6=3.\\]\n\ng, p, dof, expctd = chi2_contingency(contingency_table)\nprint(\"p-value\", p)\n\np-value 0.5186050164287255\n\n\nIn this case, we fail to reject the null hypothesis since the \\(p\\)-value is so high. This means that, the fact that the contingency table shows some non-uniform patters may be only a feature of this sample, and not of the population. It also may be that we do not have enough observations to rule out sampling variability. We cannot be certain.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing II</span>"
    ]
  },
  {
    "objectID": "Case_9.html#misc.-python-functions-from-case-9",
    "href": "Case_9.html#misc.-python-functions-from-case-9",
    "title": "9  Hypothesis testing II",
    "section": "9.4 Misc. Python functions from Case 9",
    "text": "9.4 Misc. Python functions from Case 9\n\nWe can use .sum(), .div() etc. to perform mathematical operations accross columns or rows in pandas.\nplt.legend() allows us to modify the legend in Python.\n.transpose() swaps the rows and columns of a DataFrame.\n.isin(interable) checks for each element in a series if it is in the given iterable.\ndf.date_py.dt.dayofweek gives the day of the week of a datetime object.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hypothesis testing II</span>"
    ]
  },
  {
    "objectID": "Case_10.html",
    "href": "Case_10.html",
    "title": "10  Natural language processing",
    "section": "",
    "text": "10.1 Preliminary modules\nimport nltk # imports the natural language toolkit\nnltk.download('punkt')\nnltk.download('stopwords')\nimport pandas as pd\nimport numpy  as np\nimport string\nimport plotly\nimport matplotlib.pyplot as plt\nfrom nltk.stem import PorterStemmer \nfrom wordcloud import WordCloud\nfrom pylab import rcParams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nimport re\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\12RAM\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\12RAM\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#about-nlp",
    "href": "Case_10.html#about-nlp",
    "title": "10  Natural language processing",
    "section": "10.2 About NLP",
    "text": "10.2 About NLP\n\n10.2.1 Challenge 1: Extraordinarily high dimensionality\nConsider the book War and Peace. It has 3 million characters. Can we view this as a long vector of strings taking values in a 3-million-dimensional space, and then apply machine learning methods here? This is a bad idea for two reasons:\n\nBasic approaches have terrible performance in such high-dimensional spaces\nThese approaches “miss out” on some important rules about language that we all know; e.g. that “don’t” and “do not” mean the same thing\n\nAs a result, a huge amount of NLP involves finding ways to summarize incredibly long vectors in concise ways, so that we can tractably explore, analyze, and model build with them later.\n\n\n10.2.2 Challenge 2: Text is context specific\nFor example, the word queen has many uses in English that are both very different and common:\n\nThe ruler of a country\nA size of mattress\nThe most powerful piece in chess\nThe mother insect in certain types of insect colonies\n\nGeneral purpose libraries will need to deal with all of these, but reviews for mattresses will almost always be about the second. This type of mismatch can result in misleading results that can easily be fixed by a team that is familiar with the underlying NLP computations.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#pre-processing-and-standardization",
    "href": "Case_10.html#pre-processing-and-standardization",
    "title": "10  Natural language processing",
    "section": "10.3 Pre-processing and standardization",
    "text": "10.3 Pre-processing and standardization\nStandardizing text involves many steps. Some of these include:\n\nCorrecting simple errors. For example, different text might use different encodings and you might find that special characters are corrupted and need to be fixed.\nCreating features (e.g. labeling nouns and verbs in a sentence).\nReplacing words and sentences altogether (e.g. standardizing spelling by changing “yuuuuuuck!” to “yuck”, or more extreme steps such as replacing words with near synonyms)\n\nIn a broad sense, standardization is similar to data cleaning with more conventional data; we are fixing errors, removing outliers, and transforming features. However, the details in NLP tend to be more complicated. One tip is that it is helpful to look at the lengths of each document to catch outliers.\n\n\n\n\n\n\nNote\n\n\n\nThe NLP literature uses common words in technical ways. For example a “document” means any standalone string that might be part of a larger collection. Sometimes these might be documents as we usually think of them (articles, papers, etc) as part of a collection of such documents. However, we’d also use “document” to refer to each tweet in a collection of tweets, or each review in a collection of reviews (as in the dataset we’ll work with now). Remember, a “document” is just an item containing natural language text that is part of a larger collection of similar such items.\n\n\n\n10.3.1 Libraries for NLP\nWe will be using Python’s Natural Language Toolkit (nltk) library. This library has functions that do most of the basics of NLP.\nNLTK is a great language for learning about NLP in Python. It implements nearly all standard algorithms used in NLP in pure Python, and it is very readable. It has great documentation and a companion book, and it often implements several alternatives to the same algorithm so that they can be compared.\nAnother NLP library in Python is spaCy. SpaCy is more modern than NLTK, and more focused on industry use than on education. It is opinionated and often implements only a single algorithm instead of all alternatives. It is focused on speed and efficiency over readability, and its source code is less readable as a result.\nBoth are great NLP libraries to become familiar with. In this case, we’ll use NLTK, but nearly all features that we cover can be used in spaCy too.\n\n\n\n\n\n\nNote\n\n\n\nMany text wrangling pipelines start a little before we do, with initial “cleaning” steps that involve things like: converting all characters to lower case, expanding contractions, etc.\n\n\n\n\n10.3.2 Tokenizing sentences\nJust like CSV data is composed of features, text data is composed of sentences. Thus, a natural first step is what is known as sentence tokenization: splitting a long document into its component sentences.\n\nAt first this might seem trivial: just split whenever you see a period. Unfortunately, the same symbol is used in other ways in English (e.g. to mark an abbreviation, as part of ellipses, etc.), and so slightly more care is required.\nFortunately, there are packages that will do this for us. Within nltk, we can use the nltk.sent_tokenize() function.\n\nExample:\n\n# sentence tokenization\nsentences = nltk.sent_tokenize('Tom wrote a letter to Mr. Plod, his uncle. \"I am arriving on Mon. 5 Jan. Please meet me at approx. 5 p.m.')\nfor sentence in sentences:\n    print(sentence)\n    print()\n\nTom wrote a letter to Mr. Plod, his uncle.\n\n\"I am arriving on Mon.\n\n5 Jan.\n\nPlease meet me at approx.\n\n5 p.m.\n\n\n\nIt may seem like sentence tokenization is easy, but remember that the period . can be used in many different ways. In the document:\nTom wrote a letter to Mr. Plod, his uncle. \"I am arriving on Mon. 5 Jan. Please meet me at approx. 5 p.m.\"\nA sentence tokenizer has to be intelligent enough to tokenize this as follows:\n[\n\"Tom wrote a letter to Mr. Plod, his uncle.\",\n\"I am arriving on Mon. 5 Jan.\"\n\"Please meet me at approx. 5 p.m.\"\n]\nAdditionally, the different ways that people use abbreviations and punctuation can make this a definitively non-trivial task.\n\n\n10.3.3 Tokenizing words\nWe may wish to split sentences into individual words. As with sentence tokenization, there is (i) a pretty good heuristic (split on spaces), (ii) a number of weird exceptions (e.g. compound words), and (iii) an existing package that does the job fairly well.\nTo do this task, one can use the nltk.word_tokenize() function from nltk:\nExample:\n\nnltk.word_tokenize(\"I don't like bananas\")\n\n['I', 'do', \"n't\", 'like', 'bananas']",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#wordclouds",
    "href": "Case_10.html#wordclouds",
    "title": "10  Natural language processing",
    "section": "10.4 Wordclouds",
    "text": "10.4 Wordclouds\nIn such cases, word clouds are a common and sometimes useful tool.\nTo elaborate, while wordclouds can be a useful way of quickly gaining high level insights into raw textual data, they are also limited. In some ways, they can be seen as the pie charts of NLP: often used, but also often hated. Some people would prefer if they didn’t exist at all. If used in the correct way, however, they definitely deserve their place in a data scientist’s toolbelt.\nThe main problem with word clouds is that they are difficult to interpret in a standard way. The layout algorithm has some randomness involved and although more common words are shown more prominently, it’s not possible to look at a word cloud and know which words are the most important, or how much more important these are than other words. Colours and rotation are also used randomly, making some words (e.g the ones in bright colours, positioned closer to the centre, with horizontal rotation) seem more important when in fact they are no more important than other words which were randomly assigned a less noticeable combination of color, rotation, and position.\nExample:\n\n# Sample text\ntext = \"\"\"\nPython is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. \nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. \nIt is often described as a \"batteries included\" language due to its comprehensive standard library.\n\"\"\"\n\n# Generate the word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud Example')\nplt.show()\n\n\n\n\n\n\n\n\nExplanation::\n\nSample Text: We define a sample text string text.\nGenerate Word Cloud: We create a WordCloud object with specified dimensions and background color, and then generate the word cloud from the sample text.\nPlot the Word Cloud: We use matplotlib to plot the word cloud. We set the figure size, use imshow to display the word cloud image, remove the axes with axis('off'), and add a title.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#n-grams",
    "href": "Case_10.html#n-grams",
    "title": "10  Natural language processing",
    "section": "10.5 \\(n\\)-grams",
    "text": "10.5 \\(n\\)-grams\nSince single words, known as 1-grams, are insufficient to understand the significance of certain words in our text, it is natural to consider blocks of words, or \\(n\\)-grams. \\(n\\)-grams fall under a broader category of techniques otherwise known as count-based representations. These are techniques to analyze documents by indicating how frequently certain types of structures occur throughout.\nThe simplest version of the \\(n\\)-grams model, for \\(n &gt; 1\\), is the bigram model, which looks at pairs of consecutive words. For example, the sentence “The quick brown fox jumps over the lazy dog” would have tokens “the quick”, “quick brown”,…, “lazy dog”. The following image explains this concept:\n\nThis has obvious advantages and disadvantages over looking at words individually:\nThis retains the structure of the overall document, and it paves the way for analyzing words in context; however, the dimension is vastly larger. For this reason, it is often prudent to start by extracting as much value out of 1-grams as possible, before working our way up to more complex structures.\nBigrams and trigrams are useful for analyzing a corpus because they capture the context and relationships between words, which can significantly enhance understanding and analysis of text data. Here are some key reasons why they are beneficial:\n\nContextual Understanding: They help to capture the context in which a word appears, providing more meaningful insights than individual words (unigrams).\nImproved Language Models: In natural language processing (NLP), bigrams and trigrams are used to build more accurate language models by considering word sequences rather than isolated words. This helps in predicting the next word in a sequence more effectively, which is crucial for tasks like text generation and auto-completion.\nDisambiguation: Certain words have multiple meanings depending on the context. Bigrams and trigrams help disambiguate such words by analyzing the surrounding words. For example, the word “bank” could refer to a financial institution or the side of a river. The bigrams “river bank” and “bank account” clarify the intended meaning.\nSentiment Analysis: In sentiment analysis, bigrams and trigrams can capture expressions and phrases that convey sentiment more accurately than single words. For example, “not good” (bigram) indicates a negative sentiment, which might be missed if “not” and “good” were analyzed separately.\nInformation Retrieval and Search: Using bigrams and trigrams improves the relevance of search results by considering common word pairs and phrases. This is especially useful in search engines and information retrieval systems, where user queries often consist of multi-word phrases.\nText Mining and Topic Modeling: Bigrams and trigrams help identify frequent phrases and common word combinations, which can be important for topic modeling and discovering patterns in the text. This aids in uncovering themes and topics that are not evident from analyzing single words.\n\n\n10.5.1 Word-document co-occurrence matrices\nThe simplest type of information would be whether a particular word occurs in particular documents. This leads to word-document co-occurrence matrices, where the \\((W, X)\\) entry of the word-document matrix \\(A\\) is set to 1 if word \\(W\\) occurs in document \\(X\\), and 0 otherwise.\nThere are many variants of this. In lieu of the fact that we are looking for count-based representations of our documents, one natural variable is the following: \\[A_{W,X}=\\#\\text{ times $W$ occurs in $X$},\\]\ni.e., the \\((W, X)\\) entry of the word-document matrix equals the number of times that word \\(W\\) occurs in document \\(X\\), rather than merely being a binary variable.\nCreating a word-document matrix in python:\n\n# Sample documents\ndocuments = [\n    \"Python is a high-level programming language\",\n    \"Python is dynamically typed and garbage-collected\",\n    \"Python supports multiple programming paradigms\",\n    \"Python is often described as a 'batteries included' language due to its comprehensive standard library\"\n]\n\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit and transform the documents\nX = vectorizer.fit_transform(documents)\n\n# Get the words (features)\nwords = vectorizer.get_feature_names_out()\n\n# Create the co-occurrence matrix\ndf = pd.DataFrame(X.toarray(), columns=words)\ndf.head()\n\n\n\n\n\n\n\n\nand\nas\nbatteries\ncollected\ncomprehensive\ndescribed\ndue\ndynamically\ngarbage\nhigh\n...\nlibrary\nmultiple\noften\nparadigms\nprogramming\npython\nstandard\nsupports\nto\ntyped\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n\n\n1\n1\n0\n0\n1\n0\n0\n0\n1\n1\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n1\n1\n1\n0\n1\n0\n0\n\n\n3\n0\n1\n1\n0\n1\n1\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n\n\n\n\n4 rows × 25 columns\n\n\n\n\n\n10.5.2 Counter objects\nThe Counter object is a part of Python’s collections module and is used to count the occurrences of elements in a collection. It is essentially a specialized dictionary designed for counting element occurances in an interable, where the keys are the unique elements and the values are the counts of those elements. In NLP, we can use Counter objects to get the most common \\(n\\)-grams.\n\nInitialization: Use Counter(interable) to initialize a Counter to count the occurrences of elements in the interable.\nMethods: Counter provides several useful methods, such as most_common() to get the most common elements and their counts, and elements() to get an iterator over elements repeating according to their counts.\nArithmetic Operations: You can perform arithmetic operations like addition, subtraction, intersection, and union on Counter objects.\n\nExample:\n\n# Create a Counter from a list\ndata = ['apple', 'banana', 'apple', 'orange', 'banana', 'apple']\ncounter = Counter(data)\n\n# Display the counter\nprint(counter)  # Output: Counter({'apple': 3, 'banana': 2, 'orange': 1})\n\n# Get the most common elements\nprint(counter.most_common(2))  # Output: [('apple', 3), ('banana', 2)]\n\n# Elements method (returns an iterator)\nprint(list(counter.elements()))  # Output: ['apple', 'apple', 'apple', 'banana', 'banana', 'orange']\n\n# Arithmetic operations\ncounter2 = Counter(['banana', 'banana', 'kiwi'])\ncounter.subtract(counter2)\nprint(counter)  # Output: Counter({'apple': 3, 'orange': 1, 'banana': 0, 'kiwi': -1})\n\nCounter({'apple': 3, 'banana': 2, 'orange': 1})\n[('apple', 3), ('banana', 2)]\n['apple', 'apple', 'apple', 'banana', 'banana', 'orange']\nCounter({'apple': 3, 'orange': 1, 'banana': 0, 'kiwi': -1})\n\n\nExplanation:\n\nInitialization: We create a Counter from a list of fruits.\nDisplay Counter: The Counter object shows the count of each element.\nMost Common Elements: The most_common(2) method returns the two most common elements and their counts.\nElements Method: The elements() method returns an iterator over the elements, repeating each as many times as its count.\nArithmetic Operations: We demonstrate subtraction between two Counter objects.\n\nIn Case 10, we used it to count the most common words.\nExample:\n\nlong_string=' '.join(documents)\nwords=nltk.word_tokenize(long_string)\ncounted_words=Counter(words)\ncounted_words.most_common(2)\n\n[('Python', 4), ('is', 3)]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#stop-words",
    "href": "Case_10.html#stop-words",
    "title": "10  Natural language processing",
    "section": "10.6 Stop words",
    "text": "10.6 Stop words\nStop words are very common words that are usually uninformative, and their very large occurrence values can distort the results of many NLP algorithms. They often include those that appear in every sentence of the English language: pronoums like “I”, prepositions like “but”, “of”, “and”, articles like “the”, etc.\nIt is common to pre-process text by removing words that you have a reason to believe are uninformative; these words are called stop words. Usually, it suffices to simply treat extremely common words as stop words. However, for specific types of applications it might make sense to use other stop words; e.g. the word “burger” when analyzing reviews of burger chains.\n\n\n\n\n\n\nNote\n\n\n\nStop words are often removed by default as a cleaning step in all NLP tasks. However, sometimes they can be useful. For example in authorship attribution (automatically detecting who wrote a specific piece of text by their ‘writing style’), stop words can be one of the most useful features, as they appear in nearly all texts, and yet each author uses them in slightly different ways.\n\n\nThe nltk library has a standard list of stopwords, which you can download by running nltk.download(\"stopwords\"). We can then load the stopwords package from the nltk.corpus and use it to load the stop words:\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint(stopwords.words(\"english\"))\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\12RAM\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nBelow is example code for removing stop words:\n\nsw=stopwords.words(\"english\")\n\nlist_sw=[]\nfor word in sw:\n    #check if the sw is in the text\n    word=\" \"+word+\" \"\n    if word in text:\n    # if sw is in the text, them we need to remove it from the text\n        text=re.sub(word,\" \",text)\n    # add the sw to a list of words we found in this text\n        list_sw.append(word)\n\n# print out text\nprint(text)\n\n# Print out found stop words\nprint(list_sw)\n\n\nPython high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability use significant indentation. \nPython dynamically typed garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented functional programming. \nIt often described \"batteries included\" language due comprehensive standard library.\n\n[' its ', ' is ', ' a ', ' the ', ' and ', ' as ', ' of ', ' with ', ' to ']",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#regular-expressions",
    "href": "Case_10.html#regular-expressions",
    "title": "10  Natural language processing",
    "section": "10.7 Regular Expressions",
    "text": "10.7 Regular Expressions\nHaving spent a lot of time on \\(n\\)-grams and how to featurize a document using them, we now take a break from nltk tools to introduce the most important text wrangling tool in Python (and many other languages): regular expressions.\nThe basic idea here is that you often want to perform some specific transformation (e.g. delete or substitute) every time that some possibly-complicated pattern occurs, e.g., the letter ‘A’, the word ‘hello’, any word containing the letters ‘a’,‘r’ in that order. Regular expressions are a compact and powerful language for expressing these sorts of patterns. This is super important whenever you are trying to clean a text dataset that contains thematically similar, but not exactly, the same errors.\nThe terse syntax of regular expressions has led to them having a reputation for being almost magical in some situations (with only a few characters, you can build complete computer programs) but also for being difficult to create and read, which can create more problems than they solve.\nIn Python, the re module provides regular expression matching operations and common operations. Regular expressions are a deep subject, with some documentation here: https://docs.python.org/3/library/re.html?highlight=regex.\nAs some simple examples, we have:\n\n. matches any character except \\n (newline)\n\\d matches any digit (this can also be written as [0-9])\n\\D matches any non-digit (this can also be written as [^0-9])\n\\w matches any alphanumeric character ([a-zA-Z0-9_])\n\\W matches any non-alphanumeric character ([^a-zA-Z0-9_])\n\nAs some more complex examples, regular expressions also allow you to quantify the number of times matches can occur. For example,\n\n[a-d]+ matches any time you get \\(\\{a,b,c,d\\}\\) one or more times in a row\n[a-d]{3} matches any time you get them exactly 3 times in a row\n[a-d]* matches any time you get them 0 or more times in a row\n\nFor now, we give a simple application based on the re.sub() function, which substitutes words that match a pattern:\n\nsentence = 'That was an \"interesting\" way to cook bread.'\npattern = r\"[^\\w]\" \n\n# the ^ character denotes 'not', \n# the \\w character denotes a word, and [] means\n# anything that matches anything in the brackets. \n# Together, this refers to any character that is not a word.\n\nprint(re.sub(pattern, \" \", sentence))\nprint(re.sub(pattern, \"\", sentence))\n\ntxt = \"Natesh loves all the foold and loveds sdaslo\"\n# x is a regex pattern object now, which can be used in other re functions, such as finditer\nx   = re.compile('lo')\n\niterator = x.finditer(txt)\nfor item in iterator:\n    # print(item)\n    print(item.span())\n    print(item.group())\n\nThat was an  interesting  way to cook bread \nThatwasaninterestingwaytocookbread\n(7, 9)\nlo\n(31, 33)\nlo\n(42, 44)\nlo\n\n\nSome other useful functions in re are\n\nre.split(): Divides a string into a list based on a pattern.\nre.findall(): Returns a list of all substrings in a string that match the pattern.\nre.sub(): Replaces occurrences of a specified pattern in a string with a replacement string.\nre.compile(): Compiles a regular expression pattern into a regex object for repeated use.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_10.html#part-of-speech-pos-tagging",
    "href": "Case_10.html#part-of-speech-pos-tagging",
    "title": "10  Natural language processing",
    "section": "10.8 Part-of-speech (POS) tagging",
    "text": "10.8 Part-of-speech (POS) tagging\nIn English, there are eight main parts of speech - nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections. The purpose of POS tagging is to label each word in a document with its part of speech. Unsurprisingly, POS tagging can be very difficult to do by hand. nltk has a default function for this, called nltk.pos_tag(), which we will use. As a word of warning, this function is far from infallible, especially on informal text (e.g. website reviews, forum posts, text messages, etc), and words in English often exhibit POS drift (e.g. the drift of “Google” from noun to verb):\nHere are some key use cases for POS tagging in NLP:\n\nText Parsing and Syntactic Analysis\n\nPOS tagging is essential for syntactic parsing, which involves analyzing the grammatical structure of a sentence.\nHelps in identifying the relationships between words and understanding the sentence structure.\n\nNamed Entity Recognition (NER)\n\nHelps in distinguishing between names, locations, organizations, and other proper nouns.\nPOS tags help NER systems to understand the context and improve accuracy in identifying entities.\n\nMachine Translation\n\nProvides grammatical information that aids in translating text from one language to another.\nHelps in maintaining the syntactic structure and grammatical correctness in the translated text.\n\nInformation Retrieval and Search Engines\n\nImproves the relevance of search results by understanding the query’s context and filtering out irrelevant results.\nEnhances search algorithms by recognizing and prioritizing different parts of speech.\n\nSentiment Analysis\n\nIdentifies adjectives and adverbs that are crucial for determining the sentiment of a sentence.\nHelps in understanding the context and polarity of opinions expressed in the text.\n\nSpeech Recognition and Text-to-Speech\n\nEnhances the accuracy of speech recognition systems by providing context for homophones and ambiguous words.\nHelps in generating natural and grammatically correct speech in text-to-speech systems.\n\nKeyword Extraction and Text Summarization\n\nAids in identifying key phrases and important information in the text.\nImproves the quality of summaries by focusing on nouns, verbs, and other significant parts of speech.\n\nCoreference Resolution\n\nHelps in linking pronouns to the nouns they refer to within a text.\nFacilitates better understanding and continuity in document-level NLP tasks.\n\nDependency Parsing\n\nProvides a basis for dependency parsing, which focuses on the dependencies between words in a sentence.\nHelps in understanding the grammatical relations and hierarchical structure of the sentence.\n\n\nBelow is how we can do POS tagging in Python:\n\nnltk.download('averaged_perceptron_tagger')\n#https://www.nltk.org/book/ch05.html\ntext_word_token = nltk.word_tokenize(\"Kelly is not having a good day because she has pancreatitis\")\n#text_word_token = nltk.word_tokenize(data.text[0])\nnltk.pos_tag(text_word_token)\n#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\12RAM\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\n[('Kelly', 'NNP'),\n ('is', 'VBZ'),\n ('not', 'RB'),\n ('having', 'VBG'),\n ('a', 'DT'),\n ('good', 'JJ'),\n ('day', 'NN'),\n ('because', 'IN'),\n ('she', 'PRP'),\n ('has', 'VBZ'),\n ('pancreatitis', 'NN')]\n\n\nThe following is a list of tags and their meaning:\n\n‘NNP’: Proper noun, singular - This tag is used for singular proper nouns, which are names of specific people, places, or things. In this case, ‘Jairo’ is a proper noun, and so it is tagged with ‘NNP’.\n‘VBZ’: Verb, 3rd person singular present - This tag is used for third person singular verbs in the present tense (e.g. he runs, she eats). In this case, ‘is’ is a third person singular present verb, and so it is tagged with ‘VBZ’.\n‘VBG’: Verb, gerund or present participle - This tag is used for present participles and gerunds, which are verb forms that end in -ing (e.g. running, eating). In this case, ‘having’ is a present participle, and so it is tagged with ‘VBG’.\n‘DT’: Determiner - This tag is used for determiners, which are words that specify or indicate the noun that follows. In this case, ‘a’ is a determiner that indicates the noun ‘day’, and so it is tagged with ‘DT’.\n‘JJ’: Adjective - This tag is used for adjectives, which are words that describe or modify nouns or pronouns. In this case, ‘good’ is an adjective that describes the noun ‘day’, and so it is tagged with ‘JJ’.\n‘NN’: Noun, singular or mass - This tag is used for singular or mass nouns, which are common nouns that represent people, places, things, or concepts. In this case, ‘day’ is a singular noun, and so it is tagged with ‘NN’.\n\nNLTK provides documentation for each tag, which can be queried using the tag itself; e.g. nltk.help.upenn_tagset('RB'). Since POS is context-sensitive, POS-taggers must usually be trained on an existing corpus that has been tagged by professional linguists (possibly alongside unlabeled data to take advantage of semi-supervised methods). The most popular tag set is called the Penn Treebank set:\n\n# We can get more details about any POS tag using the help function of nltk\nnltk.download('tagsets')\nnltk.help.upenn_tagset('IN')\n\nIN: preposition or conjunction, subordinating\n    astride among uppon whether out inside pro despite on by throughout\n    below within for towards near behind atop around if like until below\n    next into if beside ...\n\n\n[nltk_data] Downloading package tagsets to\n[nltk_data]     C:\\Users\\12RAM\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package tagsets is already up-to-date!\n\n\n\n10.8.1 Misc. Python functions from Case 10\n\nrcParams['figure.figsize'] = wid, hei sets the width and height of a figure displayed in a notebook.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural language processing</span>"
    ]
  },
  {
    "objectID": "Case_11.html",
    "href": "Case_11.html",
    "title": "11  Linear Regression I",
    "section": "",
    "text": "11.1 Preliminary modules\n### Load relevant packages\nimport pandas                  as pd\nfrom   scipy import stats\nimport numpy                   as np\nimport matplotlib.pyplot       as plt\nimport seaborn                 as sns\nimport statsmodels.formula.api as sm\n# import chart_studio.plotly     as py\n# https://community.plot.ly/t/solved-update-to-plotly-4-0-0-broke-application/26526/2\nimport os\n\n%matplotlib inline\nplt.style.use('ggplot')\nLinear regression is concerned with the following problem: Suppose that \\(X\\) and \\(Y\\) are some attributes of a population. What is the relationship between \\(X\\) and \\(Y\\)? How can we use \\(X\\) to predict \\(Y\\), or how can we use \\(X\\) to explain \\(Y\\)?\nFor example, questions of this form include:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression I</span>"
    ]
  },
  {
    "objectID": "Case_11.html#preliminary-modules",
    "href": "Case_11.html#preliminary-modules",
    "title": "11  Linear Regression I",
    "section": "",
    "text": "How is location, square feet, parking available related to the price of an Airbnb?\nHow is hours played and age related to win rate in League of Legends?\nHow are creatine and protein consumption related to deadlift 1RM?\nHow is treatment (A or B) related to pain levels of patients?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression I</span>"
    ]
  },
  {
    "objectID": "Case_11.html#simple-linear-regression",
    "href": "Case_11.html#simple-linear-regression",
    "title": "11  Linear Regression I",
    "section": "11.2 Simple linear regression",
    "text": "11.2 Simple linear regression\nSimple linear regression is when we are trying to find the line of best fit between one variable \\(X\\) and another \\(Y\\). A line has two parameters – intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)). Thus a linear model for the relationship between one variable \\(X\\) and another \\(Y\\) can be represented as \\[ Y = \\beta_0 + \\beta_1 X + \\text{error} .\\]\nThe model says that any subjects value of \\(X\\) and \\(Y\\) can be represented as \\(Y = \\beta_0 + \\beta_1 X + \\text{error}.\\) The interpretation of the coefficient \\(\\beta_1\\) is the following: an increase of one unit in \\(X\\) will on average lead to a change of \\(\\beta_1\\) in \\(Y\\). The intercept \\(\\beta_0\\) can be thought of as a sort of “baseline” - it is the average value of \\(Y\\) when \\(X=0\\). The error is the individual variation around the average. For instance, we can use the tips dataset as an example. We can plot the line of best fit using sns.lmplot().\n\ntips=sns.load_dataset(\"tips\")\n\nsns.lmplot(x = 'total_bill', y = 'tip', data = tips, scatter_kws = {'color': (160/255,199/255,14/255)})\nplt.title(\"Bill vs. Tip\", fontsize=20, verticalalignment='bottom')\nplt.xlabel(\"Bill\")\nplt.ylabel(\"Tip\")\n\nText(33.775000000000006, 0.5, 'Tip')\n\n\n\n\n\n\n\n\n\nNotice how the line represents the average value of the tip given the bill? The points do not exactly fall on the line, they vary around it. This variation is the error term in the above model. The intercept and slope of the line are the estimates of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\n\n\n\nNote\n\n\n\nJust like in the hypothesis testing case, we have to distinguish between a population and a sample. In this case, the parameters \\(\\beta_0\\) and \\(\\beta_1\\) are the population values which are unknown. In reality, we have a sample from the population and use the sample to estimate \\(\\beta_0\\) and \\(\\beta_1\\), given by \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\).\n\n\nA residual is the difference between an actual observed value of \\(Y\\) and the one predicted by the linear regression line \\(\\hat Y\\). That is, \\(\\hat\\epsilon_i=Y_i-\\hat Y_i\\). It is the signed error between the observed and the value guessed by the line.\nWe can estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\) as follows\n\ntips.head()\nmodel= 'tip~total_bill'\nlm1   = sm.ols(formula = model, data = tips).fit()\nprint(lm1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    tip   R-squared:                       0.457\nModel:                            OLS   Adj. R-squared:                  0.454\nMethod:                 Least Squares   F-statistic:                     203.4\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           6.69e-34\nTime:                        14:16:28   Log-Likelihood:                -350.54\nNo. Observations:                 244   AIC:                             705.1\nDf Residuals:                     242   BIC:                             712.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.9203      0.160      5.761      0.000       0.606       1.235\ntotal_bill     0.1050      0.007     14.260      0.000       0.091       0.120\n==============================================================================\nOmnibus:                       20.185   Durbin-Watson:                   2.151\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               37.750\nSkew:                           0.443   Prob(JB):                     6.35e-09\nKurtosis:                       4.711   Cond. No.                         53.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThere is a lot of output contained in the linear regression model output. First, the column coef gives \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\). We interpret the coeficient for total bill as: on average, we expect the tip to increase by 0.1050 for each dollar increase in total bill.\nThe column P&gt;|t| displays the \\(p\\)-values for the test \\(H_0\\colon \\beta_i=0\\) vs \\(H_a\\colon \\beta_i\\neq 0\\). If a coefficient is 0 theoretically, that means that the associated variable is not related to the response variable \\(Y\\). For instance, if the coefficient for total bill was 0, then that implies that the value of the bill does not impact the tip amount. So, if this \\(p\\)-value is not small, it is an indication that there is no relationship between total bill and tip. In the intercept \\(p\\)-value is small, then this means that the intercept is 0. In this case, both \\(p\\)-values are very small, and so both parameters are important.\nAnother quantity you will see is the \\(R\\)-squared. Note that the table shows \\(R\\)-squared and adjusted \\(R\\)-squared - if you have many variables, then you should use the adjusted \\(R\\)-squared. Both of these quantities are always going to be between 0 and 1. For the tips data, this quantity is 0.457 = 45.7%. This means that the total bill explains 45.7% of the variation in tips. The remaining 55% is due to other, unknown factors. The higher the (adjusted or non-adjusted) \\(R\\)-squared, the higher the percentage of observed variation that can be explained by the model.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression I</span>"
    ]
  },
  {
    "objectID": "Case_11.html#multiple-linear-regression",
    "href": "Case_11.html#multiple-linear-regression",
    "title": "11  Linear Regression I",
    "section": "11.3 Multiple linear regression",
    "text": "11.3 Multiple linear regression\nMultiple linear regression consists of a linear model with \\(p&gt;1\\) variables. The theoretical model is given by\n\\[Y = \\beta_0 + \\beta_1 X_1+ \\beta_2 X_2+\\ldots+ \\beta_p X_p + \\mathrm{error}.\\]\nIn Python, we can add another variable to the model as follows:\n\nmodel2= 'tip~total_bill+sex'\nlm2   = sm.ols(formula = model2, data = tips).fit()\nprint(lm2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    tip   R-squared:                       0.457\nModel:                            OLS   Adj. R-squared:                  0.452\nMethod:                 Least Squares   F-statistic:                     101.3\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           1.18e-32\nTime:                        14:16:28   Log-Likelihood:                -350.52\nNo. Observations:                 244   AIC:                             707.0\nDf Residuals:                     241   BIC:                             717.5\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.9067      0.175      5.182      0.000       0.562       1.251\nsex[T.Female]     0.0266      0.138      0.192      0.848      -0.246       0.299\ntotal_bill        0.1052      0.007     14.110      0.000       0.091       0.120\n==============================================================================\nOmnibus:                       20.499   Durbin-Watson:                   2.149\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               38.652\nSkew:                           0.447   Prob(JB):                     4.05e-09\nKurtosis:                       4.733   Cond. No.                         63.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe above model takes bill total and sex into account simultaneously. Sex takes only two values – male and female. Such variables are called categorical variables. The way we interpret the coefficients of categorical variables in the linear model is slightly different from those of numeric variables.\nEach categorical variable in a regression model includes a baseline category. To figure out the baseline category, it is the one missing from the table in the regression output. In this case, male is missing, so it can taken to be the baseline. The interpretation of the coefficient sex[T.Female] is that, accounting for total bill, the average tip from a female is 0.0266 higher than from a male. (Note that this coefficient is not significant - does not have a small p-value - and so there is no evidence that the tips from males and females differ, after accounting for total bill.)\nLet’s make a bigger model:\n\nmodel3= 'tip~total_bill+sex+smoker+day+time+size'\nlm3   = sm.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    tip   R-squared:                       0.470\nModel:                            OLS   Adj. R-squared:                  0.452\nMethod:                 Least Squares   F-statistic:                     26.06\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           1.20e-28\nTime:                        14:16:28   Log-Likelihood:                -347.48\nNo. Observations:                 244   AIC:                             713.0\nDf Residuals:                     235   BIC:                             744.4\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.5908      0.256      2.310      0.022       0.087       1.095\nsex[T.Female]      0.0324      0.142      0.229      0.819      -0.247       0.311\nsmoker[T.No]       0.0864      0.147      0.589      0.556      -0.202       0.375\nday[T.Fri]         0.1623      0.393      0.412      0.680      -0.613       0.937\nday[T.Sat]         0.0408      0.471      0.087      0.931      -0.886       0.968\nday[T.Sun]         0.1368      0.472      0.290      0.772      -0.793       1.066\ntime[T.Dinner]    -0.0681      0.445     -0.153      0.878      -0.944       0.808\ntotal_bill         0.0945      0.010      9.841      0.000       0.076       0.113\nsize               0.1760      0.090      1.966      0.051      -0.000       0.352\n==============================================================================\nOmnibus:                       27.860   Durbin-Watson:                   2.096\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               52.555\nSkew:                           0.607   Prob(JB):                     3.87e-12\nKurtosis:                       4.923   Cond. No.                         281.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe observe that size and total_bill are the only important variables in explaining tips, even when accounting for all other variables.\n\n\n\n\n\n\nImportant\n\n\n\nWhen there are multiple variables in the model, the interpretation of the coefficients changes. It has to include “accounting for the remaining variables in the model, …”. For example, above, we can interpret the coefficient for total bill as: “Accounting for size, sex, smoking, time and day, the average increase in tips per dollar increase in total bill is 0.0945.”\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that in the above interpretation, we say average increase. This is very important. The model does not say that a dollar increase in total bill will increase tips by 0.0945. It says that on average, that is what we would expect. But it is not for certain that each dollar increase will increase your tip. In reality, some people tip higher, some people tip lower. The model is saying that these average out to a 0.0945 increase per dollar.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression I</span>"
    ]
  },
  {
    "objectID": "Case_11.html#python-syntax-for-regression",
    "href": "Case_11.html#python-syntax-for-regression",
    "title": "11  Linear Regression I",
    "section": "11.4 Python syntax for regression",
    "text": "11.4 Python syntax for regression\nThe following is an overview of the Python syntax for regression for building a regression model.\n\nTo do linear regression in Python, we need to import the library from statsmodels: import statsmodels.formula.api as smf.\nNext, we use a formula object to define the variables in our model. The intercept is included by default. Define the formula object: formula = 'dependent_variable ~ independent_variable1 + independent_variable2 + ...'. The names should match up with the column names in the DataFrame.\nTo fit the model, we use the following syntax: model = smf.ols(formula, data=data).fit(). Here data is the DataFrame name.\nTo print the summary of the regression results: print(model.summary())",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Linear Regression I</span>"
    ]
  },
  {
    "objectID": "Case_12.html",
    "href": "Case_12.html",
    "title": "12  Linear Regression II",
    "section": "",
    "text": "12.1 Preliminary packages\n### Load relevant packages\nimport pandas                  as pd\nimport numpy                   as np\nimport matplotlib.pyplot       as plt\nimport seaborn                 as sns\nimport statsmodels.formula.api as smf\nimport statsmodels.api         as sm\nimport scipy\n\n%matplotlib inline\nplt.style.use('ggplot')\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_12.html#normality-of-the-error",
    "href": "Case_12.html#normality-of-the-error",
    "title": "12  Linear Regression II",
    "section": "12.2 Normality of the error",
    "text": "12.2 Normality of the error\nFirst, linear regression works best when the residuals are all roughly normally distributed with zero mean and the same variance. You may have come across statements before claiming that ``linear regression assumes the data are normally distributed’’, which is not entirely correct. Linear regression can still be a useful and powerful (and theoretically justified!) tool even if the data deviates from this assumption. That said, a distribution with fat tails - that is, a high propensity to observe outliers - is a particular problem for linear regression, because points “in the tails”, i.e., that are far away from their fitted values can disproportionately affect the fitted coefficients and the predictions.\nFor example, in this model:\n\ntips=sns.load_dataset(\"tips\")\nmodel3= 'tip~total_bill+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    tip   R-squared:                       0.470\nModel:                            OLS   Adj. R-squared:                  0.452\nMethod:                 Least Squares   F-statistic:                     26.06\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           1.20e-28\nTime:                        14:16:34   Log-Likelihood:                -347.48\nNo. Observations:                 244   AIC:                             713.0\nDf Residuals:                     235   BIC:                             744.4\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.5908      0.256      2.310      0.022       0.087       1.095\nsex[T.Female]      0.0324      0.142      0.229      0.819      -0.247       0.311\nsmoker[T.No]       0.0864      0.147      0.589      0.556      -0.202       0.375\nday[T.Fri]         0.1623      0.393      0.412      0.680      -0.613       0.937\nday[T.Sat]         0.0408      0.471      0.087      0.931      -0.886       0.968\nday[T.Sun]         0.1368      0.472      0.290      0.772      -0.793       1.066\ntime[T.Dinner]    -0.0681      0.445     -0.153      0.878      -0.944       0.808\ntotal_bill         0.0945      0.010      9.841      0.000       0.076       0.113\nsize               0.1760      0.090      1.966      0.051      -0.000       0.352\n==============================================================================\nOmnibus:                       27.860   Durbin-Watson:                   2.096\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               52.555\nSkew:                           0.607   Prob(JB):                     3.87e-12\nKurtosis:                       4.923   Cond. No.                         281.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe can find the residuals and fitted values using model_object.resid and model_object.fitted, respectively.\nFor example:\n\nres = lm3.resid\nfv= lm3.fittedvalues\n\nThe next step is to check normality with plots.\n\n12.2.1 Histogram of residuals\nFirst, we can use a histogram to check normality of the residuals. The histogram should appear bell shaped - it should match up roughly with a normal density overlaid. To see this, we show now ho to make a histogram of residuals, and highlight points that are more than 3 median absolute deviations (MADs) away from the center. These are points we should investigate. The median aboslute deviation is a measure of scale which is not corrupted by outliers. You can use .mad() in pandas to compute this.\nIf there are more points in the sample, we can up the number of MADs away we can look at to be more than 3. Note that using the MAD is safer if you suspect there are outliers, as the standard deviation is easily corrupted by outliers. Under the normal distribution, it holds that roughly 1.5 MAD=standard deviation.\n\nplt.hist(lm3.resid, \n    density=True, \n    bins=20,#  draw a histogram with 20 bins of equal width\n    label=\"residuals\" # label for legend\n    )\n# now plot the normal distribution for comparison\nxx = np.linspace(lm3.resid.min(), lm3.resid.max(), num=1000)\n# Compute MAD\nmad_val=np.median(np.absolute(lm3.resid))\n# Add normal density curve with mean 0 and sd 1.5*mad_val\nplt.plot(xx, scipy.stats.norm.pdf(xx, loc=0.0, scale=1.5*mad_val),    label=\"normal distribution\")\n# Label suspicious points - these are not necessarily erroneous points - but they are far from the middle\noutliers = np.abs(lm3.resid)&gt;3*mad_val\nsns.rugplot(lm3.resid[outliers],\n            color=\"C5\", # otherwise the rugplot has the same color as the histogram\n            label=\"outliers\")\nplt.legend(loc=\"upper right\");\n\n\n\n\n\n\n\n\nIn this case, the tails seem to be a little heavier than what we would expect to see under the normal distribution. You can tell from the amount of green points in the rug plot, as well as the bars rising above the tails.\n\n\n12.2.2 QQPlot\nAnother way to check normality is with a qq-plot. A qq-plot compares the quantiles of the sample to the quantiles of the theoretical normal distribution. The x-axis represents the theoretical quantiles. The y-axis represents the sample quantiles. If the sample follows a normal distribution, the points in the qq-plot will approximately lie on a line.\n\n\n\n\n\n\nNote\n\n\n\nWe expect to see the points deviate from the line \\(y=x\\) at the ends of the line, even if the data are normal.\n\n\nInterpretation:\n\nStraight Line: If the points lie on or near the straight line, the sample appears normal.\nHeavy Tails: Points deviating upwards or downwards at the ends suggest the sample has heavier or lighter tails than the normal distribution.\nS-Shape: Points forming an S-shape indicate the sample has lighter tails and a heavier center than the normal distribution.\n\nExample:\n\nsm.qqplot(lm3.resid, line=\"s\");\n\n\n\n\n\n\n\n\nFor instance, the points have a slight S, indicating a slight skew. The direction of the S indicates a right skew. This means that there is more of a propensity for the tips to be unexpectedly higher than lower. This makes sense intuitively, as tips have a lower bound but no upper bound.\nNevertheless, the S is not super pronounced, and so we can conclude it is not too problematic.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_12.html#heteroscedasticity",
    "href": "Case_12.html#heteroscedasticity",
    "title": "12  Linear Regression II",
    "section": "12.3 Heteroscedasticity",
    "text": "12.3 Heteroscedasticity\nAnother troublesome situation that can be detected using residual analysis is heteroscedasticity, which means that the residuals have small variance for in some subsets of the data, and high variance in others. The opposite of heteroscedasticity is homoscedasticity, which is what we want to see in the data, and means that the residuals have similar variance across all subsets of the data.\n\n12.3.1 Plotting the residuals against the fitted values\nTo check this assumption, we can use some other diagnostic plots. One plot is that of the fitted values \\(\\hat Y\\) (\\(x\\)-axis) against the residuals \\(\\hat\\epsilon\\) (\\(y\\)-axis). If the error depends on \\(\\hat y\\), then the identically distributed assumption on the errors is probably not valid. If the assumptions are valid, we should observe on the plots that at all levels of the response, the mean of the residuals is 0 and the variance remains the same. Thus, we should see a horizontal band centered at 0 containing the observations.\nExample:\n\nplt.scatter(fv,res);\nplt.ylim(-4,4)\nplt.axhline(0)\n\n\n\n\n\n\n\n\nObserve that as the tip amount increases, so does the variance. That is, the points form a fan. This is a strong indication of heteroscedasticity. We should apply a transformation to resolve the issue.\n\nBe VERY careful about the scale of your plot, as it can affect your interpretation. Zooming out or in too much can make everything look fine. In addition, the \\(y\\)-axis not being centered at 0 can cause you to misinterpret the plot.\n\n\n\n12.3.2 Plotting the residuals against the covariates\nPlotting the residuals against the covariates can reveal dependence between the errors. For instance, if time is a covariate, you can plot the residuals over time to see if they have any relationship with time. If there appears to be dependence among the residuals, then the assumptions of the model are violated. That is, in these plots we should also see a horizontal band centered at 0 containing the observations. If not, then the residuals have a relationship with the given covariate.\nExample:\n\nplt.scatter(tips['total_bill'] ,res);\nplt.ylim(-4,4)\nplt.axhline(0)\n\n\n\n\n\n\n\n\nHere, the we observe that the variance of the residuals is related to the total bill.\n\n\n12.3.3 Plotting the fitted values against the response\nAnother plot is that of the fitted values against the observed response \\(Y\\). This gives an idea of the overall fit of the model. We should observe the points scatters around the line \\(y=x\\).\n\nplt.scatter(tips['tip'] ,fv);\nplt.axline((0, 0), slope=1, color='blue', label='y = x')\n\n\n\n\n\n\n\n\nWe observe that the regression line underestimates the tips at small values and overestimates them at high values. The model fits only moderately well. In this case, handling the large observations and heteroscedasticity should help.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_12.html#transformations",
    "href": "Case_12.html#transformations",
    "title": "12  Linear Regression II",
    "section": "12.4 Transformations",
    "text": "12.4 Transformations\nWhen there is heteroskedasticity or issues with the plot of the fitted values against the response, we should apply a transformation to the model to correct this. The transformation should be guided by domain knowledge where possible. More advanced methods to choose transformations are outside of the scope of how to correctly apply transfomations is too wide for this course. However, it is common to try the square root and log transformations of the response. So for now, you can try each of those. You can also transform the regressors if you suspect that the response has a linear relationship with a transformation of the regressors. For instance, you might try:\n\n# Let's see what these look like\nplt.scatter(np.log(tips['tip']) ,tips['total_bill'])\nplt.show()\nplt.scatter(np.sqrt(tips['tip']) ,tips['total_bill'])\nplt.show()\nplt.scatter(tips['tip'] ,tips['total_bill'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel3= 'np.sqrt(tip)~total_bill+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\nplt.scatter(np.sqrt(tips['tip']) ,lm3.fittedvalues);\nplt.axline((1, 1), slope=1, color='blue', label='y = x')\nplt.show()\n## \nplt.scatter(lm3.fittedvalues ,lm3.resid);\nplt.ylim(-1,1)\nplt.axhline(0)\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           np.sqrt(tip)   R-squared:                       0.467\nModel:                            OLS   Adj. R-squared:                  0.448\nMethod:                 Least Squares   F-statistic:                     25.70\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           2.50e-28\nTime:                        14:16:35   Log-Likelihood:                -29.902\nNo. Observations:                 244   AIC:                             77.80\nDf Residuals:                     235   BIC:                             109.3\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          1.0386      0.070     14.920      0.000       0.901       1.176\nsex[T.Female]      0.0139      0.039      0.362      0.718      -0.062       0.090\nsmoker[T.No]       0.0188      0.040      0.470      0.639      -0.060       0.097\nday[T.Fri]         0.0437      0.107      0.408      0.684      -0.167       0.255\nday[T.Sat]        -0.0059      0.128     -0.046      0.963      -0.258       0.246\nday[T.Sun]         0.0432      0.128      0.337      0.737      -0.210       0.296\ntime[T.Dinner]    -0.0102      0.121     -0.084      0.933      -0.249       0.228\ntotal_bill         0.0252      0.003      9.638      0.000       0.020       0.030\nsize               0.0505      0.024      2.072      0.039       0.002       0.098\n==============================================================================\nOmnibus:                        3.509   Durbin-Watson:                   2.060\nProb(Omnibus):                  0.173   Jarque-Bera (JB):                3.735\nSkew:                           0.122   Prob(JB):                        0.154\nKurtosis:                       3.555   Cond. No.                         281.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe heteroscedasticity is improved, however, the fitted values don’t line up well with the response.\nWe can also try the log transform:\n\nmodel3= 'np.log(tip)~total_bill+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\nplt.scatter(np.log(tips['tip']) ,lm3.fittedvalues);\nplt.axline((1, 1), slope=1, color='blue', label='y = x')\nplt.show()\nplt.scatter(lm3.fittedvalues ,lm3.resid);\nplt.ylim(-1,1)\nplt.axhline(0)\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            np.log(tip)   R-squared:                       0.444\nModel:                            OLS   Adj. R-squared:                  0.425\nMethod:                 Least Squares   F-statistic:                     23.47\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           2.80e-26\nTime:                        14:16:35   Log-Likelihood:                -71.628\nNo. Observations:                 244   AIC:                             161.3\nDf Residuals:                     235   BIC:                             192.7\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.2668      0.083      3.230      0.001       0.104       0.430\nsex[T.Female]      0.0181      0.046      0.396      0.693      -0.072       0.108\nsmoker[T.No]       0.0180      0.047      0.380      0.704      -0.075       0.111\nday[T.Fri]         0.0532      0.127      0.419      0.676      -0.197       0.303\nday[T.Sat]        -0.0155      0.152     -0.102      0.919      -0.315       0.284\nday[T.Sun]         0.0629      0.152      0.413      0.680      -0.237       0.363\ntime[T.Dinner]    -0.0139      0.144     -0.097      0.923      -0.297       0.269\ntotal_bill         0.0283      0.003      9.140      0.000       0.022       0.034\nsize               0.0581      0.029      2.011      0.045       0.001       0.115\n==============================================================================\nOmnibus:                        4.024   Durbin-Watson:                   2.012\nProb(Omnibus):                  0.134   Jarque-Bera (JB):                3.685\nSkew:                          -0.251   Prob(JB):                        0.158\nKurtosis:                       3.332   Cond. No.                         281.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe heteroscedasticity is improved, however, the fitted values don’t line up well with the response and the residuals are biased downwards.\nNow, we can also try transformating both variables:\n\n# Let's see what these look like\nplt.scatter(np.log(tips['tip']) ,np.log(tips['total_bill']))\nplt.show()\nplt.scatter(np.sqrt(tips['tip']) ,np.sqrt(tips['total_bill']))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel3= 'np.sqrt(tip)~np.sqrt(total_bill)+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\nplt.scatter(np.sqrt(tips['tip']) ,lm3.fittedvalues);\nplt.axline((1, 1), slope=1, color='blue', label='y = x')\nplt.show()\nplt.scatter(lm3.fittedvalues , lm3.resid);\nplt.ylim(-1,1)\nplt.axhline(0)\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:           np.sqrt(tip)   R-squared:                       0.477\nModel:                            OLS   Adj. R-squared:                  0.459\nMethod:                 Least Squares   F-statistic:                     26.75\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           2.90e-29\nTime:                        14:16:35   Log-Likelihood:                -27.601\nNo. Observations:                 244   AIC:                             73.20\nDf Residuals:                     235   BIC:                             104.7\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept               0.5037      0.097      5.179      0.000       0.312       0.695\nsex[T.Female]           0.0165      0.038      0.431      0.667      -0.059       0.092\nsmoker[T.No]            0.0158      0.039      0.400      0.690      -0.062       0.093\nday[T.Fri]              0.0529      0.106      0.499      0.618      -0.156       0.262\nday[T.Sat]              0.0032      0.127      0.025      0.980      -0.247       0.253\nday[T.Sun]              0.0494      0.127      0.388      0.698      -0.201       0.300\ntime[T.Dinner]         -0.0212      0.120     -0.177      0.860      -0.258       0.215\nnp.sqrt(total_bill)     0.2406      0.024      9.957      0.000       0.193       0.288\nsize                    0.0468      0.024      1.939      0.054      -0.001       0.094\n==============================================================================\nOmnibus:                        7.867   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.020   Jarque-Bera (JB):               11.345\nSkew:                           0.194   Prob(JB):                      0.00344\nKurtosis:                       3.982   Cond. No.                         69.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel3= 'np.log(tip)~np.log(total_bill)+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\nplt.scatter(np.log(tips['tip']) ,lm3.fittedvalues);\nplt.axline((1, 1), slope=1, color='blue', label='y = x')\nplt.show()\nplt.scatter(lm3.fittedvalues ,lm3.resid);\nplt.ylim(-1,1)\nplt.axhline(0)\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            np.log(tip)   R-squared:                       0.478\nModel:                            OLS   Adj. R-squared:                  0.460\nMethod:                 Least Squares   F-statistic:                     26.90\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           2.14e-29\nTime:                        14:16:35   Log-Likelihood:                -63.952\nNo. Observations:                 244   AIC:                             145.9\nDf Residuals:                     235   BIC:                             177.4\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept             -0.9227      0.156     -5.922      0.000      -1.230      -0.616\nsex[T.Female]          0.0253      0.044      0.570      0.569      -0.062       0.113\nsmoker[T.No]           0.0075      0.046      0.165      0.869      -0.082       0.097\nday[T.Fri]             0.0704      0.123      0.571      0.568      -0.172       0.313\nday[T.Sat]           1.36e-05      0.147   9.23e-05      1.000      -0.290       0.290\nday[T.Sun]             0.0719      0.148      0.487      0.626      -0.219       0.363\ntime[T.Dinner]        -0.0317      0.139     -0.228      0.820      -0.306       0.242\nnp.log(total_bill)     0.6135      0.060     10.209      0.000       0.495       0.732\nsize                   0.0520      0.028      1.882      0.061      -0.002       0.106\n==============================================================================\nOmnibus:                        8.916   Durbin-Watson:                   1.943\nProb(Omnibus):                  0.012   Jarque-Bera (JB):               14.268\nSkew:                          -0.184   Prob(JB):                     0.000797\nKurtosis:                       4.126   Cond. No.                         54.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis fit is not bad!",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_12.html#handling-outliers",
    "href": "Case_12.html#handling-outliers",
    "title": "12  Linear Regression II",
    "section": "12.5 Handling outliers",
    "text": "12.5 Handling outliers\nOutliers can destabilize a model and significantly reduce their predictive ability despite only representing a fringe subset of the overall data. The starting point should always be to inspect the initial raw data for any data points with unusually small or large values for certain features, and understand why they are different. If there is something clearly wrong with those data (like a misplaced decimal point), or if the reason for these small or large values can be effectively explained via an external factor that is not captured by the data itself, then this justifies removing them. Otherwise, it is best to keep these data points in mind throughout the modeling process, and deal with them during the modeling process itself. If the tail of the residuals is unusually large, one can try a robust regression procedure.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_12.html#more-on-the-linear-regression-output",
    "href": "Case_12.html#more-on-the-linear-regression-output",
    "title": "12  Linear Regression II",
    "section": "12.6 More on the linear regression output",
    "text": "12.6 More on the linear regression output\nWe have not discussed some of the additional metrics in the output of the linear regression model. Let’s print the summary:\n\nmodel3= 'np.log(tip)~np.log(total_bill)+sex+smoker+day+time+size'\nlm3   = smf.ols(formula = model3, data = tips).fit()\nprint(lm3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            np.log(tip)   R-squared:                       0.478\nModel:                            OLS   Adj. R-squared:                  0.460\nMethod:                 Least Squares   F-statistic:                     26.90\nDate:                Mon, 22 Jul 2024   Prob (F-statistic):           2.14e-29\nTime:                        14:16:35   Log-Likelihood:                -63.952\nNo. Observations:                 244   AIC:                             145.9\nDf Residuals:                     235   BIC:                             177.4\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept             -0.9227      0.156     -5.922      0.000      -1.230      -0.616\nsex[T.Female]          0.0253      0.044      0.570      0.569      -0.062       0.113\nsmoker[T.No]           0.0075      0.046      0.165      0.869      -0.082       0.097\nday[T.Fri]             0.0704      0.123      0.571      0.568      -0.172       0.313\nday[T.Sat]           1.36e-05      0.147   9.23e-05      1.000      -0.290       0.290\nday[T.Sun]             0.0719      0.148      0.487      0.626      -0.219       0.363\ntime[T.Dinner]        -0.0317      0.139     -0.228      0.820      -0.306       0.242\nnp.log(total_bill)     0.6135      0.060     10.209      0.000       0.495       0.732\nsize                   0.0520      0.028      1.882      0.061      -0.002       0.106\n==============================================================================\nOmnibus:                        8.916   Durbin-Watson:                   1.943\nProb(Omnibus):                  0.012   Jarque-Bera (JB):               14.268\nSkew:                          -0.184   Prob(JB):                     0.000797\nKurtosis:                       4.126   Cond. No.                         54.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n12.6.1 Skewness and kurtosis\nFirst, we have that the std error lists the standard errors for the estimated coefficients. We expect the true population values of the coefficients to fall within 2 std errors of the estimated coefficients with high confidence. Next, Skew indicates how skewed the residuals are. Values departing from 0 indicate high skewness.Kurtosis is a measure of the tails of the residuals. Values higher than 3 indicate tails that are heavier than the normal distribution. Alternate models should be considered in thsi case.\n\n\n12.6.2 Multicollinearity\nA serious problem that may dramatically impact the usefulness of a regression model is multicollinearity, or near - linear dependence among the regression variables. Multicollinearity implies near - linear dependence among the regressors. We can check for this dependence with the condition number, written as Cond. No.. Condition numbers between 100 and 1000 imply moderate to strong multicollinearity, and if the condition number exceeds 1000, severe multicollinearity is indicated.\nMulticollinearity can be cured with:\n\nmore data (lol often not possible), 2, model re-specification: Can you include a function of the variables that preserves the information, but aren’t linearly dependent? Can you remove a variable?\nOr, a modified version of regression, one of Lasso, ridge or elastic net regression.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Linear Regression II</span>"
    ]
  },
  {
    "objectID": "Case_13.html",
    "href": "Case_13.html",
    "title": "13  SQL",
    "section": "",
    "text": "13.1 Why databases?\nWhile we have been dealing with data sitting in CSV files so far, no serious data organization runs their operations off of CSV files on a single person’s computer. This practice presents all sorts of hazards, including but not limited to:\nTherefore, our data should be stored elsewhere if we want to reliably access it in the future and, more importantly, share it and work on it with others. The database is the classic location where modern organizations have chosen to store their data for professional use. Databases have been a topic of research since the late 1960s. Many technology vendors picked up on this and developed databases software for companies to consume. Some of these vendors and products are:\nThese databases all implement the standard SQL language and are thus fairly similar to each other in terms of features. However, there are some key differences. Comparing MySQL vs. PostgreSQL, the two most popular database systems, MySQL does not implement FULL JOINS (you will learn about JOINs later on). PostgreSQL also supports some more advanced aggregation functions for statistics (you will learn about these soon). For example, in PostgreSQL you can perform regressions directly on the data before retrieving it, whereas MySQL only supports basic stats operations. However, this overhead leads to a slight performance hit, making MySQL faster for simple retrieval tasks.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#why-databases",
    "href": "Case_13.html#why-databases",
    "title": "13  SQL",
    "section": "",
    "text": "Destruction of that single device\nDestruction of the files on that device\nInability to connect to that person’s device from another device that requires the data\nInability to store more than a limited amount of data (since a single device doesn’t have that much memory)\n\n\n\nMicrosoft, initially with Microsoft Access and more recently with Microsoft SQL Server\nOracle, with their Oracle database and MySQL (a popular open source database)\nThe “PostgreSQL Global Development Group”, with the open-source PostgreSQL",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#types-of-databases",
    "href": "Case_13.html#types-of-databases",
    "title": "13  SQL",
    "section": "13.2 Types of databases",
    "text": "13.2 Types of databases\nAt this point, you might believe that databases can be thought of as a collection of data. This is true, but unfortunately it is not that simple. Data cannot simply be thrown in a database the same way you throw your socks in your sock drawer. Depending on your needs for the data, you will choose between one of two main types of databases.\n\n13.2.1 Relational databases\nThe most common database type is called a relational database, and the systems that manage these kinds of databases are called Relational Database Management Systems (RDBMS). Relational databases date back to the early 1970s and can be considered the first type of database ever conceived. Continuing with our sock example, this drawer would have many identical slots, each for one pair of socks. The socks may be of different materials, colors, brands, etc., but they need to fit into a slot.\nRelational databases deal with “relational data”, which is a fancy way of saying “tabular” data. This kind of dataset consists of rows and columns (i.e. tables) where each row corresponds to an observation and each column corresponds to an attribute of that observation. For example if we were keeping track of our friends and their phone numbers, each row on the file (or table) represents one friend and each column represents the information we want to track about that friend (name and phone number). The cell on the intersection of the row and column contains the actual data. Relational data is manipulated using a specific language called SQL (Structured Query Language), which we will learn about soon.\nA simple way to conceptualize a table inside a relational database is as a CSV file “copied” to the database. In fact, many databases offer that possibility (assuming your file is correctly formatted, of course).\n\n\n13.2.2 NoSQL databases\nAround 20 years ago, with the advent of the Internet and the necessity to store and process unstructured data (i.e. data that does not fit well in the row-by-column paradigm), developers started to discuss another type of database, which eventually ended up being referred to as a NoSQL database. These databases are not relational and are also built with more “relaxed” rules compared to their predecessors. NoSQL databases are more like a big drawer without slots and not exclusively for socks. You may choose to use this drawer primarily for socks of all sizes - small ones, big ones, maybe even a loose sock by itself - but it could also contain other items like sweaters or pants.\nAs the name implies, NoSQL databases do not rely on SQL, although many of them do allow you to use SQL to interface with them. At its core, a NoSQL database is simply a key-value store. That is, everything you store (sometimes called a document) in this database has a key associated with it. The database’s job is simply to help you retrieve your desired document as quickly as possible. Nothing pre-determines what a document contains (i.e. it does not have a concept of “tables”); however, this flexibility comes at a price. When you retrieve a document, you have to perform extra checks on it to ensure its validity as the database will not automatically do this for you as it would with a relational database. This may or may not be desireable depending on your particular application.\n\n\n13.2.3 When to pick one over the other\nPicking between RDBMS vs. NoSQL really comes down to the requirements of your project. We touched on this above, but both systems prioritize different parts of the CAP Theorem. Simply put, the CAP Theorem says that a database system can’t have all three of the following:\n\nConsistency: Every read of the database will return the most up-to-date write version or an error\nAvailability: Every request receives a (non-error) response, without the guarantee that it contains the most up-to-date write version\nPartition Tolerance: The system continues to operate no matter the network quality between nodes\n\nNoSQL prefers to be partition tolerant over consistent, whereas, RDBMS is the opposite. In certain applications, consistency is imperative, which often forces you into using RDBMS. For example, if you are a bank and you query a customer’s balance, you want to guarantee that the number you get is the most recent one and not the one from yesterday.\nFor the remainder of the case, we shall only consider RDBMS systems.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#what-is-this-sql-thing",
    "href": "Case_13.html#what-is-this-sql-thing",
    "title": "13  SQL",
    "section": "13.3 What is this “SQL” thing?",
    "text": "13.3 What is this “SQL” thing?\nSo we’ve been dropping in references to SQL throughout, yet we haven’t explained what it is. Now we will! Just like data can’t really survive without a database, a database can’t be utilized without SQL. SQL is used for a wide variety of tasks, including but not limited to extracting data, creating the internal structure of a database (in the form of tables), and reading and writing data to these tables. SQL is an international standard published by the ISO and so it is the de facto language that all database systems adhere to.\nIn this course, we will be writing SQL queries using the SQLAlchemy package in Python. This allows you to directly interface with relational databases without exiting the Python environment, while using syntax that is identical to what you would write outside of Python.\nRun the code below to set up this framework:\n\nimport pandas as pd\nfrom sqlalchemy import create_engine, text\n\n#maximum number of rows to display\npd.options.display.max_rows = 10\n\nengine=create_engine('sqlite://')\ndf = pd.read_csv('customer.csv').to_sql('customer', engine, if_exists='replace', index=False)\ndf = pd.read_csv('agent.csv').to_sql('agent', engine, if_exists='replace', index=False)\ndf = pd.read_csv('call.csv').to_sql('call', engine, if_exists='replace', index=False)\n\n# function that allows us to run queries\ndef runQuery(sql):\n    result = engine.connect().execute((text(sql)))\n    # print(type(result))\n    return pd.DataFrame(result.fetchall(), columns=result.keys())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#select-statements",
    "href": "Case_13.html#select-statements",
    "title": "13  SQL",
    "section": "13.4 SELECT statements",
    "text": "13.4 SELECT statements\nThe most important thing you will ever do in SQL is extract a subset of the data from a SQL table based on a set of rules. This is accomplished using the SELECT statement and the following syntax:\nSELECT list of columns\nFROM which dataframe?\nWHERE clause\nTo translate the above diagram into words:\n\nStart with the keyword SELECT\nFollow with the names of the columns you want to select, separated by commas (alternatively, you can use the * symbol to indicate you wish to select all columns)\nFollow with the keyword FROM\nFinish with the name of the table you wish to select data from\nOptionally, you can use the WHERE clause to only return results which satisfy certain conditions (similar to how code within Python if...then blocks only execute if the associated conditions are true)\n\nExample:\nSELECT CustomerID, Name\nFROM Customer\nWHERE Occupation != 'Unemployed'\nRunning in python:\n\nquery1 = \"\"\"SELECT CustomerID, Name\nFROM Customer\nWHERE Occupation != 'Unemployed'\"\"\"\nrunQuery(query1)\n\n\n\n\n\n\n\n\nCustomerID\nName\n\n\n\n\n0\n1\nMichael Gonzalez\n\n\n1\n2\nAmanda Wilson\n\n\n2\n3\nRobert Thomas\n\n\n3\n4\nEddie Hall\n\n\n4\n6\nMaria Johnson\n\n\n...\n...\n...\n\n\n755\n994\nRuben Steele\n\n\n756\n995\nAshley Young\n\n\n757\n996\nMr. Steven Smith\n\n\n758\n997\nMark Smith\n\n\n759\n999\nKaren Barber\n\n\n\n\n760 rows × 2 columns\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can define a multiline string in Python using triple quotes: \"\"\" my_multiline string \"\"\". This is used to write out the queries in this course. The triple quotes are not needed in regular SQL.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#order-by-statements",
    "href": "Case_13.html#order-by-statements",
    "title": "13  SQL",
    "section": "13.5 ORDER BY statements",
    "text": "13.5 ORDER BY statements\nSQL allows us to have the returned data ordered, using the ORDER BY statement. The statement ORDER BY should be followed by a comma-separated list of columns on which you want to order your results (columns that come first take priority in the subsequent ordering). Optionally, you can then append the keyword ASC or DESC (short for ascending and descending, respectively) after each column to determine the ordering type (e.g. alphabetical or reverse-alphabetical for a string column).\nExample:\nSELECT CustomerID\nFROM Customer\nWHERE Occupation != 'Unemployed'\nORDER BY Name\nSELECT CustomerID\nFROM Customer\nWHERE Occupation != 'Unemployed'\nORDER BY Name ASC",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#as-statements",
    "href": "Case_13.html#as-statements",
    "title": "13  SQL",
    "section": "13.6 AS statements",
    "text": "13.6 AS statements\nThe AS statment changes the name of a column returned by your query. However, this change is only temporary and is only valid for that particular query. For example, we can rename the Name column to CustomerName and order it alphabetically. This operation is known as aliasing.\nSELECT CustomerID, Name AS CustomerName\nFROM Customer\nWHERE Occupation != 'Unemployed'\nORDER BY CustomerName\nIn Python:\n\nquery2 = \"\"\"SELECT CustomerID, Name AS CustomerName\nFROM Customer\nWHERE Occupation != 'Unemployed'\nORDER BY CustomerName\"\"\"\nrunQuery(query2)\n\n\n\n\n\n\n\n\nCustomerID\nCustomerName\n\n\n\n\n0\n900\nAaron Gutierrez\n\n\n1\n622\nAaron Rose\n\n\n2\n226\nAdam Ward\n\n\n3\n786\nAlan Chambers\n\n\n4\n985\nAlan Mitchell\n\n\n...\n...\n...\n\n\n755\n699\nWillie Greene\n\n\n756\n715\nYesenia Wright\n\n\n757\n952\nYolanda White\n\n\n758\n421\nZachary Ruiz\n\n\n759\n392\nZachary Wilson\n\n\n\n\n760 rows × 2 columns",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#distinct-statements",
    "href": "Case_13.html#distinct-statements",
    "title": "13  SQL",
    "section": "13.7 DISTINCT statements",
    "text": "13.7 DISTINCT statements\nIf you only want unique elements from a data table, you’ll need to write the keyword DISTINCT immediately after SELECT in your query.\nExample:\nSELECT DISTINCT Occupation\nFROM Customer\nORDER BY Occupation",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#like",
    "href": "Case_13.html#like",
    "title": "13  SQL",
    "section": "13.8 LIKE",
    "text": "13.8 LIKE\nThe LIKE operator in SQL is used to search for a specified pattern in a column. It is often used with the WHERE clause to filter records based on partial matching patterns.\n\n13.8.1 Basic Syntax\nThe basic syntax for the LIKE operator is:\nSELECT column1, column2, ...\nFROM table_name\nWHERE column LIKE pattern;\n\n\n13.8.2 Wildcards\nThe LIKE operator is often used with two wildcards:\n\n%: Represents zero or more characters.\n**_**: Represents a single character.\n\n\n\n13.8.3 Examples\n\n13.8.3.1 Using the % Wildcard\nTo find all records where a column (e.g., name) starts with a specific letter (e.g., ‘J’):\nSELECT *\nFROM students\nWHERE name LIKE 'J%';\nThis query returns all students whose names start with ‘J’, such as ‘John’, ‘Jane’, ‘Jack’, etc.\nTo find all records where the column ends with a specific letter (e.g., ‘n’):\nSELECT *\nFROM students\nWHERE name LIKE '%n';\nThis query returns all students whose names end with ‘n’, such as ‘John’, ‘Megan’, etc.\nTo find all records where the column contains a specific substring (e.g., ‘an’):\nSELECT *\nFROM students\nWHERE name LIKE '%an%';\nThis query returns all students whose names contain ‘an’, such as ‘Megan’, ‘Andrew’, etc.\n\n\n13.8.3.2 Using the _ Wildcard\nTo find all records where a column (e.g., name) has a specific character at a specific position (e.g., second character is ‘a’):\nSELECT *\nFROM students\nWHERE name LIKE '_a%';\nThis query returns all students whose names have ‘a’ as the second character, such as ‘James’, ‘Sarah’, etc.\n\n\n13.8.3.3 Combining Wildcards\nYou can combine % and _ to form more complex patterns. For example, to find all names that start with ‘J’, have any two characters, and end with ‘n’:\nSELECT *\nFROM students\nWHERE name LIKE 'J__n';\nThis query returns names like ‘John’ and ‘Jian’.\n\n\n\n13.8.4 Case Sensitivity\n\nIn most databases, LIKE is case-insensitive by default (e.g., MySQL).\nIn some databases (e.g., PostgreSQL), LIKE is case-sensitive. Use ILIKE for case-insensitive matching.\n\n\n\n13.8.5 Using ESCAPE Clause\nIf you need to search for characters like % or _, which are normally wildcards, you can use the ESCAPE clause:\nSELECT *\nFROM students\nWHERE name LIKE '%\\%%' ESCAPE '\\';\nThis query finds all names that contain the % character.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#case-statements",
    "href": "Case_13.html#case-statements",
    "title": "13  SQL",
    "section": "13.9 CASE statements",
    "text": "13.9 CASE statements\nA CASE statement in SQL is used to create conditional logic within your queries. It’s similar to an if-then-else statement in programming languages. You can use CASE to perform different actions based on various conditions.\n\n13.9.1 Basic Syntax\nThe basic syntax for a CASE statement in SQL is:\nCASE\n    WHEN condition1 THEN result1\n    WHEN condition2 THEN result2\n    ...\n    ELSE resultN\nEND\n\n\n13.9.2 Example Usage\nHere’s an example to illustrate its use:\nSuppose you have a students table with columns student_id, name, and score. You want to categorize the students based on their scores into “Pass” or “Fail”.\nSELECT\n    student_id,\n    name,\n    score,\n    CASE\n        WHEN score &gt;= 50 THEN 'Pass'\n        ELSE 'Fail'\n    END AS result\nFROM\n    students;\nIn this example: - The CASE statement checks each student’s score. - If the score is 50 or more, it returns ‘Pass’. - If the score is less than 50, it returns ‘Fail’.\n\n\n13.9.3 Nested CASE Statements\nYou can also nest CASE statements for more complex conditions:\nSELECT\n    student_id,\n    name,\n    score,\n    CASE\n        WHEN score &gt;= 90 THEN 'A'\n        WHEN score &gt;= 80 THEN 'B'\n        WHEN score &gt;= 70 THEN 'C'\n        WHEN score &gt;= 60 THEN 'D'\n        ELSE 'F'\n    END AS grade\nFROM\n    students;\nIn this example: - The CASE statement categorizes scores into letter grades.\n\n\n13.9.4 Using CASE in WHERE Clause\nYou can use CASE statements in WHERE clauses, but it’s less common. Here’s an example:\nSELECT\n    student_id,\n    name,\n    score\nFROM\n    students\nWHERE\n    CASE\n        WHEN score &gt;= 50 THEN 'Pass'\n        ELSE 'Fail'\n    END = 'Pass';",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#join",
    "href": "Case_13.html#join",
    "title": "13  SQL",
    "section": "13.10 JOIN",
    "text": "13.10 JOIN\nIn SQL, the JOIN clause is used to combine rows from two or more tables based on a related column between them. There are several types of joins, each serving a different purpose.\n\n13.10.1 Types of JOINs\n\nINNER JOIN: Returns only the rows that have matching values in both tables.\nLEFT JOIN (or LEFT OUTER JOIN): Returns all rows from the left table, and the matched rows from the right table. The result is NULL from the right side if there is no match.\nRIGHT JOIN (or RIGHT OUTER JOIN): Returns all rows from the right table, and the matched rows from the left table. The result is NULL from the left side if there is no match.\nFULL JOIN (or FULL OUTER JOIN): Returns all rows when there is a match in either left or right table. Rows without a match in the other table will have NULLs for the columns from that table.\nCROSS JOIN: Returns the Cartesian product of the two tables, meaning it returns all possible combinations of rows from the tables.\nSELF JOIN: A regular join, but the table is joined with itself.\n\n\n\n13.10.2 Examples\n\n13.10.2.1 INNER JOIN\nThis join returns only the rows where there is a match in both tables.\nSELECT students.student_id, students.name, scores.score\nFROM students\nINNER JOIN scores ON students.student_id = scores.student_id;\n\n\n13.10.2.2 LEFT JOIN\nThis join returns all rows from the left table, and the matched rows from the right table. If no match is found, NULLs are returned for columns from the right table.\nSELECT students.student_id, students.name, scores.score\nFROM students\nLEFT JOIN scores ON students.student_id = scores.student_id;\n\n\n13.10.2.3 RIGHT JOIN\nThis join returns all rows from the right table, and the matched rows from the left table. If no match is found, NULLs are returned for columns from the left table.\nSELECT students.student_id, students.name, scores.score\nFROM students\nRIGHT JOIN scores ON students.student_id = scores.student_id;\n\n\n13.10.2.4 FULL JOIN\nThis join returns all rows when there is a match in either left or right table. Rows without a match in the other table will have NULLs for the columns from that table.\nSELECT students.student_id, students.name, scores.score\nFROM students\nFULL JOIN scores ON students.student_id = scores.student_id;\n\n\n13.10.2.5 CROSS JOIN\nThis join returns the Cartesian product of the two tables, meaning it returns all possible combinations of rows from the tables.\nSELECT students.student_id, students.name, scores.score\nFROM students\nCROSS JOIN scores;\n\n\n13.10.2.6 SELF JOIN\nThis join is used to join a table with itself, often to compare rows within the same table.\nSELECT A.student_id, A.name AS student_name, B.name AS buddy_name\nFROM students A\nINNER JOIN students B ON A.buddy_id = B.student_id;",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#aggregate-functions",
    "href": "Case_13.html#aggregate-functions",
    "title": "13  SQL",
    "section": "13.11 Aggregate functions",
    "text": "13.11 Aggregate functions\nAggregate functions in SQL perform calculations on a set of values and return a single value. The following are the most commonly used SQL aggregate functions:\n\nAVG() – calculates the average of a set of values\nCOUNT() – counts rows in a specified table or view\nMIN() – gets the minimum value in a set of values\nMAX() – gets the maximum value in a set of values\nSUM() – calculates the sum of values\n\nPostgreSQL as some more advanced aggregate functions. Specifically, they have some nice ones for statistics. For example,\n\nregr_intercept(Y, X) - Returns the intercept for the line of best fit\nregr_slope(Y, X) - Returns the slope of the line of best fit\ncorr(Y, X) - Returns the correlation between two columns\n\nHere are brief explanations of the previously specified aggregate functions:\n\n13.11.1 1. AVG()\n\nDescription: Calculates the average of a set of values.\nExample: To find the average score of students:\n\nSELECT AVG(score) AS average_score\nFROM students;\n\n\n13.11.2 2. COUNT()\n\nDescription: Counts the number of rows in a specified table or view.\nExample: To count the number of students:\n\nSELECT COUNT(*) AS number_of_students\nFROM students;\n\n\n13.11.3 3. MIN()\n\nDescription: Gets the minimum value in a set of values.\nExample: To find the minimum score:\n\nSELECT MIN(score) AS minimum_score\nFROM students;\n\n\n13.11.4 4. MAX()\n\nDescription: Gets the maximum value in a set of values.\nExample: To find the maximum score:\n\nSELECT MAX(score) AS maximum_score\nFROM students;\n\n\n13.11.5 5. SUM()\n\nDescription: Calculates the sum of values.\nExample: To find the total score of all students:\n\nSELECT SUM(score) AS total_score\nFROM students;",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  },
  {
    "objectID": "Case_13.html#sql-statement-types",
    "href": "Case_13.html#sql-statement-types",
    "title": "13  SQL",
    "section": "13.12 SQL statement types",
    "text": "13.12 SQL statement types\nWe have introduced SQL’s Data Manipulation Language (DML) statements; that is, statements that are used to read or write (manipulate) data from the database. However, SQL also has the ability to create, modify, and remove database objects themselves as well as the data within them. It does this by using Data Definition Language (DDL) statements which are commands that define the different structures in a database. You will learn more about these statements in future cases.\nThere are two other types of SQL statements that are important, but less likely to be used by someone who is merely focused on analyzing data. We’ll not not dig into these as it’s very unlikely that you’ll have to deal with these anytime soon, but you are free to read up about them elsewhere if you are interested. They are:\n\nData Control Language (DCL): These determine who has permission to do what in the database. Everytime you log in to a database, you do that using (your) database user account. By default, a user after being created does not have permission to do anything, so someone (normally a database administrator (DBA)) needs to grant permission to that user to perform certain operations on the database.\nTransactional Control Language (TCL): These commands are used to guarantee that full units of work are either completed as a whole or not at all. An example is a bank transfer: you need to ensure that if money has been withdrawn from account A, then it has also been deposited in account B, which requires wrapping these two commands into a transaction.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>SQL</span>"
    ]
  }
]