[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH 1130 Companion Manual",
    "section": "",
    "text": "Introduction\nThis is a short companion manual for the course MATH 1130. It contains a brief overview of the major topics and some of the Python methods covered in each case. It is not a replacement for the cases, which are the main content of the course. It is meant to be used as a reference manual and as supplementary material to aid you in your understanding of the material. Please report any typos to kramsay2@yorku.ca .",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Case_1.html",
    "href": "Case_1.html",
    "title": "1  Python basics",
    "section": "",
    "text": "1.1 Python and Jupyter\nPython is a general purpose programming language that allows for both simple and complex data analysis. Python is incredibly versatile, allowing analysts, consultants, engineers, and managers to obtain and analyze information for insightful decision-making.\nThe Jupyter Notebook is an open-source web application that allows for Python code development. Jupyter further allows for inline plotting and provides useful mechanisms for viewing data that make it an excellent resource for a variety of projects and disciplines.\nThe following section will outline how to install and begin working with Python and Juypter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#setting-up-the-python-environment-locally-optional",
    "href": "Case_1.html#setting-up-the-python-environment-locally-optional",
    "title": "1  Python basics",
    "section": "1.2 Setting up the Python Environment locally (optional)",
    "text": "1.2 Setting up the Python Environment locally (optional)\nInstruction guides for Windows and MacOS are included below. Follow the one that corresponds with your operating system.\n\n1.2.1 Windows Install\n\nOpen your browser and go to https://www.anaconda.com/\nClick on your OS and then “Download”\nRun the downloaded file found in the downloads section from Step 2\nClick through the install prompts\nGo to menu (or Windows Explorer), find the Anaconda3 folder, and double-click to run OR Use a Spotlight Search and type “Navigator”, select and run the Anaconda Navigator program. Note that MacOS also comes with Python pre-installed, but you should not use this version, which is used by the system. Anaconda will run a second installation of Python and you should ensure that you only use this second installation for all tasks.\n\n\n\n1.2.2 Compare and contrast Jupyter, Python and Anaconda\n\nJupyter Notebook is a web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text.\nPython is a programming language that is often used in scientific computing, data science, and general-purpose programming.\nAnaconda is a distribution of Python and R for scientific computing and data science. It includes the conda package manager, which makes it easy to install packages for scientific computing and data science, as well as the Jupyter Notebook and other tools.\nIn simple terms Anaconda is a distribution and python is a language, Jupyter notebook is an application to create and share document that contains live code, equations, visualizations and narrative text.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#file-management-with-python-and-jupyter",
    "href": "Case_1.html#file-management-with-python-and-jupyter",
    "title": "1  Python basics",
    "section": "1.3 File Management with Python and Jupyter",
    "text": "1.3 File Management with Python and Jupyter\nIt is common practice to have a main folder where all projects will be located (e.g. “jupyter_research”). The following are guidelines you can use for Python projects to help keep your code organized and accessible:\n\nCreate subfolders for each Jupyter-related project\nGroup related .ipynb (the file format for Jupyter Notebook) files together in the same folder\nCreate a “Data” folder within individual project folders if using a large number of related data files\n\nYou should now be set up and ready to begin coding in Python!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#fundamentals-of-python",
    "href": "Case_1.html#fundamentals-of-python",
    "title": "1  Python basics",
    "section": "1.4 Fundamentals of Python",
    "text": "1.4 Fundamentals of Python\nIn this case, we will introduce you to more basic Python concepts. If you prefer, you can first go through the more extensive official Python tutorial and/or the W3School Python tutorial and practice more fundamental concepts before proceeding with this case. It is highly recommended that you go through either or both of these tutorials either before or after going through this notebook to solidify and augment your understanding of Python. For a textbook introduction to Python, see this text, from which some of the following material is adapted/taken.\n\n1.4.1 What is a program?\n\nA program is a sequence of instructions that specifies how to perform a computation.\nThe computation might be something mathematical, such as solving a system of equations or finding the roots of a polynomial, but it can also be a symbolic computation, such as searching and replacing text in a document or something graphical, like processing an image or playing a video.\nThe details look different in different languages, but a few basic instructions appear in just about every language:\n\ninput: Get data from the keyboard, a file, the network, or some other device.\noutput: Display data on the screen, save it in a file, send it over the network, etc.\nmath: Perform basic mathematical operations like addition and multiplication.\nconditional execution: Check for certain conditions and run the appropriate code.\nrepetition: Perform some action repeatedly, usually with some variation.\n\n\nEvery program you’ve ever used, no matter how complicated, is made up of instructions that look pretty much like these. So you can think of programming as the process of breaking a large, complex task into smaller and smaller subtasks until the subtasks are simple enough to be performed with one of these basic instructions.\n\nNote that writing a program involves typing out the correct code, and then running, or executing that code:\n\nThe print() function allows you to print an object. Placing a python object in the round brackets and running the code makes the compute print the object. Example:\n\nprint(\"Hello World\")\n\nHello World\n\n\nThe simplest use of Python is as a calculator. We can use the operators +, -, /, and * perform addition, subtraction, division and multiplication. We can use ** for exponentiation. See the following examples:\n\nprint(40 + 2)\nprint(43 - 1)\nprint(6 * 7)\nprint(2**2)\n\n42\n42\n42\n4\n\n\nNote that a Python program contains one or more lines of code, which are executed in a top-down fashion.\n\n\n1.4.2 Values and types\n\nA value is one of the basic things a program works with, like a letter or a number.\nSome values we have seen so far are 2, 5.0, and ‘Hello, World!’.\nThese values belong to different types: 2 is an integer, 5.0 is a floating-point number, and ‘Hello, World!’ is a string, so-called because the letters it contains are strung together.\n\nUse type() to determine the type of a value. Example:\n\nprint(type(2))\nprint(type(42.0))\nprint(type('Hello, World!'))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\nIf you are used to using Microsoft Excel, this is similar to how Excel distinguishes between data types such as Text, Number, Currency, Date, or Scientific. As noted above, some common data types that you will come across in Python are:\n\nIntegers, type int: 1\nFloat type float: 25.5\nStrings, type str: 'Hello'\n\n\nHere we see (1) integers and (2) floats store numeric data. The difference between the two is that floats store decimal variables (fractions), whereas the integer type can only store integer variables (whole numbers).\nFinally, (3) is the string type. Strings are used to store textual data in Python (a string of one or more characters). Later in this case we will use string variables to store country names. They are often used to store identifiers such as names of people, city names, and more.\n\nThere are other data types available in Python; however, these are the three fundamental types that you will see across almost every Python program. Always keep in mind that every value, or object, in Python has a type and some of these “types” can be custom-defined by the user, which is one of the benefits of Python.\n\n\n1.4.3 Variables\nWe can assign names to objects in python so that they are easy to manipulate, a named object is called a variable. Use the = sign to assign a name to a variable.\n\nFor example, if a user aims to store the integer 5 in an object named my_int, this can be accomplished by writing the Python statement, my_int = 5.\nIn this case, my_int is a variable, much like you might find in algebra, but the = sign is for assignment not equality .\nSo my_int = 5 should be taken to mean something more like Let my_int be equal to 5 rather than my_int is equal to 5.\n\n\nmy_int=5\nprint(my_int)\n\n5\n\n\nHere, my_int is an example of a variable because it can change value (we could later assign a different value to it) and it is of type Integer, known in Python simply as int. Unlike some other programming languages, Python guesses the type of the variable from the value that we assign to it, so we do not need to specify the type of the variable explicitly. For example,\n\nIntegers, type int: my_int = 1\nFloat type float: my_float = 25.5\nStrings, type str: my_string = 'Hello'\n\nNote that the names my_int, my_float and my_string are arbitrary here. While it is useful to name your variables so that the names contain some information about what they are used for, this code would be functionally identical to if we had used x, xrtqp2 and my_very_long_variable_name, respectively.\n\nmy_int=5\nprint(my_int)\ncountry=\"Canada\"\nprint(country)\n\n5\nCanada\n\n\nWe mentioned before that variables can change value. Let’s take a look at how this works, and also introduce a few more Python operations:\n\nx = 4\nprint(x)\ny = 2\nx = y + x\nprint(x)\n\n4\n6\n\n\nAgain, if you’re used to syntax from mathematics “x = y + x” might look very wrong at first glance. However, it simply means “throw out the existing value of x, and assign a new value which is calculated as the sum of y and x”. Here, we can see that the value of x changes, demonstrating why we call them “variables”.\nWe can also use more operators than just + and -. We use * for multiplication, / for division, and ** for power. The standard order of operations rules from mathematics also applies and parentheses () can be used to override these.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_1.html#data-structures",
    "href": "Case_1.html#data-structures",
    "title": "1  Python basics",
    "section": "1.5 Data structures:",
    "text": "1.5 Data structures:\nA data structure is a data/value type which organizes and stores multiple values. These are more complicated data types that comprise many single pieces of data, organized in a specific way. Examples include dictionaries, arrays, lists, stacks, queues, trees, and graphs. Each type of data structure is designed to handle specific scenarios.\n\nAs before, we use =, the assignment operator, to assign a value to the variable.\n\n\n1.5.1 Dictionaries\n\nPython’s dictionary type stores a mapping of key-value pairs that allow users to quickly access information for a particular key.\nBy specifying a key, the user can return the value corresponding to the given key. Python’s syntax for dictionaries uses curly braces {}:\n\nSyntax for creation:\nuser_dictionary = {'Key1': Value1, 'Key2': Value2, 'Key3': Value3}\nNotes:\nIn Python, the dictionary type has built-in methods to access the dictionary keys and values. - These methods are called by typing .keys() or .values() after the dictionary object. - We will change the return type of calling .keys() and .values() to a list by using the list() method. Below when we print the unconverted keys, the first thing you see is dict_keys, indicating the type of the data. Convert it to a list which is a simpler and more common data type. We can do this by passing the data into the list(...) function.\nExample:\n\n# Creating a dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n# Accessing values\nprint(person[\"name\"])  # Output: Alice\nprint(person[\"age\"])   # Output: 30\nprint(person[\"city\"])  # Output: New York\n\n# Adding a new key-value pair\nperson[\"job\"] = \"Engineer\"\n\n# Updating an existing value\nperson[\"age\"] = 31\n\n# Deleting a key-value pair\ndel person[\"city\"]\n\n# Printing the updated dictionary\nprint(person)\n\n# Getting all keys\nkeys = person.keys()\n\n# Converting to list\nkeys_list=list(keys)\nprint(keys)  # Output: dict_keys(['name', 'age', 'job'])\nprint(keys_list) # Output: type list\n\n# Getting all values\nvalues = person.values()\nprint(values)  # Output: dict_values(['Alice', 31, 'Engineer'])\n\n# Converting to list\nvalues_list=list(values)\nprint(values_list) # Output: type list\n\n\nprint(type(values_list))\nprint(type(list(values)))\n\n\n# Creating a nested dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"address\": {\n        \"city\": \"New York\",\n        \"zipcode\": \"10001\"\n    }\n}\n\n# Accessing elements in a nested dictionary\nprint(person[\"address\"][\"city\"])    # Output: New York\nprint(person[\"address\"][\"zipcode\"]) # Output: 10001\n\nAlice\n30\nNew York\n{'name': 'Alice', 'age': 31, 'job': 'Engineer'}\ndict_keys(['name', 'age', 'job'])\n['name', 'age', 'job']\ndict_values(['Alice', 31, 'Engineer'])\n['Alice', 31, 'Engineer']\n&lt;class 'list'&gt;\n&lt;class 'list'&gt;\nNew York\n10001\n\n\n\n\n1.5.2 Lists\nA list is an incredibly useful data structure in Python that can store any number of Python objects. Lists are denoted by the use of square brackets []:\nSyntax:\nuser_list = [Value1, Value2, Value3]\nExample:\n\n# Creating a list\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n\n# Adding a new element\nfruits.append(\"orange\")\n\n# Updating an existing element\nfruits[1] = \"blueberry\"\n\n# Deleting an element\ndel fruits[2]\n\n# Printing the updated list\nprint(fruits)\n\n# Getting the length\nprint(len(fruits))\n\n\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n\n\n\n# Accessing elements\nprint(fruits[0])  # Output: apple\nprint(fruits[1])  # Output: banana\nprint(fruits[2])  # Output: cherry\n\n# Accessing elements by negative index\nprint(fruits[-1])  # Output: date\nprint(fruits[-2])  # Output: cherry\nprint(fruits[-3])  # Output: banana\nprint(fruits[-4])  # Output: apple\n\n\n\n# Accessing a range of elements\nprint(fruits[1:3])  # Output: ['banana', 'cherry']\nprint(fruits[:2])   # Output: ['apple', 'banana']\nprint(fruits[2:])   # Output: ['cherry', 'date']\nprint(fruits[:])    # Output: ['apple', 'banana', 'cherry', 'date']\n\n# Creating a nested list\nnested_list = [[\"apple\", \"banana\"], [\"cherry\", \"date\"]]\n\n# Accessing elements in a nested list\nprint(nested_list[0][0])  # Output: apple\nprint(nested_list[0][1])  # Output: banana\nprint(nested_list[1][0])  # Output: cherry\nprint(nested_list[1][1])  # Output: date\n\n['apple', 'blueberry', 'date', 'orange']\n4\napple\nbanana\ncherry\ndate\ncherry\nbanana\napple\n['banana', 'cherry']\n['apple', 'banana']\n['cherry', 'date']\n['apple', 'banana', 'cherry', 'date']\napple\nbanana\ncherry\ndate\n\n\nList computations example:\n\n# Creating a list of numbers\nnumbers = [3.5, 1.2, 6.8, 2.4, 5.1]\n\n# Finding the minimum value\nmin_value = min(numbers)\nprint(\"Minimum value:\", min_value)  # Output: Minimum value: 1.2\n\n# Finding the maximum value\nmax_value = max(numbers)\nprint(\"Maximum value:\", max_value)  # Output: Maximum value: 6.8\n\n# Summing all elements in the list\ntotal_sum = sum(numbers)\nprint(\"Sum of all elements:\", total_sum)  # Output: Sum of all elements: 19.0\n\n# Rounding each element to the nearest integer\nrounded_numbers = [round(num) for num in numbers]\nprint(\"Rounded numbers:\", rounded_numbers)  # Output: Rounded numbers: [4, 1, 7, 2, 5]\n\nMinimum value: 1.2\nMaximum value: 6.8\nSum of all elements: 19.0\nRounded numbers: [4, 1, 7, 2, 5]\n\n\n\n\n1.5.3 The in operator\nThe in operator in Python is used to check for the presence of an element within a collection, such as a list, tuple, set, or dictionary. When used with lists or other sequences, it checks if a specific value is contained in the sequence and returns True if it is, and False otherwise. When used with dictionaries, the in operator checks for the presence of a specified key. If the key exists in the dictionary, it returns True; otherwise, it returns False. This operator provides a simple and readable way to perform membership tests in various data structures.\nExample:\n\n# Creating a dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n# Using the 'in' operator to check for a key\nprint(\"name\" in person)  # Output: True\nprint(\"job\" in person)   # Output: False\n\n# Using the 'in' operator to check for a value\nprint(\"Alice\" in person.values())  # Output: True\nprint(\"Engineer\" in person.values())  # Output: False\n\n\n# Creating a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Using the 'in' operator to check for an element\nprint(\"banana\" in fruits)  # Output: True\nprint(\"orange\" in fruits)  # Output: False\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n1.5.4 Comments and debugging\n\nComments are lines of code which begin with the # symbol. Nothing happens when you run these lines. Their purpose is to describe the code you have written, especially if it would be unclear to someone else reading it. You should be commenting your code as you go along. For example “these lines compute the average” or “These lines remove missing data” etc.\nWe have seen that programs may have errors or bugs. The process of resolving bugs is known as debugging. Debugging can be a very frustrating activity. Please be prepared for this. Following, this Python textbook, there are three kinds of errors you may encouter:\n\nSyntax error: “Syntax” refers to the rules of the language. If there is a syntax error anywhere in your program, Python displays an error message and quits.\nRuntime error: The second type of error is a runtime error, so called because the error does not appear until after the program has started running. These errors are also called exceptions because they usually indicate that something exceptional (and bad) has happened.\nSemantic error: If there is a semantic error in your program, it will run without generating error messages, but it will not do the right thing. It will do something else. Specifically, it will do what you told it to do. Identifying semantic errors can be tricky because it requires you to work backward by looking at the output of the program and trying to figure out what it is doing. Try runnning your code one line at a time and ensuring each line is doing what you expect, using the print feature.\n\n\n\n\n1.5.5 For loops\nOne control flow element in Python is the for loop.\n- The for loop allows one to execute the same statements over and over again (i.e. looping). - This saves a significant amount of time coding repetitive tasks and aids in code readability.\nSyntax:\nfor iterator_variable in some_sequence:\n    statements(s)\nThe for loop iterates over some_sequence and performs statements(s) at each iteration. - That is, at each iteration the iterator_variable is updated to the next value in some_sequence. - As a concrete example, consider the loop:\nExample:\n\nfor i in [1,2,3,4]:\n    print(i*i)\n\n1\n4\n9\n16\n\n\n\nHere, the for loop will print to the screen four times; that is it will print 1 on the first iteration of the loop, 4 on the second iteration, 9 on the third, and 16 on the fourth.\nHence, the for loop statement will iterate over all the elements of the list [1,2,3,4], and at each iteration it updates the iterator variable i to the next value in the list [1,2,3,4].\nIn for loops, it is an extremely good habit to choose an iterator variable that provides context rather than a random letter.\nIn this case, we will use both to get you accustomed to both.\nThis is because you will see both throughout the course of your data science career, but we encourage you to not use a generic name like i whenever possible for ease of communication.\n\n\n\n1.5.6 List comprehensions\nA list comprehension is a concise way to create a new list by applying an expression to each element of an existing list while optionally filtering elements based on a condition. It combines loops and conditional statements into a single line of code, making it efficient and readable for creating lists with specific transformations or filters. It can be used to replace a for loop with shorter code.\nExample:\n\n# Using a for loop\nnumbers = [1, 2, 3, 4, 5]\nsquared_numbers = []\nfor num in numbers:\n    squared_numbers.append(num ** 2)\nprint(\"Squared numbers (using for loop):\", squared_numbers)\n\n# Using list comprehension\nsquared_numbers = [num ** 2 for num in numbers]\nprint(\"Squared numbers (using list comprehension):\", squared_numbers)\n\n\n# Using a for loop\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\neven_numbers = []\nfor num in numbers:\n    if num % 2 == 0:\n        even_numbers.append(num)\nprint(\"Even numbers (using for loop):\", even_numbers)\n\n# Using list comprehension\neven_numbers = [num for num in numbers if num % 2 == 0]\nprint(\"Even numbers (using list comprehension):\", even_numbers)\n\nSquared numbers (using for loop): [1, 4, 9, 16, 25]\nSquared numbers (using list comprehension): [1, 4, 9, 16, 25]\nEven numbers (using for loop): [2, 4, 6, 8, 10]\nEven numbers (using list comprehension): [2, 4, 6, 8, 10]\n\n\n\n\n1.5.7 If statements and booleans\nA boolean is a data type that represents one of two possible states: True or False. Booleans are used extensively for making decisions and comparisons. They are often the result of logical operations, such as comparisons (e.g., greater than, less than) or boolean operations (e.g., and, or, not).\nHere’s a brief summary:\nTrue: Represents a condition that is considered true or valid. False: Represents a condition that is considered false or invalid. Booleans are crucial for controlling the flow of programs, making decisions, and executing code based on specific conditions being met or not.\nIf statements are conditional statements that allow you to execute certain blocks of code based on specified conditions. They form the foundation of decision-making in code, enabling programs to make choices and take different actions depending on whether certain conditions are True or False.\nSyntax:\nif test_expression_1:\n    block1_statement(s)\nelif test_expression_2:\n    block2_statement2(s)\nelse:\n    block3_statement(s)\nExample:\n\n# Example 0: A boolean\nx=2\ny=7\nprint(True)\nprint(x==y)\nprint(x &gt; 5)\n\n\n# Example 1: Simple if statement\nx = 10\n\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\n# Example 2: if-else statement\ny = 3\n\nif y % 2 == 0:\n    print(\"y is even\")\nelse:\n    print(\"y is odd\")\n\n# Example 3: if-elif-else statement\ngrade = 75\n\nif grade &gt;= 90:\n    print(\"Grade is A\")\nelif grade &gt;= 80:\n    print(\"Grade is B\")\nelif grade &gt;= 70:\n    print(\"Grade is C\")\nelif grade &gt;= 60:\n    print(\"Grade is D\")\nelse:\n    print(\"Grade is F\")\n\nTrue\nFalse\nFalse\nx is greater than 5\ny is odd\nGrade is C\n\n\nExplanation:\nExample 0: Creates different types of boolean variables.\nExample 1: Checks if x is greater than 5. If true, it prints “x is greater than 5”.\nExample 2: Checks if y is even (remainder of division by 2 is zero). If true, it prints “y is even”; otherwise, it prints “y is odd”.\nExample 3: Evaluates the value of grade and prints a corresponding grade based on the ranges specified using if, elif (else if), and else statements. This demonstrates chaining conditions to determine a grade based on numerical thresholds.\n\n\n1.5.8 String formatting\nString formatting refers to the various techniques used to insert dynamic values into strings in a controlled and formatted manner. We cover .format and f-strings.\nf-strings (Formatted String Literals):\nSyntax:\nf\"some text {expression1} more text {expression2} ...\"\n\nf prefix before the string indicates it’s an f-string.\n{expression} inside curly braces {} evaluates expression and inserts its value into the string.\nYou can directly embed Python expressions, variables, or function calls inside {}.\n\n.format() Method:\nSyntax:\n\"some text {} more text {}\".format(value1, value2)\n\n{} acts as placeholders in the string.\n.format() method is called on a string object, and values passed to it replace corresponding {} in the string.\nYou can specify the order of substitution using {0}, {1}, etc., or use named placeholders {name}, {age}.\n\nDifferences: - f-strings are more concise and readable. - .format() method offers more flexibility, such as specifying formatting options or reusing values.\nExamples:\n\n# Example using .format() method\nname = \"Bob\"\nage = 25\nformatted_string = \"Hello, {}! You are {} years old.\".format(name, age)\nprint(formatted_string)\n\n\n# Example using f-strings\nname = \"Charlie\"\nage = 35\nformatted_string = f\"Hello, {name}! You are {age} years old.\"\nprint(formatted_string)\n\nHello, Bob! You are 25 years old.\nHello, Charlie! You are 35 years old.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python basics</span>"
    ]
  },
  {
    "objectID": "Case_2.html",
    "href": "Case_2.html",
    "title": "2  Data extraction and transformation",
    "section": "",
    "text": "2.1 Installing and importing packages\nExternal libraries (a.k.a. packages) are code bases that contain a variety of pre-written functions and tools. This allows you to perform a variety of complex tasks in Python without having to “reinvent the wheel”, i.e., build everything from the ground up. We will use two core packages: pandas and numpy.\npandas is an external library that provides functionality for data analysis. Pandas specifically offers a variety of data structures and data manipulation methods that allow you to perform complex tasks with simple, one-line commands.\nnumpy is a external library that offers numerous mathematical operations. We will use numpy later in the case. Together, pandas and numpy allow you to create a data science workflow within Python. numpy is in many ways foundational to pandas, providing vectorized operations, while pandas provides higher level abstractions built on top of numpy.\nBefore you use a module/package/library, it must be installed. Note that you only need to install each module/package/library once per machine. The syntax for installing a module/package/library on your machine will be either:\nor\nFor example, you can run one of the following commands in a code cell to install the package pandas.\nBefore using a package in each notebook or session, it must be imported. Unlike installation, importing must be done every time you use python. Let’s import both packages using the import keyword. We will rename pandas to pd and numpy to np using the as keyword. This allows us to use the short name abbreviation when we want to reference any function that is inside either package. The abbreviations we chose are standard across the data science industry and should be followed unless there is a very good reason not to.\n# Import the Pandas package\nimport pandas as pd\n\n# Import the NumPy package\nimport numpy as np",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#installing-and-importing-packages",
    "href": "Case_2.html#installing-and-importing-packages",
    "title": "2  Data extraction and transformation",
    "section": "",
    "text": "!pip install package name\n\n!conda install package name\n\n# If your machine uses pip\n!pip install pandas\n# If your machine uses Anaconda\n!conda install pandas",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#fundamentals-of-pandas",
    "href": "Case_2.html#fundamentals-of-pandas",
    "title": "2  Data extraction and transformation",
    "section": "2.2 Fundamentals of pandas",
    "text": "2.2 Fundamentals of pandas\n\n2.2.1 Series and DataFrame value types\npandas is a Python library that facilitates a wide range of data analysis and manipulation. Before, you saw basic data structures in Python such as lists and dictionaries. While you can build a basic data table (similar to an Excel spreadsheet) using nested lists in Python, they get quite difficult to work with. By contrast, in pandas the table data structure, known as the DataFrame, is a first-class citizen. It allows us to easily manipulate data by thinking of data in terms of rows and columns.\nIf you’ve ever used or heard of R or SQL before, pandas brings some functionality from each of these to Python, allowing you to structure and filter data more efficiently than pure Python. This efficiency is seen in two distinct ways:\n\nScripts written using pandas will often run faster than scripts written in pure Python\nScripts written using pandas will often contain far fewer lines of code than the equivalent script written in pure Python.\n\nAt the core of the pandas library are two fundamental data structures/objects: 1. Series 2. DataFrame\nA Series object stores single-column data along with an index. An index is just a way of “numbering” the Series object. For example, in this case study, the indices will be dates, while the single-column data may be stock prices or daily trading volume.\nA DataFrame object is a two-dimensional tabular data structure with labeled axes. It is conceptually helpful to think of a DataFrame object as a collection of Series objects. Namely, think of each column in a DataFrame as a single Series object, where each of these Series objects shares a common index - the index of the DataFrame object.\nBelow is the syntax for creating a Series object, followed by the syntax for creating a DataFrame object. Note that DataFrame objects can also have a single-column – think of this as a DataFrame consisting of a single Series object:\n\nSeries: A one-dimensional labeled array capable of holding data of any type (integer, string, float, etc.). Created using pd.Series(data, index=index), where data can be a list, dictionary, or scalar value.\nDataFrame: A two-dimensional labeled data structure with columns of potentially different types. Created using pd.DataFrame(data, index=index, columns=columns), where data can be a dictionary of lists, list of dictionaries, or 2D array-like object.\n\nExample:\n\n# Create a simple Series object\nsimple_series = pd.Series(\n    index=[0, 1, 2, 3], name=\"Volume\", data=[1000, 2600, 1524, 98000]\n)\nsimple_series\n\n\n# Create a simple DataFrame object\nsimple_df = pd.DataFrame(\n    index=[0, 1, 2, 3], columns=[\"Volume\"], data=[1000, 2600, 1524, 98000]\n)\nsimple_df\n\n\n\n\n\n\n\n\nVolume\n\n\n\n\n0\n1000\n\n\n1\n2600\n\n\n2\n1524\n\n\n3\n98000\n\n\n\n\n\n\n\nDataFrame objects are more general than Series objects, and one DataFrame can hold many Series objects, each as a different column. Let’s create a two-column DataFrame object:\n\n# Create another DataFrame object\nanother_df = pd.DataFrame(\n    index=[0, 1, 2, 3],\n    columns=[\"Date\", \"Volume\"],\n    data=[[20190101, 1000], [20190102, 2600], [20190103, 1524], [20190104, 98000]]\n)\nanother_df\n\n\n\n\n\n\n\n\nDate\nVolume\n\n\n\n\n0\n20190101\n1000\n\n\n1\n20190102\n2600\n\n\n2\n20190103\n1524\n\n\n3\n20190104\n98000\n\n\n\n\n\n\n\nNotice how a list of lists was used to specify the data in the another_df DataFrame. Each element of the outer list corresponds to a row in the DataFrame, so the outer list has 4 elements because there are 4 indices. Each element of the each inner list has 2 elements because the DataFrame has two columns.\n\n\n2.2.2 Reading in data\npandas allows easy loading of CSV files through the use of the method pd.read_csv().\nSyntax:\ndf = pd.read_csv(File name with path as a string)\nBefore loading the CSV file, you need to specify its location on your computer. The file path is the address that tells Python where to find the file. You cna use one of the following ways to specify the location\n\nAbsolute Path: This is the complete path to the file starting from the root directory (e.g., C:/Users/username/Documents/data.csv or /Users/YourUsername/Documents/data.csv).\nRelative Path: This is the path relative to the current working directory where your Python script or Jupyter notebook is located (e.g., data/data.csv). If your CSV file is in the same directory as your Python script or Jupyter notebook, you can just provide the file name.\n\nExamples:\n# Load a CSV file as a DataFrame and assign to df\n# Same folder: Here D.csv is in the same folder as my notebook\ndf = pd.read_csv(\"D.csv\")\n\n# Relative path: Here D.csv is in a folder called data, and the folder data is in the same folder as my notebook\ndf = pd.read_csv(\"data/D.csv\")\n\n# Absolute path: Here the full path starting from my hardrive, C:, to the file D.csv is stated. \n# On mac, it will look like: /Users/YourUsername/Documents/data.csv\ndf = pd.read_csv(\"C:\\Users\\OneDrive - York University\\Teaching\\Courses\\Math1130\\D.csv\")\nTo find out which folder your relative path starts from, use the command getcwd() from the os module.\n\nimport os\n\n# Get the current working directory\ncurrent_directory = os.getcwd()\n\n# Print the current working directory\nprint(\"Current Directory:\", current_directory)\n\n\n2.2.3 Basic commands for DataFrames\nThere are several common methods and attributes that allow one to take a peek at the data and get a sense of it:\n\nDataFrame.head() -&gt; returns the column names and first 5 rows by default\nDataFrame.tail() -&gt; returns the column names and last 5 rows by default\nDataFrame.shape -&gt; returns (num_rows, num_columns)\nDataFrame.columns -&gt; returns index of columns\nDataFrame.index -&gt; returns index of rows\n\nIn your spare time please check the pandas documentation and explore the parameters of these methods as well as other methods. Familiarity with this library will dramatically improve your productivity as a data scientist.\nUsing df.head() and df.tail() we can take a look at the data contents. Unless specified otherwise, Series and DataFrame objects have indices starting at 0 and increase monotonically upward along the integers.\nExample:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\n\n# 1. DataFrame.head()\nprint(\"DataFrame.head():\")\nprint(df.head())\nprint()  # Blank line for separation\n\n# 2. DataFrame.tail()\nprint(\"DataFrame.tail():\")\nprint(df.tail())\nprint()  # Blank line for separation\n\n# 3. DataFrame.shape\nprint(\"DataFrame.shape:\")\nprint(df.shape)  # Output: (5, 3) - 5 rows, 3 columns\nprint()  # Blank line for separation\n\n# 4. DataFrame.columns\nprint(\"DataFrame.columns:\")\nprint(df.columns)  # Output: Index(['Name', 'Age', 'City'], dtype='object')\nprint()  # Blank line for separation\n\n# 5. DataFrame.index\nprint(\"DataFrame.index:\")\nprint(df.index)  # Output: RangeIndex(start=0, stop=5, step=1)\nprint()  # Blank line for separation\n\n# Attributes\n# 1. shape attribute\nprint(\"df.shape attribute:\", df.shape)  # Output: (5, 3) - 5 rows, 3 columns\n\n# 2. columns attribute\nprint(\"df.columns attribute:\", df.columns)  # Output: Index(['Name', 'Age', 'City'], dtype='object')\n\n# 3. index attribute\nprint(\"df.index attribute:\", df.index)  # Output: RangeIndex(start=0, stop=5, step=1)\n\nDataFrame.head():\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nDataFrame.tail():\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nDataFrame.shape:\n(5, 3)\n\nDataFrame.columns:\nIndex(['Name', 'Age', 'City'], dtype='object')\n\nDataFrame.index:\nRangeIndex(start=0, stop=5, step=1)\n\ndf.shape attribute: (5, 3)\ndf.columns attribute: Index(['Name', 'Age', 'City'], dtype='object')\ndf.index attribute: RangeIndex(start=0, stop=5, step=1)\n\n\n\n\n2.2.4 Creating new columns and variables\nWe can create new columns by adding new columns to the DataFrame or creating a new column based on existing columns:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\n\n# 1. Adding new columns to the DataFrame\ndf['Gender'] = ['Female', 'Male', 'Male', 'Male', 'Female']\ndf['Salary'] = [50000, 60000, 75000, 80000, 70000]\n\nprint(\"DataFrame with new columns:\")\nprint(df)\nprint()  # Blank line for separation\n\n# 2. Creating a new column based on existing ones\ndf['Age_Squared'] = df['Age']**df['Age']\n\nprint(\"DataFrame with new 'Age_Squared' column:\")\nprint(df)\n\n# 3. We can also create columns based on multiple, other columns\ndf['Salary_over_Age'] = df['Salary']/df['Age']\n\nprint(\"DataFrame with new 'Salary_over_Age' column:\")\nprint(df)\n\nDataFrame with new columns:\n      Name  Age         City  Gender  Salary\n0    Alice   25     New York  Female   50000\n1      Bob   30  Los Angeles    Male   60000\n2  Charlie   35      Chicago    Male   75000\n3    David   40      Houston    Male   80000\n4      Eve   45        Miami  Female   70000\n\nDataFrame with new 'Age_Squared' column:\n      Name  Age         City  Gender  Salary          Age_Squared\n0    Alice   25     New York  Female   50000 -6776596920136667815\n1      Bob   30  Los Angeles    Male   60000  2565992168703393792\n2  Charlie   35      Chicago    Male   75000  8407224849895527163\n3    David   40      Houston    Male   80000                    0\n4      Eve   45        Miami  Female   70000  2604998672350111773\nDataFrame with new 'Salary_over_Age' column:\n      Name  Age         City  Gender  Salary          Age_Squared  \\\n0    Alice   25     New York  Female   50000 -6776596920136667815   \n1      Bob   30  Los Angeles    Male   60000  2565992168703393792   \n2  Charlie   35      Chicago    Male   75000  8407224849895527163   \n3    David   40      Houston    Male   80000                    0   \n4      Eve   45        Miami  Female   70000  2604998672350111773   \n\n   Salary_over_Age  \n0      2000.000000  \n1      2000.000000  \n2      2142.857143  \n3      2000.000000  \n4      1555.555556  \n\n\nHere we see the power of pandas. We can simply perform mathematical operations on columns of DataFrames just as if the DataFrames were single variables themselves.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#distributions-and-summary-statistics",
    "href": "Case_2.html#distributions-and-summary-statistics",
    "title": "2  Data extraction and transformation",
    "section": "2.3 Distributions and summary statistics",
    "text": "2.3 Distributions and summary statistics\nA common first step in data analysis is to learn about the characteristics or distribution of each of the relevant columns.\n\n2.3.1 Summary statistics\nSummary statistics are numerical measures that describe important aspects of a column in a dataset. They provide a concise overview of the data’s characteristics without needing to examine each individual value.\n\nExamples of Summary Statistics:\n\nMean: The average value of all data points.\nMedian: The middle value in a sorted list of numbers.\nMode: The most frequently occurring value.\nMax and Minimum: The maximum and minimum values in a column.\nRange: The difference between the maximum and minimum values.\nStandard Deviation: A measure of the amount of variation or dispersion in a set of values. The standard deviation is the square root of the average of the squared distances between the data points and the the mean of the column.\nPercentiles: Values below which a given percentage of observations fall.\n\n\nFor now, we can think of the distribution of a data column as a description of various aspects of that column. The distribution can be described through summary statistics, or as we will see later, through plots.\n\n\n2.3.2 Standard deviation:\nStandard Deviation is a measure of how spread out or dispersed the values in a dataset are from the mean (average) of the dataset. It tells you how much the individual data points typically differ from the mean value.\n\nSmall Standard Deviation:\n\nWhat it means: Most of the data points are close to the mean.\nExample: If the standard deviation of test scores in a class is small, it means most students scored close to the average score. There is less variability in scores.\n\nLarge Standard Deviation:\n\nWhat it means: The data points are spread out over a wider range of values.\nExample: If the standard deviation of test scores in a class is large, it means students’ scores vary widely from the average. Some students scored much higher or lower than the average.\n\nZero Standard Deviation:\n\nWhat it means: All data points are exactly the same.\nExample: If every student in a class scored the same on a test, the standard deviation would be zero, indicating no variability.\n\n\nImagine you have two sets of data representing the ages of two different groups of people.\n\nGroup 1: Ages are [25, 26, 25, 24, 25].\n\nThe mean age is 25.\nThe standard deviation is small because all ages are very close to the mean.\n\nGroup 2: Ages are [20, 30, 25, 40, 10].\n\nThe mean age is also 25.\nThe standard deviation is large because the ages are spread out over a wide range (from 10 to 40).\n\n\nIn summary, the standard deviation helps you understand the variability of your data. A smaller standard deviation indicates data points are close to the mean, while a larger standard deviation indicates data points are more spread out. This information is useful for comparing datasets, understanding data consistency, and identifying outliers.\n\n\n2.3.3 Summary statistics example:\n\n# Example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 30, 40, 35],\n    'Salary': [50000, 60000, 75000, 80000, 70000]\n}\n\ndf = pd.DataFrame(data)\n\n\n# Mean: The average value of all data points\nmean_value = df['Age'].mean()\nprint(f\"Mean: {mean_value}\")\n\n# Median: The middle value in a sorted list of numbers\nmedian_value = df['Age'].median()\nprint(f\"Median: {median_value}\")\n\n# Mode: The most frequently occurring value\nmode_value = df['Age'].mode()\nprint(f\"Mode: {mode_value.values}\")\n\n# Max and Minimum: The maximum and minimum values in a column\nmax_value = df['Age'].max()\nmin_value = df['Age'].min()\nprint(f\"Max: {max_value}\")\nprint(f\"Min: {min_value}\")\n\n# Range: The difference between the maximum and minimum values\nrange_value = max_value - min_value\nprint(f\"Range: {range_value}\")\n\n# Standard Deviation: A measure of the amount of variation or dispersion in a set of values\nstd_dev = df['Age'].std()\nprint(f\"Standard Deviation: {std_dev}\")\n\n# Percentiles: Values below which a given percentage of observations fall\npercentile_25 = df['Age'].quantile(0.25)\npercentile_50 = df['Age'].quantile(0.50)\npercentile_75 = df['Age'].quantile(0.75)\nprint(f\"25th Percentile: {percentile_25}\")\nprint(f\"50th Percentile: {percentile_50}\")\nprint(f\"75th Percentile: {percentile_75}\")\n\n\n# Describe:\ndf['Name'].describe()\ndf['Age'].describe()\ndf.describe()\n\nMean: 32.0\nMedian: 30.0\nMode: [30]\nMax: 40\nMin: 25\nRange: 15\nStandard Deviation: 5.70087712549569\n25th Percentile: 30.0\n50th Percentile: 30.0\n75th Percentile: 35.0\n\n\n\n\n\n\n\n\n\nAge\nSalary\n\n\n\n\ncount\n5.000000\n5.000000\n\n\nmean\n32.000000\n67000.000000\n\n\nstd\n5.700877\n12041.594579\n\n\nmin\n25.000000\n50000.000000\n\n\n25%\n30.000000\n60000.000000\n\n\n50%\n30.000000\n70000.000000\n\n\n75%\n35.000000\n75000.000000\n\n\nmax\n40.000000\n80000.000000\n\n\n\n\n\n\n\nIn addition to describe, there is a value_counts() method for checking the frequency of elements in categorical data. When applied to a DataFrame class, value_counts() will return the frequency of each row in the DataFrame. In other words, for each unique row it returns how many instances of that row are in the DataFrame. When applied to a Series class value_counts() will return the frequency of each unique value in the given Series class:\n\ndict_data = {\n    \"numbers\": [1, 2, 3, 4, 5, 6, 7, 8,1],\n    \"color\": [\"red\", \"red\", \"red\", \"blue\", \"blue\", \"green\", \"blue\", \"green\",\"red\"],\n}\ncategory_df = pd.DataFrame(data=dict_data)\n\ncategory_df\n\n#Gives the frquency of each unique row in the DataFrame\ncategory_df.value_counts()\n\n#Gives the frquency of each unique value in the Series\ncategory_df['color'].value_counts()\n\ncolor\nred      4\nblue     3\ngreen    2\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#more-on-pandas",
    "href": "Case_2.html#more-on-pandas",
    "title": "2  Data extraction and transformation",
    "section": "2.4 More on pandas",
    "text": "2.4 More on pandas\n\n2.4.1 Aggregating DataFrames\nOne way to combined multiple DataFrames is through the use of the pd.concat() method from pandas. We can input a list of DataFrames into pd.concat() that we’d like to concatenate. The pd.concat() function is used to concatenate (combine) two or more DataFrames or Series along a particular axis (rows or columns).\nExamples:\n\n# 1\n\n# Create two example DataFrames\ndata1 = {\n    'Name': ['Alice', 'Bob'],\n    'Age': [25, 30],\n    'City': ['New York', 'Los Angeles']\n}\ndf1 = pd.DataFrame(data1)\n\ndata2 = {\n    'Name': ['Charlie', 'David'],\n    'Age': [35, 40],\n    'City': ['Chicago', 'Houston']\n}\ndf2 = pd.DataFrame(data2)\n\n# Concatenate the two DataFrames\nresult = pd.concat([df1, df2], ignore_index=True)\n\nprint(\"Concatenated DataFrame:\")\nprint(result)\n\nConcatenated DataFrame:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\nExplanation: Two DataFrames (df1 and df2) are created with identical columns. The pd.concat([df1, df2]) function concatenates df1 and df2 along the rows (default behavior). The ignore_index=True argument reindexes the resulting DataFrame to have a continuous index.\n\n# 2\n\n# Create two example DataFrames\ndata1 = {\n    'Name': ['Alice', 'Bob'],\n    'Age': [25, 30]\n}\ndf1 = pd.DataFrame(data1)\n\ndata2 = {\n    'City': ['New York', 'Los Angeles'],\n    'Salary': [50000, 60000]\n}\ndf2 = pd.DataFrame(data2)\n\n# Concatenate the two DataFrames along columns\nresult = pd.concat([df1, df2], axis=1)\n\nprint(\"Concatenated DataFrame along columns:\")\nprint(result)\n\nConcatenated DataFrame along columns:\n    Name  Age         City  Salary\n0  Alice   25     New York   50000\n1    Bob   30  Los Angeles   60000\n\n\nExplanation: pd.concat([df1, df2], axis=1) concatenates df1 and df2 along the columns, resulting in a DataFrame that combines the columns of both input DataFrames. Using pd.concat(), you can easily combine multiple DataFrames or Series into a single DataFrame, which is useful for data manipulation and analysis tasks.\n\n\n2.4.2 Filtering DataFrames\nSure! Filtering a pandas DataFrame means selecting rows that meet certain criteria. This is often done using conditions on one or more columns. Here’s a simple example to illustrate how filtering works:\n\n\n2.4.3 Example DataFrame:\nimport pandas as pd\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n\n2.4.4 Filtering the DataFrame:\nFiltering a DataFrame means selecting rows that meet certain criteria. This is often done using conditions on one or more columns. Here’s a simple example to illustrate how filtering works:\n\nimport pandas as pd\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Filter rows where Age is greater than 30\nfiltered_df = df[df['Age'] &gt; 30]\n\nprint(\"\\nFiltered DataFrame (Age &gt; 30):\")\nprint(filtered_df)\n\n# Filter rows where City is 'Chicago'\nfiltered_df_city = df[df['City'] == 'Chicago']\n\nprint(\"\\nFiltered DataFrame (City is Chicago):\")\nprint(filtered_df_city)\n\n\n# Filter rows where Age is between 30 and 40 (inclusive)\nfiltered_df_age_range = df[(df['Age'] &gt;= 30) & (df['Age'] &lt;= 40)]\n\nprint(\"\\nFiltered DataFrame (30 &lt;= Age &lt;= 40):\")\nprint(filtered_df_age_range)\n\nOriginal DataFrame:\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n4      Eve   45        Miami\n\nFiltered DataFrame (Age &gt; 30):\n      Name  Age     City\n2  Charlie   35  Chicago\n3    David   40  Houston\n4      Eve   45    Miami\n\nFiltered DataFrame (City is Chicago):\n      Name  Age     City\n2  Charlie   35  Chicago\n\nFiltered DataFrame (30 &lt;= Age &lt;= 40):\n      Name  Age         City\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\nExplanation:\n\ndf[‘Age’] &gt; 30: This creates a boolean Series that is True for rows where the ‘Age’ value is greater than 30 and False otherwise.\ndf[df[‘Age’] &gt; 30]: This filters the DataFrame, returning only the rows where the condition is True.\ndf[‘City’] == ‘Chicago’: This creates a boolean Series that is True for rows where the ‘City’ value is ‘Chicago’.\ndf[(df[‘Age’] &gt;= 30) & (df[‘Age’] &lt;= 40)]: This filters the DataFrame using multiple conditions. The & operator is used to combine conditions, ensuring both conditions must be True for a row to be included.\n\nThis example demonstrates how to filter a pandas DataFrame based on different conditions, helping you to extract specific subsets of data that meet your criteria.\n\n\n2.4.5 Sorting\nThe sort_values() method in pandas is used to sort a DataFrame or Series by one or more columns or indices.\nSyntax:\nDataFrame.sort_values(by, axis=0, ascending=True,  na_position='last', ignore_index=False)\n\n\n2.4.6 Parameters:\n\nby: (str or list of str) The name(s) of the column(s) or index level(s) to sort by.\naxis: (int or str, default 0) The axis to sort along. 0 or ‘index’ to sort rows, 1 or ‘columns’ to sort columns.\nascending: (bool or list of bool, default True) Sort ascending vs. descending. Specify list for multiple sort orders.\nna_position: (str, default ‘last’) ‘first’ puts NaNs at the beginning, ‘last’ puts NaNs at the end.\nignore_index: (bool, default False) If True, the resulting index will be labeled 0, 1, …, n - 1.\n\nExample:\nLet’s create an example DataFrame and sort it using sort_values().\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45],\n    'Salary': [50000, 60000, 75000, 80000, 70000]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Sort the DataFrame by 'Age' in ascending order\nsorted_df = df.sort_values(by='Age')\nprint(\"\\nDataFrame sorted by Age:\")\nprint(sorted_df)\n\n# Sort the DataFrame by 'Salary' in descending order\nsorted_df_desc = df.sort_values(by='Salary', ascending=False)\nprint(\"\\nDataFrame sorted by Salary in descending order:\")\nprint(sorted_df_desc)\n\n# Sort the DataFrame by 'Age' and then by 'Salary'\nsorted_df_multi = df.sort_values(by=['Age', 'Salary'])\nprint(\"\\nDataFrame sorted by Age and then by Salary:\")\nprint(sorted_df_multi)\nIn these examples: - The DataFrame is sorted by the ‘Age’ column in ascending order. - The DataFrame is sorted by the ‘Salary’ column in descending order. - The DataFrame is sorted first by the ‘Age’ column, and within each age group, by the ‘Salary’ column.\n\n\n2.4.7 Groupby\npandas offers the ability to group related rows of DataFrames according to the values of other rows. This useful feature is accomplished using the groupby() method. The groupby method in pandas is used to group data based on one or more columns. It is often used with aggregation functions like sum(), mean(), count(), etc., to summarize data.\nSyntax:\nDataFrame.groupby(by, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=&lt;no_default&gt;, observed=False, dropna=True)\nParameters descriptions:\n\nby: Specifies the column(s) or keys to group by. This can be a single column name, a list of column names, or a dictionary mapping column names to group keys.\naxis: Determines whether to group by rows (axis=0, default) or columns (axis=1).\nas_index: If True (default), the group labels are used as the index. If False, the group labels are retained as columns.\nsort: If True (default), the groups are sorted. If False, the groups are not sorted.\ngroup_keys: If True (default), adds group keys to the index. If False, the group keys are not added.\n\nSyntax of common usages:\n\n# Grouping by a Single Column\ngrouped = df.groupby('column_name')\n\n\n# **Grouping by Multiple Columns**:\n\n\ngrouped = df.groupby(['column_name1', 'column_name2'])\n\n\n# **Applying Aggregation Functions**:\n\ngrouped_mean = df.groupby('column_name')['target_column'].mean()\n\n\n# **Using Multiple Aggregation Functions**:\n\n\ngrouped_agg = df.groupby('column_name').agg({\n    'target_column1': 'mean',\n    'target_column2': 'sum'\n})\nExample:\n\n# Use the groupby() method, notice a DataFrameGroupBy object is returned\ndf[['City',\"Age\"]].groupby('City').mean()\n\n\n\n\n\n\n\n\nAge\n\n\nCity\n\n\n\n\n\nChicago\n35.0\n\n\nHouston\n40.0\n\n\nLos Angeles\n30.0\n\n\nMiami\n45.0\n\n\nNew York\n25.0\n\n\n\n\n\n\n\n\nHere, the DataFrameGroupBy object can be most readily thought of as containing a DataFrame object for every group (in this case, a DataFrame object for each city).\nSpecifically, each item of the object is a tuple, containing the group identifier (in this case the city), and the corresponding rows of the DataFrame that have that city.\n\nLonger example:\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n    'Age': [25, 30, 35, 40, 45, 30],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami', 'Chicago'],\n    'Salary': [50000, 60000, 75000, 80000, 70000, 65000]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Group by 'City' and calculate the mean salary for each city\ngrouped = df.groupby('City')['Salary'].mean()\n\nprint(\"\\nMean Salary by City:\")\nprint(grouped)\n\n# Group by 'City' and calculate the sum of salaries for each city\ngrouped_sum = df.groupby('City')['Salary'].sum()\n\nprint(\"\\nSum of Salaries by City:\")\nprint(grouped_sum)\n\n# Group by 'City' and count the number of people in each city\ngrouped_count = df.groupby('City')['Name'].count()\n\nprint(\"\\nCount of People by City:\")\nprint(grouped_count)\n\nOriginal DataFrame:\n      Name  Age         City  Salary\n0    Alice   25     New York   50000\n1      Bob   30  Los Angeles   60000\n2  Charlie   35      Chicago   75000\n3    David   40      Houston   80000\n4      Eve   45        Miami   70000\n5    Frank   30      Chicago   65000\n\nMean Salary by City:\nCity\nChicago        70000.0\nHouston        80000.0\nLos Angeles    60000.0\nMiami          70000.0\nNew York       50000.0\nName: Salary, dtype: float64\n\nSum of Salaries by City:\nCity\nChicago        140000\nHouston         80000\nLos Angeles     60000\nMiami           70000\nNew York        50000\nName: Salary, dtype: int64\n\nCount of People by City:\nCity\nChicago        2\nHouston        1\nLos Angeles    1\nMiami          1\nNew York       1\nName: Name, dtype: int64\n\n\nExplanation:\n\ndf.groupby(‘City’): This groups the DataFrame by the ‘City’ column. Each unique value in ‘City’ will form a group.\n[‘Salary’].mean(): This calculates the mean salary for each group (city).\n[‘Salary’].sum(): This calculates the sum of salaries for each group (city).\n[‘Name’].count(): This counts the number of entries for each group (city).\nGrouping: df.groupby('City') creates groups based on unique values in the ‘City’ column.\nAggregation: Using aggregation functions like mean(), sum(), and count() allows you to summarize the data within each group.\nResult: The output shows the mean salary, sum of salaries, and count of people for each city, respectively.\n\nThe groupby method is very powerful for data analysis and manipulation, allowing you to easily aggregate and summarize data based on specific criteria.\n\n\n2.4.8 Numpy’s where()\nThe np.where function in is used to return elements chosen from two values based on whether a condition holds.\nSyntax:\nnp.where(condition, x, y)\nParameters: - condition: An array-like object (e.g., a series or NumPy array, list, or etc) that evaluates to True or False. - x: The value to choose when the condition is True. - y: The value to choose when the condition is False.\nnp.where(condition, x, y) returns an array with elements from x where the condition is True and elements from y where the condition is False.\n\n# Create an example DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 35, 40, 45]\n}\ndf = pd.DataFrame(data)\n\n# Use np.where to create a new column 'Age Group'\ndf['Age Group'] = np.where(df['Age'] &gt;= 35, 'Senior', 'Junior')\n\nprint(df)\n\n      Name  Age Age Group\n0    Alice   25    Junior\n1      Bob   30    Junior\n2  Charlie   35    Senior\n3    David   40    Senior\n4      Eve   45    Senior\n\n\nExplanation: - Condition: df['Age'] &gt;= 35 checks if the ‘Age’ column values are greater than or equal to 35. - True: For rows where the condition is True, it assigns ‘Senior’. - False: For rows where the condition is False, it assigns ‘Junior’.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_2.html#plotting-with-matplotlib",
    "href": "Case_2.html#plotting-with-matplotlib",
    "title": "2  Data extraction and transformation",
    "section": "2.5 Plotting with Matplotlib",
    "text": "2.5 Plotting with Matplotlib\nThe standard Python plotting library is matplotlib. Let’s import the library and instruct Jupyter to display the plots inline (i.e. display the plots to the notebook screen so we can see them as we run the code).\n\n# import fundamental plotting library in Python\nimport matplotlib.pyplot as plt\n\n# Instruct jupyter/VS Code to plot in the notebook\n%matplotlib inline\n\nTo plot a series, use .plot(). We will come back to matplotlib later.\n\ndf['Age'].plot()\n\n\n\n\n\n\n\n\n\n2.5.1 Datetime objects\nPython’s internal data representation of dates is given by DateTime objects. Datetime objects are crucial for handling time-related data in a structured way, enabling various operations like comparison, arithmetic, and formatting. pandas offers the to_datetime() method to convert a string that represents a given date format into a datetime-like object. This is useful for ensuring that date and time data are properly recognized and can be used for time series analysis, indexing, and plotting.\nSyntax:\npd.to_datetime(arg, format=None)\n\n\n2.5.2 Parameters:\n\narg: The date/time string(s) or list-like object to convert.\nformat: The strftime to parse time. For example, “%Y-%m-%d”.\n\nThe format parameter in pd.to_datetime() is used to specify the exact format of the date/time strings being parsed. This is particularly useful when the input date strings do not conform to standard formats or when you want to improve parsing performance by explicitly defining the format.\nDate Formatting Directives:\n\n%Y: Four-digit year (e.g., 2023).\n%y: Two-digit year (e.g., 23 for 2023).\n%m: Month as a zero-padded decimal number (e.g., 07 for July).\n%B: Full month name (e.g., July).\n%b or %h: Abbreviated month name (e.g., Jul for July).\n%d: Day of the month as a zero-padded decimal number (e.g., 03 for the 3rd).\n%A: Full weekday name (e.g., Monday).\n%a: Abbreviated weekday name (e.g., Mon).\n%H: Hour (24-hour clock) as a zero-padded decimal number (e.g., 14 for 2 PM).\n%I: Hour (12-hour clock) as a zero-padded decimal number (e.g., 02 for 2 PM).\n%p: AM or PM designation.\n%M: Minute as a zero-padded decimal number (e.g., 30 for 30 minutes past the hour).\n%S: Second as a zero-padded decimal number (e.g., 00 for 0 seconds).\n%f: Microsecond as a decimal number, zero-padded on the left (e.g., 000000).\n%j: Day of the year as a zero-padded decimal number (e.g., 189 for July 8th).\n%U: Week number of the year (Sunday as the first day of the week) as a zero-padded decimal number (e.g., 27).\n%W: Week number of the year (Monday as the first day of the week) as a zero-padded decimal number (e.g., 27).\n%w: Weekday as a decimal number (0 for Sunday, 6 for Saturday).\n%Z: Time zone name (e.g., UTC, EST).\n%z: UTC offset in the form +HHMM or -HHMM (e.g., +0530, -0800).\n%%: A literal ‘%’ character.\n\nExample formulae:\n\n%Y-%m-%d matches dates like 2023-07-03.\n%d/%m/%Y matches dates like 03/07/2023.\n%B %d, %Y matches dates like July 3, 2023.\n%I:%M %p matches times like 02:30 PM.\n%H:%M:%S.%f matches times like 14:30:00.000000.\n\nExamples: Parsing Date in Non-Standard Format:\n\n# Example date string in non-standard format\ndate_str = '03-07-23'  # This represents July 3, 2023 in YY-MM-DD format\n\n# Convert to datetime using format parameter\ndate = pd.to_datetime(date_str, format='%y-%m-%d')\nprint(date)\n\n2003-07-23 00:00:00\n\n\nIn this example: - %y-%m-%d specifies the format where %y represents the two-digit year, %m represents the month, and %d represents the day.\nParsing Date and Time Together:\n\ndate_time_str = '07/03/2023 14:30:00'\n\n# Convert to datetime using format parameter\ndate_time = pd.to_datetime(date_time_str, format='%m/%d/%Y %H:%M:%S')\nprint(date_time)\n\n2023-07-03 14:30:00\n\n\nIn this example: - %m/%d/%Y %H:%M:%S specifies the format where %m/%d/%Y represents the date in MM/DD/YYYY format, and %H:%M:%S represents the time in HH:MM:SS format.\nHandling Dates with Textual Month:\n\ndate_str_textual = 'July 3, 2023'\n\n# Convert to datetime using format parameter\ndate_textual = pd.to_datetime(date_str_textual, format='%B %d, %Y')\nprint(date_textual)\n\n2023-07-03 00:00:00\n\n\nIn this example: - %B %d, %Y specifies the format where %B represents the full month name (e.g., July), %d represents the day, and %Y represents the four-digit year.\nYou can also use ChatGPT or Google the format of your date at hand!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data extraction and transformation</span>"
    ]
  },
  {
    "objectID": "Case_3.html",
    "href": "Case_3.html",
    "title": "3  Data Transformation I",
    "section": "",
    "text": "3.1 Preliminary modules\nimport numpy as np\nimport pandas as pd",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#some-more-pandas-functions",
    "href": "Case_3.html#some-more-pandas-functions",
    "title": "3  Data Transformation I",
    "section": "3.2 Some more pandas functions",
    "text": "3.2 Some more pandas functions\n\n3.2.1 Unique\nThe unique function in pandas is used to find the unique values in a Series or a column of a DataFrame. It returns the unique values as a NumPy array. This function is useful when you need to identify the distinct values in a dataset.\nSyntax:\npandas.Series.unique()\nThis method returns the unique values in the Series.\nExamples:\n\n# Creating a Series\ndata = pd.Series([1, 2, 2, 3, 4, 4, 4, 5])\n\n# Finding unique values\nunique_values = data.unique()\n\nprint(unique_values)\n\n\n# In this example, `data.unique()` returns a NumPy array containing the unique values `[1, 2, 3, 4, 5]` in the Series `data`.\n\n\n# Example 2: Finding Unique Values in a DataFrame Column\n\n# In this example, `df['A'].unique()` returns a NumPy array of unique values in column 'A', and `df['B'].unique()` returns a NumPy array of unique values in column 'B'.\n\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 2, 3, 4, 4, 4, 5],\n    'B': ['a', 'b', 'b', 'c', 'd', 'd', 'd', 'e']\n})\n\n# Finding unique values in column 'A'\nunique_values_A = df['A'].unique()\n\n# Finding unique values in column 'B'\nunique_values_B = df['B'].unique()\n\nprint(\"Unique values in column A:\", unique_values_A)\nprint(\"Unique values in column B:\", unique_values_B)\n\n[1 2 3 4 5]\nUnique values in column A: [1 2 3 4 5]\nUnique values in column B: ['a' 'b' 'c' 'd' 'e']\n\n\nThe .apply function\nSure! The .apply() function in pandas is used to apply a function along the axis of a DataFrame or to elements of a Series. This function is highly versatile and can be used to perform complex operations on your data.\nBasic Syntax\nFor a Series:\nSeries.apply(func, convert_dtype=True, args=())\nFor a DataFrame:\nDataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), **kwds)\n\nfunc: The function to apply to each element (Series) or to each column/row (DataFrame).\naxis: {0 or ‘index’, 1 or ‘columns’}, default 0. The axis along which the function is applied:\n\n0 or ‘index’: apply function to each column.\n1 or ‘columns’: apply function to each row.\n\nargs: Positional arguments to pass to the function.\nraw: Determines if the function is passed raw ndarray values or pandas objects (Series/DataFrame).\nresult_type: {‘expand’, ‘reduce’, ‘broadcast’, None}, default None.\n\nExamples:\nExample 1: Applying a Function to Each Element in a Series\nimport pandas as pd\n\n# Creating a Series\ndata = pd.Series([1, 2, 3, 4, 5])\n\n# Function to square each element\ndef square(x):\n    return x * x\n\n# Applying the function\nsquared_data = data.apply(square)\n\nprint(squared_data)\nOutput:\n0     1\n1     4\n2     9\n3    16\n4    25\ndtype: int64\nIn this example, the square function is applied to each element of the Series data, resulting in a new Series squared_data where each value is the square of the corresponding original value.\nExample 2: Applying a Function to Each Column in a DataFrame\nimport pandas as pd\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Function to sum the elements of a column\ndef column_sum(col):\n    return col.sum()\n\n# Applying the function to each column\ncolumn_sums = df.apply(column_sum, axis=0)\n\nprint(column_sums)\nOutput:\nA     6\nB    15\nC    24\ndtype: int64\nIn this example, the column_sum function is applied to each column of the DataFrame df, resulting in a Series column_sums containing the sum of the elements in each column.\nExample 3: Applying a Function to Each Row in a DataFrame\nimport pandas as pd\n\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6],\n    'C': [7, 8, 9]\n})\n\n# Function to find the maximum value in a row\ndef row_max(row):\n    return row.max()\n\n# Applying the function to each row\nrow_maxs = df.apply(row_max, axis=1)\n\nprint(row_maxs)\nOutput:\n0    7\n1    8\n2    9\ndtype: int64\nIn this example, the row_max function is applied to each row of the DataFrame df, resulting in a Series row_maxs containing the maximum value in each row.\nThe .apply() function in pandas is a powerful tool for applying custom functions to Series or DataFrame columns/rows. It is highly flexible and can handle a variety of operations, making it essential for data transformation and analysis tasks. The basic syntax involves calling the .apply() method with the function to apply and the axis along which to apply it, yielding a transformed Series or DataFrame.\n\n\n3.2.2 Upper and lower case strings\n\nstr.lower\n\nThe str.lower function in pandas is used to convert all characters in a string to lowercase. It is often used when you want to standardize text data, making it easier to compare strings that might have different cases.\nSyntax:\nSeries.str.lower()\nExample:\n\n# Creating a Series\ndata = pd.Series(['Hello', 'World', 'PANDAS'])\n\n# Converting to lowercase\nlowercase_data = data.str.lower()\n\nprint(lowercase_data)\nOutput:\n0    hello\n1    world\n2    pandas\ndtype: object\n\nstr.upper\n\nThe str.upper function in pandas is used to convert all characters in a string to uppercase. This is useful for standardizing text data to a uniform case.\nSyntax:\nSeries.str.upper()\nExample:\n\n# Creating a Series\ndata = pd.Series(['hello', 'world', 'pandas'])\n\n# Converting to uppercase\nuppercase_data = data.str.upper()\n\nprint(uppercase_data)\nOutput:\n0    HELLO\n1    WORLD\n2    PANDAS\ndtype: object\n\ncount\n\nThe count function in pandas is used to count the number of occurrences of a specified value in a Series or to count non-NA/null entries across a DataFrame.\nSyntax for Series:\nSeries.str.count(pat)\n\npat: The pattern or substring to count.\n\nExample: Counting substrings in a Series:\n\n\n# Creating a Series\ndata = pd.Series(['apple', 'banana', 'apple pie', 'cherry'])\n\n# Counting occurrences of 'apple'\napple_count = data.str.count('apple')\n\nprint(apple_count)\nOutput:\n0    1\n1    0\n2    1\n3    0\ndtype: int64\n\nstr.lower: Converts all characters in a string to lowercase.\nstr.upper: Converts all characters in a string to uppercase.\nstr.count: Counts the occurrences of a pattern or substring in a Series.\n.count(): Counts the number of non-NA/null entries in a Series or DataFrame.\n\nThese functions are useful for text data standardization and for counting occurrences of specific values or patterns in your data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#personalized-functions",
    "href": "Case_3.html#personalized-functions",
    "title": "3  Data Transformation I",
    "section": "3.3 Personalized functions",
    "text": "3.3 Personalized functions\n\n3.3.1 Custom functions\nIn Python, you can create your own functions to encapsulate reusable code, improve readability, and make your programs more modular. Here’s a brief explanation of how to define and use your own functions in Python.\nA function is defined using the def keyword, followed by the function name, parentheses (), and a colon :. The code block within the function is indented.\nSyntax:\ndef function_name(parameters):\n    \"\"\"Docstring (optional): A brief description of the function.\"\"\"\n    # Function body\n    # Code to be executed\n    return value  # Optional: return statement to return a value\n\nfunction_name: The name of the function.\nparameters: (Optional) A list of parameters (or arguments) that the function accepts.\nDocstring: (Optional) A string describing what the function does.\nreturn: (Optional) The value that the function returns.\n\nExample 1: A Simple Function\nHere’s a simple example of a function that takes no parameters and prints a message:\ndef greet():\n    \"\"\"Print a greeting message.\"\"\"\n    print(\"Hello, world!\")\n\n# Calling the function\ngreet()\nOutput:\nHello, world!\nExample 2: A Function with Parameters\nHere’s a function that takes two parameters and returns their sum:\ndef add(a, b):\n    \"\"\"Return the sum of two numbers.\"\"\"\n    return a + b\n\n# Calling the function\nresult = add(3, 5)\nprint(result)\nOutput:\n8\nExample 3: A Function with Default Parameters\nYou can also define functions with default parameter values:\ndef greet(name=\"world\"):\n    \"\"\"Print a greeting message to the given name.\"\"\"\n    print(f\"Hello, {name}!\")\n\n# Calling the function with and without an argument\ngreet(\"Alice\")\ngreet()\nOutput:\nHello, Alice!\nHello, world!\nExample 4: A Function with Variable Number of Arguments\nSometimes you might want to define a function that can accept a variable number of arguments using *args and **kwargs:\ndef print_numbers(*args):\n    \"\"\"Print all the numbers passed as arguments.\"\"\"\n    for number in args:\n        print(number)\n\n# Calling the function with multiple arguments\nprint_numbers(1, 2, 3, 4, 5)\nOutput:\n1\n2\n3\n4\n5\nExample 5: A Function with Keyword Arguments\nYou can use **kwargs to accept a variable number of keyword arguments:\ndef print_info(**kwargs):\n    \"\"\"Print key-value pairs of the passed keyword arguments.\"\"\"\n    for key, value in kwargs.items():\n        print(f\"{key}: {value}\")\n\n# Calling the function with multiple keyword arguments\nprint_info(name=\"Alice\", age=30, city=\"New York\")\nOutput:\nname: Alice\nage: 30\ncity: New York\n\nDefine a function using the def keyword.\nProvide parameters within the parentheses (optional).\nAdd a docstring to describe the function (optional but recommended).\nWrite the function body with the code to be executed.\nReturn a value using the return statement (optional).\n\nCreating functions in Python helps organize code, reduce repetition, and improve clarity. Functions can take parameters, have default values, and handle a variable number of arguments.\n\n\n3.3.2 The functions map, filter and sorted\nThe map(), filter(), and sorted() functions are built-in Python functions that provide powerful tools for working with iterable objects, such as lists or tuples. Here’s a brief overview of each function:\nmap()\nThe map() function applies a given function to all items in an input list (or other iterable).\nSyntax:\nmap(function, iterable, ...)\n\nfunction: A function that is applied to each item of the iterable.\niterable: One or more iterable objects (e.g., lists, tuples).\n\nExample:\n# Example function\ndef square(x):\n    return x ** 2\n\n# Input list\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply `square` function to each item in the list\nsquared_numbers = map(square, numbers)\n\n# Convert the map object to a list and print\nprint(list(squared_numbers))\nOutput:\n[1, 4, 9, 16, 25]\nThe filter() function constructs an iterator from elements of an iterable for which a function returns true.\nSyntax:\nfilter(function, iterable)\n\nfunction: A function that tests if each element of an iterable returns true or false.\niterable: An iterable to be filtered.\n\nExample:\n# Example function\ndef is_even(x):\n    return x % 2 == 0\n\n# Input list\nnumbers = [1, 2, 3, 4, 5]\n\n# Apply `is_even` function to filter even numbers\neven_numbers = filter(is_even, numbers)\n\n# Convert the filter object to a list and print\nprint(list(even_numbers))\nOutput:\n[2, 4]\nsorted()\nThe sorted() function returns a new sorted list from the items in an iterable.\nSyntax:\nsorted(iterable, key=None, reverse=False)\n\niterable: An iterable to be sorted.\nkey: A function that serves as a key for the sort comparison (optional).\nreverse: A boolean value. If True, the list elements are sorted as if each comparison were reversed (optional, default is False).\n\nExample:\n# Input list\nnumbers = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n\n# Sort the list\nsorted_numbers = sorted(numbers)\n\nprint(sorted_numbers)\nOutput:\n[1, 1, 2, 3, 4, 5, 5, 6, 9]\n\nmap(): Applies a function to every item in an iterable and returns a map object of the results.\nfilter(): Constructs an iterator from elements of an iterable for which a function returns true.\nsorted(): Returns a new sorted list from the items in an iterable, with optional custom sorting behavior.\n\n\n\n3.3.3 Anonymous functions\nIn Python, anonymous functions are defined using the lambda keyword. These functions are often called “lambda functions” and are used for creating small, one-time, and inline function objects. Unlike regular functions defined with def, lambda functions are limited to a single expression.\nSyntax of Lambda Functions\nlambda arguments: expression\n\nlambda: The keyword used to define an anonymous function.\narguments: A comma-separated list of parameters.\nexpression: A single expression that is evaluated and returned.\n\nExample 1: Simple Lambda Function\nHere’s a simple example of a lambda function that adds two numbers:\n# Lambda function to add two numbers\nadd = lambda x, y: x + y\n\n# Using the lambda function\nresult = add(3, 5)\nprint(result)\nOutput:\n8\nExample 2: Lambda Function in map()\nLambda functions are often used with functions like map(), filter(), and sorted().\n# List of numbers\nnumbers = [1, 2, 3, 4, 5]\n\n# Using lambda with map() to square each number\nsquared_numbers = list(map(lambda x: x ** 2, numbers))\n\nprint(squared_numbers)\nOutput:\n[1, 4, 9, 16, 25]\nExample 3: Lambda Function in filter()\n# List of numbers\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Using lambda with filter() to get even numbers\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n\nprint(even_numbers)\nOutput:\n[2, 4, 6, 8, 10]\nExample 4: Lambda Function in sorted()\n# List of tuples\npoints = [(1, 2), (3, 1), (5, -1), (2, 3)]\n\n# Using lambda with sorted() to sort by the second element of each tuple\nsorted_points = sorted(points, key=lambda x: x[1])\n\nprint(sorted_points)\nOutput:\n[(5, -1), (3, 1), (1, 2), (2, 3)]\nCharacteristics and Limitations:\n\nSingle Expression: Lambda functions can only contain a single expression. They cannot include statements or annotations.\nNo return Statement: The expression result is implicitly returned.\nLimited Use: Lambda functions are best used for short, simple operations. For more complex functions, it is better to define a regular function using def.\n\nWhen to Use Lambda Functions:\n\nShort, Simple Functions: When you need a quick function for a short, simple operation.\nInline Functions: When you need a function for a one-time use, especially within functions like map(), filter(), or sorted().\nReadability: When using a lambda function improves the readability and conciseness of your code.\n\nLambda functions in Python provide a concise way to create anonymous functions for simple, one-time operations. They are defined using the lambda keyword and can be used wherever function objects are required. However, due to their limitations, they are best suited for short, simple tasks and should be used judiciously to maintain code readability.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#pivot-tables",
    "href": "Case_3.html#pivot-tables",
    "title": "3  Data Transformation I",
    "section": "3.4 Pivot Tables",
    "text": "3.4 Pivot Tables\nThe pd.pivot_table() function in pandas is a powerful tool for summarizing and reshaping data. It allows you to aggregate data and create a new table that is a more compact and organized representation of your original DataFrame.\nSyntax:\npd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None,  dropna=True)\n\ndata: The DataFrame to pivot.\nvalues: Column(s) to aggregate.\nindex: Column(s) to set as index.\ncolumns: Column(s) to pivot.\naggfunc: Function to aggregate the data (default is ‘mean’).\nfill_value: Value to replace missing values.\ndropna: Do not include columns whose entries are all NaN (default is True).\n\nExample:\nLet’s consider a dataset containing sales data:\n\n# Sample data\ndata = {\n    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'],\n    'Product': ['A', 'A', 'B', 'B', 'A', 'B', 'A', 'B'],\n    'Sales': [100, 150, 200, 130, 120, 170, 160, 180],\n    'Quantity': [10, 15, 20, 13, 12, 17, 16, 18]\n}\n\ndf = pd.DataFrame(data)\n\nprint(df)\nOutput:\n   Region Product  Sales  Quantity\n0   North       A    100        10\n1   South       A    150        15\n2    East       B    200        20\n3    West       B    130        13\n4   North       A    120        12\n5   South       B    170        17\n6    East       A    160        16\n7    West       B    180        18\nCreating a Pivot Table:\nLet’s create a pivot table to summarize the total sales and quantity for each region and product.\npivot_table = pd.pivot_table(df, values=['Sales', 'Quantity'], index=['Region'], columns=['Product'], aggfunc='sum', fill_value=0)\n\nprint(pivot_table)\nOutput:\n        Sales          Quantity         \nProduct      A    B        A   B\nRegion                              \nEast       160  200       16  20\nNorth      220    0       22   0\nSouth      150  170       15  17\nWest         0  310        0  31\nExplanation:\n\ndata: The DataFrame to pivot (df in this case).\nvalues: The columns to aggregate ('Sales' and 'Quantity').\nindex: The column(s) to set as the index of the pivot table ('Region').\ncolumns: The column(s) to pivot ('Product').\naggfunc: The aggregation function ('sum'), to get the total sales and quantity.\nfill_value: The value to replace missing values (0).\n\nThe resulting pivot table shows the total sales and quantities for each combination of region and product. The rows represent the regions, and the columns represent the products. The values in the table are the sums of sales and quantities.\nThe pd.pivot_table() function in pandas is a versatile tool for data summarization and reshaping. It allows you to: - Aggregate data using various functions (e.g., sum, mean). - Pivot data by specifying index and columns. - Handle missing values using fill_value.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#line-plots-with-seaborn",
    "href": "Case_3.html#line-plots-with-seaborn",
    "title": "3  Data Transformation I",
    "section": "3.5 Line plots with seaborn",
    "text": "3.5 Line plots with seaborn\nLine plots are useful for visualizing data over continuous intervals or time series. The module seaborn is a powerful data visualization library built on top of Matplotlib, you can create line plots easily using the lineplot function.\nSyntax:\nseaborn.lineplot(data=None, *, x=None, y=None, hue=None, size=None, style=None, palette=None, data_frame=None, **kwargs)\n\ndata: DataFrame, array, or list of arrays, optional. Dataset for plotting.\nx: Name of the x-axis variable.\ny: Name of the y-axis variable.\nhue: Grouping variable that will produce lines with different colors.\nsize: Grouping variable that will produce lines with different widths.\nstyle: Grouping variable that will produce lines with different dash patterns.\npalette: Colors to use for different levels of the hue variable.\n\nExample 1: Simple Line Plot\nHere’s a basic example of creating a line plot with Seaborn.\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700]\n}\n\ndf = pd.DataFrame(data)\n\n# Create a line plot\nsns.lineplot(data=df, x='Year', y='Sales')\n\n# Show the plot\nplt.show()\nExample 2: Line Plot with Multiple Lines\nYou can plot multiple lines by specifying a hue parameter.\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020, 2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700, 100, 150, 200, 250, 300, 350],\n    'Product': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B']\n}\n\ndf = pd.DataFrame(data)\n\n# Create a line plot with multiple lines\nsns.lineplot(data=df, x='Year', y='Sales', hue='Product')\n\n# Show the plot\nplt.show()\nExample 3: Customizing Line Plot\nYou can customize the appearance of your line plot by using various parameters and options.\n# Sample data\ndata = {\n    'Year': [2015, 2016, 2017, 2018, 2019, 2020],\n    'Sales': [200, 300, 400, 500, 600, 700]\n}\n\ndf = pd.DataFrame(data)\n\n# Create a customized line plot\nsns.lineplot(data=df, x='Year', y='Sales', marker='o', linestyle='--', color='red')\n\n# Add titles and labels\nplt.title('Yearly Sales')\nplt.xlabel('Year')\nplt.ylabel('Sales')\n\n# Show the plot\nplt.show()\n\nCreating Line Plots: Use sns.lineplot() to create line plots easily.\nMultiple Lines: Differentiate groups with the hue parameter.\nCustomization: Customize appearance using parameters like marker, linestyle, and color.\nData Integration: Seamlessly integrates with pandas DataFrames, allowing for intuitive plotting using column names.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_3.html#writing-to-a-csv-file",
    "href": "Case_3.html#writing-to-a-csv-file",
    "title": "3  Data Transformation I",
    "section": "3.6 Writing to a csv file",
    "text": "3.6 Writing to a csv file\nCertainly! The to_csv() function in pandas is used to export a DataFrame to a CSV (Comma-Separated Values) file. CSV files are a common data storage format that is both human-readable and easy to process with various software tools. You can customize the output by specifying various parameters.\nSyntax:\nDataFrame.to_csv(path_or_buf=None, sep=',', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None,, encoding=None, date_format=None)\nParameters:\n\npath_or_buf: The file path or object where the CSV will be saved. If not specified, the result is returned as a string.\nsep: The string to use as the separating character (default is a comma ,).\nna_rep: String representation of missing data.\nfloat_format: Format string for floating-point numbers.\ncolumns: Subset of columns to write to the CSV file.\nheader: Write out the column names (default is True).\nindex: Write row names (index) (default is True).\nindex_label: Column label for the index column(s) if desired.\nencoding: Encoding to use for writing (default is None).\ndate_format: Format string for datetime objects.\n\nExample 1: Simple Export\nHere’s a basic example of exporting a DataFrame to a CSV file.\nimport pandas as pd\n\n# Sample DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'Los Angeles', 'Chicago']\n}\n\ndf = pd.DataFrame(data)\n\n# Export DataFrame to CSV\ndf.to_csv('people.csv')\nThis will create a file named people.csv with the following content:\n,Name,Age,City\n0,Alice,25,New York\n1,Bob,30,Los Angeles\n2,Charlie,35,Chicago\nExample 2: Customizing Output\nYou can customize the output by specifying different parameters.\n# Export DataFrame to CSV without the index and with a custom separator\ndf.to_csv('people_no_index.csv', index=False, sep=';')\nThis will create a file named people_no_index.csv with the following content:\nName;Age;City\nAlice;25;New York\nBob;30;Los Angeles\nCharlie;35;Chicago\nExample 3: Handling Missing Values and Specifying Columns\n# Sample DataFrame with missing values\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, None, 35, 40],\n    'City': ['New York', 'Los Angeles', 'Chicago', None]\n}\n\ndf = pd.DataFrame(data)\n\n# Export DataFrame to CSV with custom NA representation and specific columns\ndf.to_csv('people_missing_values.csv', na_rep='N/A', columns=['Name', 'Age'])\nThis will create a file named people_missing_values.csv with the following content:\n,Name,Age\n0,Alice,25.0\n1,Bob,N/A\n2,Charlie,35.0\n3,David,40.0\nSummary\n\nBasic Usage: Use to_csv('filename.csv') to export a DataFrame to a CSV file.\nCustom Separator: Change the delimiter using the sep parameter.\nNo Index: Exclude the DataFrame index using index=False.\nHandle Missing Values: Specify a representation for missing values with na_rep.\nSelect Columns: Export only specific columns using the columns parameter.\nAdvanced Options: Control formatting, quoting, encoding, and more with additional parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation I</span>"
    ]
  },
  {
    "objectID": "Case_4.html",
    "href": "Case_4.html",
    "title": "4  Data Transformation II",
    "section": "",
    "text": "4.1 Preliminary modules\n# Load packages\nimport os\nimport pandas as pd\nimport numpy as np\n\n# This line is needed to display plots inline in Jupyter Notebook\n%matplotlib inline\n\n# Required for basic python plotting functionality\nimport matplotlib.pyplot as plt\n\n# Required for formatting dates later in the case\nimport datetime\nimport matplotlib.dates as mdates\n\n# Advanced plotting functionality with seaborn\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")  # can set style depending on how you'd like it to look",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-matplotlib-plotting",
    "href": "Case_4.html#overview-of-matplotlib-plotting",
    "title": "4  Data Transformation II",
    "section": "4.2 Overview of matplotlib plotting",
    "text": "4.2 Overview of matplotlib plotting\nFor an introduction to matplotlib please use their quickstart guide.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-seaborn-plotting",
    "href": "Case_4.html#overview-of-seaborn-plotting",
    "title": "4  Data Transformation II",
    "section": "4.3 Overview of seaborn plotting",
    "text": "4.3 Overview of seaborn plotting\nSeaborn can be used to make more advanced plots, it also has a fairly simple syntax. We give a very brief introduction here. For more information, please follow the tutorials here.\nSyntax:\nsns.plot_function(data=pandas DF, x=column for the x-axis, y=column for the y-axis, hue=column to seperate the lines or points by color, kind= lines or points)\nEssentially, you specify the DataFrame, which variables go on the \\(y\\)-axis and the \\(x\\)-axis. Optionally, you can specidy a variable for the hue parameter. When it is specified, Seaborn assigns different colors to different levels of the hue variable, making it easy to see how different categories compare. kind tells seaborn whether to use lines or points etc.\nExample:\nOur example will use relplot. The relplot function in Seaborn is a high-level interface for creating relational plots that combine several plots like scatterplot and lineplot. It allows you to easily visualize relationships between multiple variables in a dataset. We’ll use the built-in tips dataset from Seaborn, which contains information about restaurant tips.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a relational plot using relplot\nsns.relplot(data=tips, x='total_bill', y='tip', hue='day', kind='scatter')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nExplanation:\n\nData: The tips dataset contains columns such as total_bill, tip, sex, smoker, day, time, and size.\nx: The total_bill column is used for the x-axis.\ny: The tip column is used for the y-axis.\nhue: The day column is used to color the points differently for each day of the week.\n\nThe resulting plot shows a scatter plot of total_bill vs. tip, with different colors representing different days of the week. This makes it easy to see if there are any trends or differences in tipping behavior on different days.\nYou can further customize the relplot with additional parameters, such as changing the kind of plot to a line plot and adding more dimensions with style and size.\n\n# Create a line plot with relplot\nsns.relplot(data=tips, x='total_bill', y='tip', hue='day', style='time', size='size', kind='line')\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#overview-of-new-plots",
    "href": "Case_4.html#overview-of-new-plots",
    "title": "4  Data Transformation II",
    "section": "4.4 Overview of new plots",
    "text": "4.4 Overview of new plots\nBelow is a brief overview of each of the new plot-types introduced in Case 4, along with the Seaborn syntax for creating them:\n\n4.4.1 Scatterplot\nA scatter plot is used to display the relationship between two continuous variables. Each point represents an observation in the dataset.\nExample:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load example data\ntips = sns.load_dataset('tips')\n\n# Create a scatter plot\nsns.scatterplot(data=tips, x='total_bill', y='tip', hue='day')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.2 Histogram\nA histogram displays the distribution of a single continuous variable by dividing the data into bins and counting the number of observations in each bin. When analyzing a histogram, people typically look for several key characteristics:\n\nShape of the Distribution\n\nSymmetry: Whether the distribution is symmetric or asymmetric. A symmetric histogram has a bell-shaped curve.\nSkewness: The direction of the tail. A histogram with a long tail on the right side is right-skewed (positive skewness), and a histogram with a long tail on the left side is left-skewed (negative skewness).\nModality: The number of peaks in the histogram. A unimodal histogram has one peak, a bimodal histogram has two peaks, and a multimodal histogram has multiple peaks.\n\nCentral Tendency\n\nMean: The average value of the data, which can be roughly identified by the center of the distribution.\nMedian: The middle value of the data, which is helpful to compare with the mean.\n\nSpread or Variability\n\nRange: The difference between the maximum and minimum values, indicating the spread of the data.\nStandard Deviation: Although not directly visible on the histogram, the spread of the bars gives an idea of how dispersed the data is.\nInterquartile Range (IQR): The range within which the central 50% of the data lies, often inferred by looking at the central bulk of the histogram.\n\nOutliers\n\nExtreme Values: Data points that fall far outside the general distribution, which can be seen as isolated bars away from the main body of the histogram.\n\n\nExample:\nConsider a histogram of the variable total_bill from a restaurant tips dataset:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a histogram\nsns.histplot(data=tips, x='total_bill', bins=20, kde=True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nWhen looking at this histogram, you might analyze it as follows:\n\nShape: Determine if the distribution is symmetric, skewed left, or skewed right. For example, if the histogram is right-skewed, it suggests that higher total bills are less common.\nCentral Tendency: Look for the central peak to get a sense of where most of the total_bill values lie. This provides insight into the average bill amount.\nSpread: Assess how spread out the total_bill values are. A wide spread indicates more variability in the bill amounts.\nOutliers: Identify any bars that are far away from the main distribution, indicating unusually high or low bill amounts.\nFrequency: Observe the height of the bars to understand how many observations fall into each bin.\n\n\n\n4.4.3 Boxplot\nLike a histogram, a boxplot (or box-and-whisker plot) displays the distribution of a continuous variable. It displays the five-number summary:\n\nLower whisker: The maximum of 1.5 times the interquartile range and the minimum.\nFirst quartile (Q1): The 25th percentile of the data. It is the left (or bottom) edge of the box.\nMedian (Q2): The middle value of the data set (50th percentile). It is represented by the line inside the box.\nThird quartile (Q3): The 75th percentile of the data. It is the right (or top) edge of the box.\nUpper whisker: The minimum of 1.5 times the interquartile range and the maximum.\n\nWe can view the following characteristics: 1. Spread of the data - The Interquartile range (IQR) is the difference between the first quartile (Q1) and the third quartile (Q3). It represents the middle 50% of the data. This is the size of the box. THe larger the box, the larger the spread of the data.\n\nLocation of the data\n\nThe location of the data can be read by looking at the line in the middle of the box, the median.\n\nSymmetry and Skewness of the data\n\nSymmetry: If the median is in the center of the box and the whiskers are of equal length, the data is symmetric.\nSkewness: If the median is not centered or the whiskers are of unequal length, the data is skewed. A longer whisker on the right indicates right skewness (positive skew), and a longer whisker on the left indicates left skewness (negative skew).\n\nTails or outliers\n\nMany points beyond the whiskers represents a heavy tail (a high tendency for observations to be far from the median.) A few points beyond the whiskers may indicate outliers.\n\n\nMultiple boxplots are often plotted together, to compare distributions for different populations. In this case, the boxplot can be used to determine the relationship between a categorical variable and a continuous variable.\nExample:\nLet’s create a boxplot using the tips dataset from Seaborn:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the tips dataset\ntips = sns.load_dataset('tips')\n\n# Create a box plot\nsns.boxplot(data=tips, x='day', y='total_bill')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nHere, the spread of Saturday is larger than that of the Thursday. In addition, we have that average the bills are larger on the weekend. The distributions appear symmetric for all days.\n\n\n4.4.4 Heatmap\nA heatmap compares two categorical variables to a continuous variable. The two categorical variables are represented on the horizontal and vertical axes, and the intensity of the cells represent the continuous variable.\nExample:\n\n# Load example data\nflights = sns.load_dataset('flights')\n\n# Pivot the data to create a matrix\nflights_pivot = flights.pivot(index='month', columns='year', values='passengers')\n\n# Create a heatmap\nsns.heatmap(flights_pivot, annot=True, fmt='d', cmap='YlGnBu')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.5 Stripplot\nA strip plot is used to display the distribution of a single continuous variable or the relationship between a continuous variable and a categorical variable. Each observation is represented as a point.\nExample:\n\n# Create a strip plot\nsns.stripplot(data=tips, x='day', y='total_bill', jitter=True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.6 Lineplot\nA line plot is used to display the relationship between two continuous variables, often to visualize trends over time.\n\n# Create a line plot\nsns.lineplot(data=tips, x='size', y='tip', hue='time', style='time', markers=True, dashes=False)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4.7 Summary\n\nScatterplot: Shows the relationship between two continuous variables.\nHistogram: Displays the distribution of a single continuous variable.\nBoxplot: Summarizes the distribution of a continuous variable or the relationship between a continuous and a categorical variable.\nHeatmap: Represents data in a matrix format with colors indicating values.\nStripplot: Displays the distribution of a continuous variable or the relationship between a continuous and a categorical variable.\nLineplot: Visualizes the relationship between two continuous variables, often to show trends.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_4.html#misc.-python-functions-used",
    "href": "Case_4.html#misc.-python-functions-used",
    "title": "4  Data Transformation II",
    "section": "4.5 Misc. Python functions used:",
    "text": "4.5 Misc. Python functions used:\nHere are brief explanations for some of the new functions:\n\n.dropna():\n\nThis method is used to remove missing values (NaNs) from a DataFrame or Series. By default, it drops any row containing at least one missing value. However, it can be configured to drop columns instead, or only rows/columns with all missing values, depending on the parameters passed. This is useful for cleaning data before analysis or visualization.\n\npd.merge:\n\nThis function from the pandas library is used to merge two DataFrames based on one or more common columns or indices. This is useful for combining datasets that have related information spread across different tables.\n\nplt.subplots():\n\nThis function from the matplotlib library creates a figure and a set of subplots. It returns a tuple containing a figure object and an array of axes objects. This function is handy for creating complex plots with multiple subplots in a single figure, allowing for more detailed and organized visualizations. The function can be customized to specify the number of rows and columns of subplots.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Transformation II</span>"
    ]
  },
  {
    "objectID": "Case_5.html",
    "href": "Case_5.html",
    "title": "5  Interpretation of charts and graphs",
    "section": "",
    "text": "5.1 Steps you can take to avoid bad charts\nHere are some steps to help avoid people misinterpretating your charts and ensure your charts communicate data effectively and accurately:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#steps-you-can-take-to-avoid-bad-charts",
    "href": "Case_5.html#steps-you-can-take-to-avoid-bad-charts",
    "title": "5  Interpretation of charts and graphs",
    "section": "",
    "text": "Label Axes Clearly:\n\nAction: Always provide clear and descriptive labels for the x-axis and y-axis.\nBenefit: This helps viewers understand what the chart is measuring and comparing.\n\nUse Appropriate Scales:\n\nAction: Ensure the scales on your axes are appropriate for the data being presented, starting from zero where necessary.\nBenefit: This avoids exaggerating differences or trends in the data.\n\nProvide Context:\n\nAction: Include necessary context or background information, such as data source, time period, and any relevant notes.\nBenefit: Context helps viewers interpret the data correctly and understand its relevance.\n\nChoose the Right Chart Type:\n\nAction: Select the chart type that best represents your data (e.g., bar chart for categorical data, line chart for trends over time).\nBenefit: This ensures that the data is presented in the most understandable format.\n\nAvoid Overloading with Data:\n\nAction: Limit the amount of data displayed in a single chart to avoid clutter.\nBenefit: A cleaner chart helps viewers focus on the key insights without being overwhelmed.\n\nUse Consistent Color Schemes:\n\nAction: Use consistent and color-blind friendly color schemes that differentiate data points without causing confusion.\nBenefit: This helps in distinguishing different categories or series and avoids misinterpretation due to color issues.\n\nHighlight Key Insights:\n\nAction: Use annotations, different colors, or other visual cues to highlight key data points or trends.\nBenefit: This draws attention to the most important parts of the data and aids in interpretation.\n\nAvoid Distorting Data:\n\nAction: Avoid using 3D effects or other distortions that can misrepresent data.\nBenefit: This ensures that the data is presented accurately and prevents misleading viewers.\n\nShow Data Distribution:\n\nAction: Use boxplots, histograms, or other charts to show data distribution, not just summary statistics.\nBenefit: This provides a fuller picture of the data, revealing any underlying patterns or outliers.\n\nAdd Descriptive Titles and Legends:\n\nAction: Include descriptive titles and legends that clearly explain what the chart is showing.\nBenefit: This helps viewers quickly grasp the purpose and content of the chart.\n\nUse Gridlines Sparingly:\n\nAction: Use gridlines to help viewers accurately read values, but don’t overuse them to the point of clutter.\nBenefit: This aids in precision without overwhelming the chart.\n\nAvoid Cherry-Picking Data:\n\nAction: Present all relevant data, not just data that supports a particular narrative or viewpoint.\nBenefit: This ensures a fair and unbiased representation of the data.\n\nReview and Test Charts:\n\nAction: Review charts with colleagues or test with a sample audience to identify potential misinterpretations.\nBenefit: Feedback can help identify and correct any issues before presenting the chart to a wider audience.\n\nUse Interactive Elements:\n\nAction: When possible, use interactive charts that allow viewers to explore the data more deeply.\nBenefit: This enables viewers to drill down into the data and gain a better understanding.\n\nRegularly Update Data:\n\nAction: Ensure that the data in your charts is up-to-date and accurate.\nBenefit: This maintains the relevance and reliability of the insights provided by the charts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#parallel-coordinates-plots",
    "href": "Case_5.html#parallel-coordinates-plots",
    "title": "5  Interpretation of charts and graphs",
    "section": "5.2 Parallel coordinates plots",
    "text": "5.2 Parallel coordinates plots\nCertainly! A parallel coordinates plot displays the values of multiple variables for a set of data points on parallel axes, allowing you to see patterns and relationships between the variables.\n\nAxes: Each variable in the dataset is represented by a vertical axis.\nLines: Each data point is represented by a line that connects the values of each variable across the parallel axes.\nPatterns: By observing how the lines intersect and align, you can identify patterns, correlations, and clusters in the data.\nComparisons: It is useful for comparing multiple observations and understanding the overall structure of the dataset.\n\nExample:\nLet’s create an example using the pandas and matplotlib libraries in Python. We’ll use the Iris dataset, which contains measurements of different species of iris flowers. Install scikit-learn if neccessary:\n!pip install scikit-learn\nLet’s create the plot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\n\n# Load the Iris dataset\nfrom sklearn.datasets import load_iris\niris_data = load_iris()\n\n# Convert to a DataFrame\niris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris['species'] = pd.Categorical.from_codes(iris_data.target, iris_data.target_names)\n\n# Create a parallel coordinates plot\nplt.figure(figsize=(10, 6))\nparallel_coordinates(iris, 'species', color=('#556270', '#4ECDC4', '#C7F464'))\n\n# Show the plot\nplt.title('Parallel Coordinates Plot of Iris Data')\nplt.xlabel('Attributes')\nplt.ylabel('Values')\nplt.show()\n\n\n\n\n\n\n\n\n\nLines: Each line represents an individual iris flower sample.\nAxes: The vertical axes represent the four measurements (sepal length, sepal width, petal length, and petal width).\nColors: Different colors represent different species of iris flowers (Setosa, Versicolor, Virginica).\nPatterns: By examining how the lines group and intersect, we can observe that:\n\nSetosa flowers have distinctly different measurements compared to Versicolor and Virginica flowers.\nVersicolor and Virginica have overlapping measurements, but with careful observation, patterns and differences can still be discerned.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_5.html#colored-backgrounds-on-scatterplots",
    "href": "Case_5.html#colored-backgrounds-on-scatterplots",
    "title": "5  Interpretation of charts and graphs",
    "section": "5.3 Colored backgrounds on scatterplots",
    "text": "5.3 Colored backgrounds on scatterplots\nIn this case, we colored the background based on the color of the closest point in the dataset. This is useful for classification tasks - where we want to determine if continuous variates are able to classify data points. Here is a simple example of how to do this in Python. This process includes creating a mesh grid of points, determining the closest point in the dataset for each point in the grid, and coloring the grid accordingly.\nExample:\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import cKDTree\nfrom matplotlib.colors import ListedColormap\n\n# Load the Iris dataset\niris = sns.load_dataset('iris')\n\n# Extract the relevant data\nx = iris['sepal_length']\ny = iris['sepal_width']\nspecies = iris['species']\n\n# Create a color map for species - dictionary of colors to species\nspecies_colors = {'setosa': (0.1215, 0.466666, 0.705882), \n                  'versicolor': (0.17254, 0.6274, 0.17259), \n                  'virginica': (0.83921, 0.15294, 0.1568)}\n\n# This is a series object where each row is the color associated with teh species\ncolors = iris['species'].map(species_colors)\nprint(colors)\n\n# Create a mesh grid\nx_min, x_max = x.min() - 0.5, x.max() + 0.5\ny_min, y_max = y.min() - 0.5, y.max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n\n# Combine the grid points into a single array\ngrid_points = np.c_[xx.ravel(), yy.ravel()]\n\n# This code finds the closes points\n\n# Create a KDTree for fast nearest-neighbor lookup\ntree = cKDTree(np.c_[x, y])\n\n# Find the index of the closest point in the dataset for each grid point\n_, idx = tree.query(grid_points)\n\n# Map the indices to colors\ngrid_colors = np.array(colors.tolist())[idx].reshape(xx.shape + (3,))\n\n# Plot the background grid\nplt.figure(figsize=(10, 6))\nplt.imshow(grid_colors, extent=(x_min, x_max, y_min, y_max), origin='lower', aspect='auto', alpha=0.3)\n\n# Overlay the scatter plot\nsns.scatterplot(x=x, y=y, hue=species, palette=species_colors, style=species, edgecolor='k')\n\n# Customize and show the plot\nplt.title('2D Scatterplot of Iris Data with Colored Background Grid')\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.legend(title='Species')\nplt.show()\n\n0      (0.1215, 0.466666, 0.705882)\n1      (0.1215, 0.466666, 0.705882)\n2      (0.1215, 0.466666, 0.705882)\n3      (0.1215, 0.466666, 0.705882)\n4      (0.1215, 0.466666, 0.705882)\n                   ...             \n145      (0.83921, 0.15294, 0.1568)\n146      (0.83921, 0.15294, 0.1568)\n147      (0.83921, 0.15294, 0.1568)\n148      (0.83921, 0.15294, 0.1568)\n149      (0.83921, 0.15294, 0.1568)\nName: species, Length: 150, dtype: object",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Interpretation of charts and graphs</span>"
    ]
  },
  {
    "objectID": "Case_6.html",
    "href": "Case_6.html",
    "title": "6  Exploratory Data Analysis",
    "section": "",
    "text": "6.1 Preliminary modules\nimport numpy                 as np\nimport pandas                as pd\nimport matplotlib.pyplot     as plt\nimport seaborn               as sns\nimport sklearn.metrics       as Metrics\n\n\nimport folium  #needed for interactive map\nfrom folium.plugins import HeatMap\n\nfrom   collections           import Counter\nfrom   sklearn               import preprocessing\nfrom   datetime              import datetime\nfrom   collections           import Counter\nfrom   math                  import exp\nfrom   sklearn.linear_model  import LinearRegression as LinReg\nfrom   sklearn.metrics       import mean_absolute_error\nfrom   sklearn.metrics       import median_absolute_error\nfrom   sklearn.metrics       import r2_score\n\n%matplotlib inline\nsns.set()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#attributes-of-the-dataset",
    "href": "Case_6.html#attributes-of-the-dataset",
    "title": "6  Exploratory Data Analysis",
    "section": "6.2 Attributes of the dataset",
    "text": "6.2 Attributes of the dataset\nWhen loading in a new dataset it is important to answer the following questions:\n\nWhat is the size of the dataset?\n\nPotentially useful Python attributes/methods: shape\n\nWhat are the important columns to my research question?\n\nPotentially useful Python attributes/methods: .columns, .head(), .index\n\nWhat does the data look like in these columns? Is it clean? What are the unique values?\n\nPotentially useful Python attributes/methods: .head(), .unique(), .isnull()\n\nHow many missing values are there? How are they coded?\n\nPotentially useful Python attributes/methods: .isnull(), .dropna(), .isnull().sum()\n\nDo I need to create new columns?\n\nPotentially useful Python attributes/methods: .apply()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#assessing-variables-individually",
    "href": "Case_6.html#assessing-variables-individually",
    "title": "6  Exploratory Data Analysis",
    "section": "6.3 Assessing variables individually",
    "text": "6.3 Assessing variables individually\nOnce our dataset is clean and we have identified the important variables, then we can investigate the distributions of the important variables in isolation. This helps identify anything “odd” going on in our data, helps us understand what the dataset looks like, and helps us understand the properties of each of the variables (location, spread, skewness, tails).\nOne way to do this is through printing summary statistics. Potentially useful Python attributes/methods: .describe(), .value_counts(), .mean(), .quantile(), .std(), .median() - See Data extraction and transformation for more.\nWe can also make one-dimensional charts - bar charts, histograms and boxplots - to assess each variate. Potentially useful Python attributes/methods: .hist(), .barplot(), .boxplot(), .bar() - See the plots covered in the previous cases for more.\n\n6.3.1 Barplots\nBarplots provide a way to summarize a categorical column or variable. Each bar displays the number of occurences of each category. Sometimes, when one category has most of the responses, it is useful to instead set the height of the bar to be the proportion of the responses attributed to each category.\nExample:\n\nSeaborn: Use sns.barplot() for creating bar plots with a high-level, statistical focus.\nMatplotlib: Use plt.bar() for more basic bar plots and lower-level control over plot elements.\n\nHere is an example of how to use Seaborn’s .barplot() function:\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample DataFrame\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 12, 29]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.barplot(x='Category', y='Values', data=df)\nplt.title('Barplot of Categories vs. Values')\nplt.show()\n\n\n\n\n\n\n\n\nWhile .barplot() is specific to Seaborn, Matplotlib provides similar functionality through the bar() function:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 12, 29]\n\n# Create a barplot\nplt.bar(categories, values)\nplt.title('Barplot of Categories vs. Values')\nplt.xlabel('Category')\nplt.ylabel('Values')\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#relationships-between-two-variables",
    "href": "Case_6.html#relationships-between-two-variables",
    "title": "6  Exploratory Data Analysis",
    "section": "6.4 Relationships between two variables",
    "text": "6.4 Relationships between two variables\n\n6.4.1 Investigating relationships between two variables\nOnce we have sufficient knowledge of the single variable distributions, known as the univariate distributions, we can inspect relationships between variables. Sometimes, it is too much to look at each pair of variables, so our overall research question(s) should guide which relationships we investigate. Here, to compare continuous to categorical variables, we can use boxplots, and to compare two continuous variables, we can use scatterplots or line plots. Potentially useful Python attributes/methods: .relplot(), .boxplot() - See the plots covered in the previous cases for more.\nWe can also use the correlation (see below) between two continuous variates. Note that when a correlation does not match with out intution, there may be an interaction effect, and we should investigate three-way relationships with these variables.\nWe should be looking for how the mean/median, scale, skewness and outliers of one variate changes with respect to the other. Take note of any interesting patterns. If we see something unusual or unexpected, then we should investigate further to explain it. This will also help in the future, when we use staitsitcal/machine learning models on the data. Having a good knowledge of what the dataset looks like will help you build better models, and identify problems with your models.\n\n\n6.4.2 Correlation\nCorrelation is a statistical measure that describes the strength and direction of a relationship between two variables. Correlation tells us how closely two variables move together. When two variables are correlated, knowing the value of one variable helps you predict the value of the other. Correlation falls on a scale of -1 to 1. There are two main types of correlation:\n\nPositive Correlation: This occurs when two variables tend to increase or decrease together. If one variable increases, the other variable also tends to increase. Conversely, if one decreases, the other tends to decrease.\nNegative Correlation: This occurs when one variable tends to increase as the other decreases, and vice versa. When one variable increases, the other tends to decrease.\n\n\n\n6.4.3 Examples:\n\nPositive Correlation:\n\nExample: As the number of hours studied increases, test scores tend to increase.\nExample: Higher levels of exercise are correlated with lower levels of obesity.\n\nNegative Correlation:\n\nExample: As the temperature decreases, heating costs tend to increase.\nExample: Increased smoking is correlated with a higher incidence of lung cancer.\n\n\n\n\n6.4.4 Pitfalls of Correlation:\n\nCorrelation Does Not Imply Causation: Just because two variables are correlated does not mean that one causes the other to change. There may be other factors (confounding variables) influencing both variables.\nNon-linear Relationships: Correlation measures only linear relationships. If the relationship between variables is non-linear (e.g., quadratic), correlation may not accurately reflect the strength of the relationship.\nOutliers: Extreme values (outliers) can disproportionately influence correlation calculations, leading to misleading results.\nSpurious Correlations: Sometimes variables may appear to be correlated by chance, without any meaningful relationship. Care should be taken to analyze whether we have enough sample points to ensure that the computed correlations are useful.\n\nCorrelation is a useful measure for understanding relationships between variables. However, it’s important to interpret correlation carefully, considering other factors and potential limitations, to avoid drawing incorrect conclusions.\nThe relevent python function is DataFrame.corr(). This returns the correlation matrix of the variates in the relevant DataFrame. The correlation matrix is a matrix, whose \\(ij\\) th entry is the correlation between variable in column \\(i\\) and column \\(j\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_6.html#relationships-between-three-variables",
    "href": "Case_6.html#relationships-between-three-variables",
    "title": "6  Exploratory Data Analysis",
    "section": "6.5 Relationships between three variables",
    "text": "6.5 Relationships between three variables\n\n6.5.1 Location data and folium\nWhenever one variable we are interested in is a set of locations, it is useful to use the folium package to create visuals. Please visit this Quickstart guide to learn more about using folium.\n\n\n6.5.2 Other relationships between three variables\nAgain, we can use color and/or size to add a third variate to the scatter and boxplots. You can use the hue parameter in the seaborn package to achieve this. This is particularly useful for identifying or investigating interaction effects.\n\n\n6.5.3 Interaction effects\nAn interaction effect refers to a situation where the effect of one variable on an outcome depends on the level or value of another variable.\nIn simpler terms:\nImagine you have two variables: - Variable A: Age (Young or Old) - Variable B: Treatment (New Drug or Placebo)\nAn interaction effect occurs when the effect of Variable A (Age) on the outcome (e.g., improvement in health) is different depending on whether Variable B (Treatment) is a New Drug or Placebo.\n\nDependence: An interaction effect means that the relationship between one variable and an outcome is not consistent across different levels of another variable.\nSignificance: Finding an interaction effect can change how we interpret the impact of individual variables on outcomes. It suggests that the combined effect of variables is different from what would be predicted by their individual effects alone.\n\nExample:\nLet’s say researchers are studying the effect of a new drug on health improvement in both young and old patients. They find that the drug is more effective in younger patients than older patients. This shows a main effect of age on health improvement.\nInteraction Effect: However, further analysis reveals that the difference in effectiveness between young and old patients depends on whether they received the drug or a placebo. If the drug has a significantly larger effect on young patients compared to old patients (more than what would be expected from just the main effects of age and treatment), then we say there is an interaction effect between age and treatment.\nExample:\n\nImagine we have a study on the effectiveness of a new teaching method (Variable A) on student performance (Outcome), where we also consider the student’s prior knowledge (Variable B). If the teaching method is more effective for students with high prior knowledge but less effective for students with low prior knowledge, then there is an interaction effect between teaching method and prior knowledge.\n\n\n\n6.5.4 Kernel density plots\nWe learned a new plot, called the kernel density plot. This is a smoothed version of the histogram. The bandwidth parameter controls the smoothness of the resulting KDE plot. Kernel density plots are useful to compare the distributions of two or more different samples, as they can be nicely overlaid. They also look nicer than a histogram.\nExample:\nTo create a kernel density plot, use the sns.kdeplot() function.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = pd.Series([23, 45, 12, 29, 30, 35, 42, 18, 25, 33])\n\n# Create a KDE plot with Seaborn\nsns.kdeplot(data, shade=True)\nplt.title('Kernel Density Estimation (KDE) Plot')\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.show()\n\nC:\\Users\\12RAM\\AppData\\Local\\Temp\\ipykernel_14620\\2075725077.py:9: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(data, shade=True)\n\n\n\n\n\n\n\n\n\nExplanation of the Code:\n\nsns.kdeplot(): This function plots the KDE of the given data.\n\ndata: The input data for which the KDE plot will be generated. It can be a Pandas Series, NumPy array, or a list.\nshade=True: Adds shading beneath the KDE curve for better visualization.\n\nplt.title(), plt.xlabel(), plt.ylabel(): These functions from Matplotlib are used to add a title, x-axis label, and y-axis label to the plot, respectively.\n\nAdditional Options:\n\nAdjusting Bandwidth: You can adjust the bandwidth of the KDE plot using the bw_adjust parameter in sns.kdeplot(). Lower values result in a smoother plot, while higher values result in a more jagged plot.\nMultiple Plots: sns.kdeplot() can also be used to plot KDEs for multiple variables or groups by passing multiple datasets or using the hue parameter.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "Case_7.html",
    "href": "Case_7.html",
    "title": "7  Data cleaning/wrangling",
    "section": "",
    "text": "7.1 Preliminary modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport base64\nimport datetime\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom geopy.geocoders import Nominatim",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#handling-missing-data",
    "href": "Case_7.html#handling-missing-data",
    "title": "7  Data cleaning/wrangling",
    "section": "7.2 Handling missing data",
    "text": "7.2 Handling missing data\nOne of the first steps in data cleaning is to deal with null or missing values. First, one should determine how missing values are labelled in the dataset, as this can vary widely. Some common encodings for missing data are as follows:\n\nNaN or nan: This is the most common representation of missing values, especially in Python.\nNone: In some programming languages, None is used to indicate the absence of a value.\nNull or NULL: Common in SQL databases.\nEmpty strings (““): Often used in text data.\nSpecial values: Sometimes a special value, like -9999 or 9999, is used to represent missing data.\nNA or N/A: These are often used in spreadsheets and text files to denote missing values.\n0 or -1: Occasionally used in cases where 0 or -1 are not valid values in the context of the data.\nBlanks or Whitespaces: Spaces or tabs might be used to signify missing values in some text data.\nPlaceholders like “missing” or “unknown”: Textual placeholders indicating the data is not available.\nINF or -INF: Infinity values, sometimes used to denote missing values.\n\nBefore moving forward, it is important to convert the missing values to either ‘NaN’ or ‘nan’, as this is what python functions built for missing values will expect to see.\nPrevious cases only gave a passing treatment of missing data and resulted in dropping the rows containing null values entirely. Here, we will be more nuanced and look at ways that missing values can be replaced appropriately. As always, domain knowledge is crucial in selecting what method to go forward with.\nThe goal of filling in missing data is to not neccessarily to recover the exact values of the missing values, but rather to ensure that the conclusions of the analysis are not overtly impacted by the missing values.\nWhen dealing with missing data, it is important not to just use generic pandas/Python functions without thinking to fill them in. This can drastically corrupt your analysis.\nInstead, a first step is to find the source of the missing data. Why is it missing? and/or how did it go missing? If we can answer these questions, then this will help us in determining how to fill in the missing values.\nOne way to handle missing values is to simply remove any row which contains a missing value from the dataset. This is known as complete case analysis. Complete case analysis is desirable when there are not that many rows with missing values, and we do not lose any critical sub-populations as a results of dropping those rows. For instance, a rule of thumb is that if &lt;1% of the rows are missing, then it might be desirable to do a complete case analysis. It is fast and easy, and does not require extensive statistical knowledge. This also depends on the size of the dataset. If you have 10 million observations, and 1 million within each subpopulation, then removing even 5% of the rows should not impact the analysis too much. On the other hand, if we have only 20 observations, we should try not to remove any rows.\nAnother approach is to fill in the missing values - this is known as imputation. Usually, we use the other observations or rows to fill in the missing values.\nNote that a once population method of imputation which you should not use is replacing missing values with the mean/median of the available values in the column. The reason for this is that it lowers the sample standard deviation of the resulting column and can bias the resulting analysis.\nSome other methods of imputation are given below:\n\nForward/Backward Fill: Use the next or previous value to fill in the missing value. Forward/backward filling may make sense when there is a relationship between the rows. Example: In a time series dataset where stock prices are recorded daily, you might use forward fill to propagate the last known price to the days when the market data is missing.\nInterpolation: Estimate missing values using interpolation techniques. Example: In a climate dataset with daily temperature readings, you could use linear interpolation to estimate the temperature on days when data is missing by averaging the temperatures of the surrounding days.\nK-Nearest Neighbors (KNN): The KNN algorithm uses points that are similar to the points with missing data (called nearest neighbors) to impute missing values. Example: In a survey dataset where some participants skipped a few questions, KNN can impute their missing responses by considering the responses of the most similar participants (neighbors).\nDomain-Specific Imputation: Depending on the domain, you might use specific rules or external datasets to impute missing values. Example: In an educational dataset where some students’ test scores are missing, you might impute the missing scores using the average scores from other tests taken by the same student or from other students in the same class or grade level. This method leverages the relationship between students’ performance across different tests or the performance of their peers.\nRegression/Machine Learning Models: Use models, such as regression models or Neural Networks to predict and fill in missing values. Example: In a housing dataset where some houses are missing the number of bedrooms, you could use a regression model that predicts the number of bedrooms based on the house’s size, location, and price. In a complex dataset with multiple features about customer behavior, a Random Forest model can be used to predict and fill in missing values by leveraging the patterns and interactions between the different features.\n\n\n7.2.1 Interpolate\nThe .interpolate() function allows us to interpolate numerical values, based on the surrounding values. Interpolation is a method of estimating unknown values based on values that are close to in (in terms of rows). In this case, we know that the rate of change in riders is probably similar to the rate of change on the surrounding days. If we think that it falls between the values of the preceding and following days, and the rows are evenly spaced in time, then we can use the default parameters, which uses linear interpolation. There are a number of other methods of intperolation that can be specified via the method parameter. Be sure to read and understand each method carefully before using it. Otherwise, you could just be filling in the values with nonsense.\nParameters of .interpolate()\n\nmethod: Specifies the interpolation method to use.\n\n'linear' (default): Linear interpolation.\n'time': Interpolation for time-series data.\n'index': Uses the index for interpolation.\nOther methods like 'polynomial', 'spline', 'barycentric', etc. see the documentation.\n\naxis: Specifies the axis along which to interpolate.\n\n0 or 'index': Interpolates along the index (default for DataFrame).\n1 or 'columns': Interpolates along the columns.\n\n\nExample:\nHere’s a basic example demonstrating the use of .interpolate():\n\nimport pandas as pd\nimport numpy as np\n\n# Creating a sample DataFrame with NaN values\ndf = pd.DataFrame({\n    'A': [1, np.nan, 3, np.nan, 5],\n    'B': [np.nan, 2, np.nan, 4, np.nan]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Interpolating missing values\ndf_interpolated = df.interpolate()\n\nprint(\"\\nInterpolated DataFrame:\")\nprint(df_interpolated)\n\nOriginal DataFrame:\n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  NaN\n3  NaN  4.0\n4  5.0  NaN\n\nInterpolated DataFrame:\n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  4.0  4.0\n4  5.0  4.0\n\n\nOutput:\nOriginal DataFrame:\n     A    B\n0  1.0  NaN\n1  NaN  2.0\n2  3.0  NaN\n3  NaN  4.0\n4  5.0  NaN\n\nInterpolated DataFrame:\n     A    B\n0  1.0  NaN\n1  2.0  2.0\n2  3.0  3.0\n3  4.0  4.0\n4  5.0  NaN\nInterpolation is particularly useful for time-series data and datasets where you want to estimate missing values based on the surrounding data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#handling-string-formats",
    "href": "Case_7.html#handling-string-formats",
    "title": "7  Data cleaning/wrangling",
    "section": "7.3 Handling string formats",
    "text": "7.3 Handling string formats\nIt is important to print the unique values of a string variable, in order to check for spaces, mixed case and spelling mistakes. If the variable has many entries, we may not be able to check all unique values. In that case, it is a good idea to use .strip() and str.lower()/str.upper() to remove extra spaces and convert all characters to lower case. In the natural language processing lesson, we will learn more about manipulating strings.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#unique-ids",
    "href": "Case_7.html#unique-ids",
    "title": "7  Data cleaning/wrangling",
    "section": "7.4 Unique IDs",
    "text": "7.4 Unique IDs\nWhen generating IDs, it is important to make sure the generation process is idempotent (i.e. the same ID should be generated for each trip no matter how many times you run the script). The idempotency is required because there may be chances that the same trip is input into this tool multiple times. For example, the customer first uploads the data set for the first week of the month (may be for testing purposes, or based on data availability, etc.) and then uploads the data for the entire month. Now if the same trip is assigned different IDs on each run, then it might result in the analytics platform interpreting this as two different trips and this will corrupt the analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#more-on-datetime-objects",
    "href": "Case_7.html#more-on-datetime-objects",
    "title": "7  Data cleaning/wrangling",
    "section": "7.5 More on datetime objects",
    "text": "7.5 More on datetime objects\nWe have learnt about datetime objects in the past. In this case, we covered some new functionalities of these objects. We should almost always convert dates and times to datetime objects.\nHere’s an overview of some new attributes:\n\n.year: Returns the year.\n.month: Returns the month (1-12).\n.day: Returns the day of the month (1-31).\n.hour: Returns the hour (0-23).\n.minute: Returns the minute (0-59).\n.second: Returns the second (0-59).\n.microsecond: Returns the microsecond (0-999999).\n.tzinfo: Returns the timezone information.\n\nIn addition, you can use the following methods to get week-related information:\n\n.weekday(): Returns the day of the week as an integer, where Monday is 0 and Sunday is 6.\n.isoweekday(): Returns the day of the week as an integer, where Monday is 1 and Sunday is 7.\n.isocalendar(): Returns a tuple containing the ISO year, ISO week number, and ISO weekday.\n\nExample:\n\nfrom datetime import datetime\n\n# Create a datetime object\ndt = datetime(2023, 7, 5, 14, 30, 0)\n\n# Year, month, day, etc.\nprint(\"Year:\", dt.year)          # Output: 2023\nprint(\"Month:\", dt.month)        # Output: 7\nprint(\"Day:\", dt.day)            # Output: 5\nprint(\"Hour:\", dt.hour)          # Output: 14\nprint(\"Minute:\", dt.minute)      # Output: 30\nprint(\"Second:\", dt.second)      # Output: 0\n\n# Weekday (Monday = 0, Sunday = 6)\nprint(\"Weekday:\", dt.weekday())  # Output: 2 (Wednesday)\n\n# ISO Weekday (Monday = 1, Sunday = 7)\nprint(\"ISO Weekday:\", dt.isoweekday())  # Output: 3 (Wednesday)\n\n# ISO Calendar (Year, Week number, Weekday)\niso_calendar = dt.isocalendar()\nprint(\"ISO Calendar:\", iso_calendar)    # Output: (2023, 27, 3)\n\nYear: 2023\nMonth: 7\nDay: 5\nHour: 14\nMinute: 30\nSecond: 0\nWeekday: 2\nISO Weekday: 3\nISO Calendar: datetime.IsoCalendarDate(year=2023, week=27, weekday=3)\n\n\nAdditional operations:\n\nCurrent Date and Time:\n\nnow = datetime.now()\nprint(now)  # Output: current date and time\n\n2024-07-08 17:21:09.173909\n\n\nParsing Dates:\n\ndate_str = \"2023-07-05\"\nparsed_date = datetime.strptime(date_str, \"%Y-%m-%d\")\nprint(parsed_date)  # Output: 2023-07-05 00:00:00\n\n2023-07-05 00:00:00\n\n\nFormatting Dates:\n\ndt = datetime(2023, 7, 5, 14, 30, 0)\nformatted_date = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\nprint(formatted_date)  # Output: 2023-07-05 14:30:00\n\n2023-07-05 14:30:00\n\n\nDate Arithmetic:\n\ndt1 = datetime(2023, 7, 5)\ndt2 = datetime(2023, 7, 10)\ndelta = dt2 - dt1\nprint(delta)  # Output: 5 days, 0:00:00\n\n5 days, 0:00:00\n\n\nHandling Time Zones:\n\nfrom datetime import timezone, timedelta\n\nutc_dt = datetime(2023, 7, 5, 14, 30, 0, tzinfo=timezone.utc)\nlocal_tz = timezone(timedelta(hours=-5))\nlocal_dt = utc_dt.astimezone(local_tz)\nprint(local_dt)  # Output: 2023-07-05 09:30:00-05:00\n\n2023-07-05 09:30:00-05:00",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_7.html#adding-new-data",
    "href": "Case_7.html#adding-new-data",
    "title": "7  Data cleaning/wrangling",
    "section": "7.6 Adding new data",
    "text": "7.6 Adding new data\nOften, you will have to add data from other datasets, websites or sources to your dataset. For instance, in Case 7, we showed how to get address data using geopy.\nThe function geolocator.geocode() can be used to obtain longitude and latitude from a string which contains the address. To learn more about geopy, you can follow the functions and tutorials here. In general, you can use geopy to obtain coordinates and other location data, given data like address, postal code etc. You can use this in conjunction with the folium package to create some nice visualizations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data cleaning/wrangling</span>"
    ]
  },
  {
    "objectID": "Case_8.html",
    "href": "Case_8.html",
    "title": "8  Hypothesis testing I",
    "section": "",
    "text": "8.1 Preliminary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport statsmodels\nfrom scipy import stats \nfrom pingouin import pairwise_tests #this is for performing the pairwise tests\nfrom pingouin import pairwise_ttests #this is for performing the pairwise tests",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#hypothesis-testing-overview",
    "href": "Case_8.html#hypothesis-testing-overview",
    "title": "8  Hypothesis testing I",
    "section": "8.2 Hypothesis testing overview",
    "text": "8.2 Hypothesis testing overview\nHypothesis help you rule out sampling variation as an explanation for an observed pattern in a data set. It helps us lift a pattern from a sample to a population. The idea is this - every data set we have seen in this course is comprised of a small subset of a population we are interested in learning about. That small subset is called a sample. For instance, in the previous case, we were interested in the population of trips taken on the rentable bikes. We had access to a small subset of the trips taken - this is the sample. Now, there is a small problem with the results of any data analysis. The subset of observations or sample that we have access to is random. We could have easily drawn a different subset and a different subset will give different output in data analysis - that is - different plots and summary statistics. For instance, the average length of a bike trip could be 45 minutes in one subset and 30 minutes in another. The true average bike trip length for the population could be anything - say 39. The purpose of hypothesis testing is to rule out the situation where the sample may not resemble the population. In other words, it is useful to say that the conclusions made in our data analysis are likely to reflect that of the population, and are not due to drawing a bad sample.\nIn this course, we will mainly focus on saying something about the mean of potentially several populations. In general, the hypothesis testing procedure concerns two hypotheses - the null and alternative hypothesis. These contain mutually exclusive statements about a population.\nFor example: \\[H_0: \\text{The population trip length is } 40\\ vs.\\ H_a: \\text{The population trip length is not } 40.\\]\n\\(H_0\\) is called the null hypothesis. The null hypothesis often corresponds to the situation of “no effect” - but not always.\nIn opposition to the null hypothesis, we define an alternative hypothesis (often indicated with \\(H_1\\) or \\(H_a\\)) to challenge the null hypothesis. In general, this is the hypothesis of there existing some pattern or effect in the data.\nFor instance, we may suspect that the average trip length is 40, and we may be concerned about whether our data shows that the trip length is longer than this. This may be of concern for computing bike maintenance costs. In that case, we would define \\(H_a: \\text{The population trip length is greater than } 40.\\)\nThe next step in a hypothesis test is to compute a p-value using our sample. The p-value measures the evidence against the null hypothesis. It is a number between 0 and 1. The closer to 0 the p-value is, the more evidence there is against the null hypothesis. The p-value can be interpreted as follows: The p-value is the probability that, assuming the null hypothesis is true, we drew a sample that differs from the null hypothesis at least as much as the one at hand.\nLet’s unpack this statement using our example. First, before computing our p-value we assume that the null hypothesis is true. In our example, we would assume that the mean population trip length is 40 minutes. Then, if the mean population trip length is 40 minutes we can measure how far the sample mean is from 40. Say that our sample mean is 45, and so then we would have observed a sample whose mean is 5 minutes higher than that of the assumed population mean. The p-value is then the probability that we saw a sample whose sample mean is at least 5 minutes away from 40. You will learn in a later course how to compute such a probability. Intuitively, the higher the distance of the sample mean from 40, the lower the p-value. Therefore, if we observe a sample mean far from 40, the p-value will be very close to 0. We can interpret the p-value as the probability, assuming the mean population trip length is 40, that we drew a sample whose mean differs from 40 at least as much as the one at hand.\n\n\n\n\n\n\nCaution\n\n\n\nThe \\(p\\)-value DOESN’T mean that the probability of \\(H_a\\) is 1-(\\(p\\) - value).\n\n\nIn general, p-values close to 0, as in &lt;0.1 or &lt;0.05 present evidence against the null hypothesis. This threshold can depend on the application.\nWhen doing a formal hypothesis tests, there are two possible outcomes for a test: - (1) We conclude \\(H_0\\) is false, and say we reject \\(H_0\\). In this case we will conclude that there is statistical evidence for the alternative \\(H_a\\). - Or (2) we fail to reject \\(H_0\\). In this case, we conclude that there is not enough statistical evidence to say that \\(H_0\\) is false.\n\n\n\n\n\n\nCaution\n\n\n\nNotice that in the second case we cannot say that the original hypothesis is true - it might be that we just don’t have enough data to rule it out.\n\n\nIn a formal hypothesis test, we define a threshold \\(\\alpha\\), where if the \\(p\\)-value falls below this threshold, then we reject the null hypothesis. In general, less formal cases, one may just compute the p-value and use it to inform decision making, along with other factors.\nTo summarize:\n\nA hypothesis test is used to confirm that a pattern is a feature of the population, and is not due to sampling variation.\nTo conduct a hypothesis test, we define a null and alternative hypothesis.\nAfter defining the hypohtheses, we then compute the evidence against the null hypothesis, given by the \\(p\\)-value.\nIf the \\(p\\)-value is small, we reject the null hypothesis and conclude the alternative. Otherwise, we fail to reject the null hypothesis.\nYou should always interpret the \\(p\\)-value and state your conclusion as the final step in the hypothesis test.\n\nTo elaborate on point 5. - the point of any statistical analysis is not to run the correct code, but to properly choose the analysis procedure and interpret the results appropriately.\n\n\n\n\n\n\nWarning\n\n\n\nA hypothesis test cannot tell you which scenario is certainly true - we would have to have access to the whole population to know that. It can tell us that things hold with very high certainty, which is generally enough for most situations.\n\n\nThe hypothesis tests introduced in Case 8 concern only testing for the mean of one or more populations. We cover each of those in turn.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-the-mean-in-a-single-population",
    "href": "Case_8.html#testing-for-the-mean-in-a-single-population",
    "title": "8  Hypothesis testing I",
    "section": "8.3 Testing for the mean in a single population",
    "text": "8.3 Testing for the mean in a single population\nWe first cover how to perform a hypothesis test concerning the mean value of a single population. Define the population mean of a single population to be \\(\\mu\\). The null hypothesis in this case is in the form of \\(H_0\\colon \\mu=\\mu_0\\). For instance, above, \\(\\mu_0=40\\).\nWe have three different ways to define an alternative hypothesis:\n\n\\(H_a: \\mu \\neq \\mu_0\\) (two-sided test)\n\\(H_a: \\mu &gt; \\mu_0\\) (one-sided test)\n\\(H_a: \\mu &lt; \\mu_0\\) (one-sided test)\n\nThe syntax for perfomring the test is given as follows:\nstats.ttest_1samp(Series you want to test, popmean= mu_0, alternative= specify which of (1-3) here)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-a-difference-in-mean-for-two-populations",
    "href": "Case_8.html#testing-for-a-difference-in-mean-for-two-populations",
    "title": "8  Hypothesis testing I",
    "section": "8.4 Testing for a difference in mean for two populations",
    "text": "8.4 Testing for a difference in mean for two populations\nWe now cover how to perform a hypothesis test concerning the difference in the mean values between two populations. We would like to test whether two populations have different population means. That is, whether the difference between the population mean of group 1 (\\(\\mu_1\\)) is different from the population mean of group 2 (\\(\\mu_2\\)). The hypotheses look like: \\[ H_0: \\mu_1=\\mu_2\\] \\[H_a: \\mu_1 \\neq \\mu_2\\]\nThe syntax for performing this test is given as follows:\nstats.ttest_ind(data for group 1 , data for group 2, equal_var=False)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#testing-for-a-difference-in-mean-for-several-populations",
    "href": "Case_8.html#testing-for-a-difference-in-mean-for-several-populations",
    "title": "8  Hypothesis testing I",
    "section": "8.5 Testing for a difference in mean for several populations",
    "text": "8.5 Testing for a difference in mean for several populations\nLastly, if we would like to perform a hypothesis test for whether or not all the population means are the same when considering \\(k&gt;2\\) populations, we can do the following.\nFirst, the hypotheses are given by\n\\[H_0: \\mu_1=\\mu_2\\ldots\\mu_3=\\mu_k,\\] vs. \\[H_a : \\mathrm{At \\,least\\, one\\, of\\, the\\, means\\,} \\mu_j \\mathrm{\\,is \\,different\\, from\\, the \\,others}.\\]\nTo test this hypothesis we need an extension of the capabilities of the \\(t\\) - test (which can test only two groups at the same time). This test is called Analysis of Variance (ANOVA).\nThe syntax is given as follows:\n# This is code you can fill in to perform this test\nmod = ols('quantity of interest ~ grouping variable', data= YOUR_DATAFRAME).fit()  \nsm.stats.anova_lm(mod, typ=2)\nThe \\(p\\)-value is listed in the output, and can be interpreted as instructed above.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#errors-in-hypothesis-testing",
    "href": "Case_8.html#errors-in-hypothesis-testing",
    "title": "8  Hypothesis testing I",
    "section": "8.6 Errors in hypothesis testing",
    "text": "8.6 Errors in hypothesis testing\nThere are two ways that a test can lead us to an incorrect decision:\n\nWhen \\(H_0\\) is true and we reject it. This is called Type 1 Error. It corresponds to obtaining a false positive.\nWhen \\(H_0\\) is false and we do not reject it. This is called Type 2 Error. It corresponds to having a false negative.\n\n\n\n\n\n\n\\(H_0\\) is true \n\n\n \\(H_0\\) is False\n\n\n\n\nReject \\(H_0\\)\n\n\nType I error\n\n\nCorrect Decision (True Positive)\n\n\n\n\nFail to Reject \\(H_0\\) \n\n\nCorrect Decision (True negative)\n\n\nType II error\n\n\n\nIn general, a Type I error is thought to be more serious and so it is standard practice to control the probability of making a Type I error. In a formal hypothesis test, when the null hypothesis is true, the probability of making a Type I error is the threshold \\(\\alpha\\), also known as the significance level, that we introduced above. Often, we choose our significance level \\(\\alpha\\) to be small, e.g., \\(1\\%,5\\%,10\\%\\). Lowering the \\(\\alpha\\) value (say to \\(1\\%\\)) will decrease the probability of making a false positive conclusion, when the null hypothesis is true. Of course, because we control \\(\\alpha\\), we cannot control the Type II error we make. Note that lowering \\(\\alpha\\) is not without consequence, as, if the alternative hypothesis is true, then a lower threshold increases the probability of a Type II error.\nIn summary, it is important to be aware of the two types of error and evaluate the gravity of what making each error would mean for your context. In particular, you should evaluate the consequences of making a Type I error and choose your the threshold \\(\\alpha\\) accordingly. Lastly, you should know that there is a trade-of between Type I error and Type II error in a given hypothesis test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  },
  {
    "objectID": "Case_8.html#misc.-python-functions",
    "href": "Case_8.html#misc.-python-functions",
    "title": "8  Hypothesis testing I",
    "section": "8.7 Misc. Python functions",
    "text": "8.7 Misc. Python functions\n\nplt.subplot(rows, cols, curr_plot) - use this function to position multiple plots in a single figure. For example, plt.subplot(2, 3, 4) creates a grid with 2 rows and 3 columns and activates the 4th subplot.\nsns.countplot - creates a barplot.\nenumerate Use this function to get both the index and the value from an iterable in a loop. Example:\n\n\nfruits = ['apple', 'banana', 'cherry']\nfor index, fruit in enumerate(fruits, start=1):\n    print(f\"{index}: {fruit}\")\n\n1: apple\n2: banana\n3: cherry\n\n\n\nplt.xticks(rotation = 90) used to rotate the \\(x\\)-axis labels.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hypothesis testing I</span>"
    ]
  }
]